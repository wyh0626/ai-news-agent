"""Writer Agent - å°†æå–åçš„ç»“æ„åŒ–ä¿¡æ¯ç¼–æ’ä¸ºå¯è¯»çš„æ¯æ—¥ç®€æŠ¥"""

import logging
from collections import defaultdict
from datetime import datetime, timedelta, timezone

from langchain_openai import ChatOpenAI

from src.config import settings
from src.graph.state import PipelineState
from src.models import ArticleSection, ExtractedItem, GeneratedArticle

logger = logging.getLogger(__name__)

WRITER_PROMPT = """You are a professional AI news editor. Write an English AI daily newsletter based on the following news items.

âš ï¸ You MUST strictly follow the template below. Do not add or remove sections, change heading levels, or alter list format.

â”â”â”â”â”â” Output Template (follow strictly) â”â”â”â”â”â”

# AI Daily â€” {date}

> Covering {total_count} AI news items

## ğŸ”¥ Top Stories

Pick the 2-3 most important news from featured items, expand with ### numbered headings:

### 1. News Headline

2-3 sentence analysis with background and impact. [Source-source](url)

### 2. News Headline

2-3 sentence analysis. [Source-source](url)

## ğŸ“° Featured

Group remaining featured news by topic, each group as a ### heading, items as bullet list:

### Topic Name

- **News Headline** â€” One-sentence summary + brief analysis. [Source-source](url)
- **News Headline** â€” One-sentence summary. [Source-source](url)

## âš¡ Quick Bites

All brief items as a flat bullet list, no grouping:

- **News Headline** â€” One-sentence summary. [Source-source](url)
- **News Headline** â€” One-sentence summary. [Source-source](url)

---

*Generated by AI News Agent | {date}*

â”â”â”â”â”â” End of Template â”â”â”â”â”â”

Writing rules:
1. "Top Stories" has only 2-3 items with ### numbered headings, each with 2-3 sentence analysis
2. "Featured" groups by topic (e.g. "Open Source Models", "AI Safety", "Tools & Frameworks"), items as - **Title** â€” summary format
3. "Quick Bites" is a flat list, - **Title** â€” one sentence format
4. Every news item MUST include [Source-source](url), where "source" comes from the source field (x, reddit, hackernews, arxiv, github, rss)
5. Similar news from different sources can be merged
6. Professional, concise writing for AI practitioners
7. Output Markdown directly, no code blocks

## Featured Items (importance â‰¥ {threshold}, {featured_count} items)
{featured_json}

## Brief Items ({brief_count} items)
{brief_json}"""


def _build_llm() -> ChatOpenAI:
    kwargs = {
        "model": settings.openai_model,
        "max_tokens": 32768,
    }
    if settings.openai_api_key:
        kwargs["api_key"] = settings.openai_api_key
    if settings.openai_base_url:
        kwargs["base_url"] = settings.openai_base_url
    return ChatOpenAI(**kwargs)


def _group_by_topic(items: list[ExtractedItem]) -> dict[str, list[ExtractedItem]]:
    """æŒ‰ä¸»é¢˜èšç±»æ–°é—»"""
    groups: dict[str, list[ExtractedItem]] = defaultdict(list)
    for item in items:
        primary_topic = item.topics[0] if item.topics else "Other"
        groups[primary_topic].append(item)
    return dict(groups)


def _build_fallback_markdown(items: list[ExtractedItem], today: str) -> str:
    """LLM ä¸å¯ç”¨æ—¶çš„é™çº§æ¨¡æ¿æ¸²æŸ“ï¼Œä¸ LLM è¾“å‡ºæ ¼å¼ä¿æŒä¸€è‡´"""
    lines = [
        f"# AI Daily â€” {today}",
        "",
        f"> Covering {len(items)} AI news items",
        "",
    ]

    # Top Stories: importance >= 8 çš„å‰ 3 æ¡
    top_items = [i for i in items if i.importance_score >= 8][:3]
    if top_items:
        lines.append("## ğŸ”¥ Top Stories")
        lines.append("")
        for idx, item in enumerate(top_items, 1):
            summary = item.summary or item.title
            lines.append(f"### {idx}. {item.title}")
            lines.append("")
            src = item.source_type.value
            lines.append(f"{summary} [Source-{src}]({item.url})")
            lines.append("")

    # Featured: æŒ‰è¯é¢˜åˆ†ç»„ï¼Œæ’é™¤å·²åœ¨ç„¦ç‚¹ä¸­å±•ç¤ºçš„
    top_ids = {id(i) for i in top_items}
    featured = [i for i in items if i.importance_score >= 7 and id(i) not in top_ids]
    if featured:
        groups = _group_by_topic(featured)
        lines.append("## ğŸ“° Featured")
        lines.append("")
        for topic, topic_items in groups.items():
            lines.append(f"### {topic}")
            lines.append("")
            for item in topic_items:
                summary = item.summary or item.title
                src = item.source_type.value
                lines.append(f"- **{item.title}** â€” {summary} [Source-{src}]({item.url})")
            lines.append("")

    # Quick Bites
    brief = [i for i in items if i.importance_score < 7]
    if brief:
        lines.append("## âš¡ Quick Bites")
        lines.append("")
        for item in brief:
            summary = item.summary or item.title
            src = item.source_type.value
            lines.append(f"- **{item.title}** â€” {summary} [Source-{src}]({item.url})")
        lines.append("")

    lines.append("---")
    lines.append("")
    lines.append(f"*Generated by AI News Agent | {today}*")

    return "\n".join(lines)


def _normalize_source_links(markdown: str) -> str:
    """åå¤„ç†ï¼šç»Ÿä¸€æ‰€æœ‰é“¾æ¥æ ‡ç­¾ä¸º [Source-xxx](url) æ ¼å¼"""
    import re

    # åŸŸå â†’ æ¥æºæ ‡ç­¾æ˜ å°„
    domain_map = {
        "x.com": "x", "twitter.com": "x",
        "reddit.com": "reddit",
        "news.ycombinator.com": "hackernews", "hn.algolia.com": "hackernews",
        "arxiv.org": "arxiv",
        "github.com": "github",
        "huggingface.co": "huggingface",
    }

    def _get_source(url: str) -> str:
        for domain, label in domain_map.items():
            if domain in url:
                return label
        return "rss"

    # åŒ¹é…æ‰€æœ‰ markdown é“¾æ¥ [ä»»æ„æ–‡æœ¬](url)
    def replacer(m):
        text = m.group(1)
        url = m.group(2)
        source = _get_source(url)
        return f"[Source-{source}]({url})"

    return re.sub(r'\[([^\]]*)\]\((https?://[^\)]+)\)', replacer, markdown)


def _items_to_json(items: list[ExtractedItem], brief: bool = False) -> str:
    """å°†æ–°é—»æ¡ç›®è½¬ä¸º JSON å­—ç¬¦ä¸²ï¼›brief=True æ—¶åªä¿ç•™å¿…è¦å­—æ®µä»¥èŠ‚çœ token"""
    import json

    data = []
    for item in items:
        if brief:
            data.append({
                "title": item.title,
                "source": item.source_type.value,
                "url": item.url,
            })
        else:
            data.append({
                "title": item.title,
                "summary": item.summary,
                "topics": item.topics,
                "importance_score": item.importance_score,
                "source": item.source_type.value,
                "url": item.url,
            })
    return json.dumps(data, ensure_ascii=False, indent=2)


def _split_featured_brief(
    items: list[ExtractedItem],
) -> tuple[list[ExtractedItem], list[ExtractedItem]]:
    """æŒ‰ importance_score å°†æ–°é—»åˆ†ä¸ºé‡ç‚¹å’Œç®€è¦ä¸¤å±‚

    items å·²æŒ‰ importance_score é™åºæ’åˆ—ï¼ˆç”± Extractor ä¿è¯ï¼‰
    """
    top_k = settings.top_k_featured
    featured = items[:top_k]
    brief = items[top_k:]
    return featured, brief


async def _generate_description(top3: list[ExtractedItem], date: str) -> str:
    """ç”¨ LLM ç”Ÿæˆä¸€å¥è¯æ—¥æŠ¥æ‘˜è¦ï¼Œé™çº§æ—¶æ‹¼æ¥æ ‡é¢˜"""
    if not settings.openai_api_key or not top3:
        titles = " Â· ".join(
            (it.title[:40] + "...") if len(it.title) > 40 else it.title
            for it in top3
        )
        return titles

    headlines = "\n".join(f"- {it.title}" for it in top3)
    prompt = (
        f"Based on these top AI news headlines from {date}, write ONE concise sentence "
        f"(max 25 words) that captures the day's most important AI developments. "
        f"Be specific, not generic. Output only the sentence, no quotes.\n\n"
        f"{headlines}"
    )
    try:
        llm = _build_llm()
        resp = await llm.ainvoke(prompt)
        desc = resp.content.strip().strip('"').strip("'")
        if len(desc) > 150:
            desc = desc[:147] + "..."
        return desc
    except Exception as e:
        logger.warning(f"Description generation failed, falling back: {e}")
        return " Â· ".join(
            (it.title[:40] + "...") if len(it.title) > 40 else it.title
            for it in top3
        )


async def writer_node(state: PipelineState) -> dict:
    """Writer èŠ‚ç‚¹ï¼šç”Ÿæˆæ¯æ—¥æ–°é—»ç®€æŠ¥ï¼ˆé‡ç‚¹å±•å¼€ + å¿«è®¯é€Ÿè§ˆï¼‰"""
    extracted = state.get("extracted_items", [])
    if not extracted:
        logger.warning("No extracted items, skipping writing")
        return {"article": None}

    today = (datetime.now(tz=timezone.utc) - timedelta(days=1)).strftime("%Y-%m-%d")
    featured, brief = _split_featured_brief(extracted)
    threshold = featured[-1].importance_score if featured else 0

    logger.info(
        f"Writing {today} newsletter: "
        f"featured {len(featured)} + brief {len(brief)} "
        f"(cutoff importance â‰¥ {threshold:.1f})"
    )

    groups = _group_by_topic(extracted)
    sections = [ArticleSection(topic=t, items=items) for t, items in groups.items()]

    # å°è¯•ç”¨ LLM ç”Ÿæˆ
    markdown = ""
    if settings.openai_api_key:
        try:
            llm = _build_llm()
            prompt = WRITER_PROMPT.format(
                date=today,
                total_count=len(extracted),
                featured_count=len(featured),
                brief_count=len(brief),
                threshold=threshold,
                featured_json=_items_to_json(featured),
                brief_json=_items_to_json(brief, brief=True),
            )
            resp = await llm.ainvoke(prompt)
            markdown = resp.content
            # å®Œæ•´æ€§æ£€æŸ¥ï¼šLLM å¯èƒ½æˆªæ–­ï¼Œç¼ºå°‘ç« èŠ‚åˆ™é™çº§
            required_sections = ["Top Stories", "Quick Bites"]
            missing = [s for s in required_sections if s not in markdown]
            if missing:
                logger.warning(f"LLM output incomplete, missing sections: {missing}, falling back to template")
                markdown = _build_fallback_markdown(extracted, today)
            else:
                # LLM output link labels may not be uniform, use post-processing to correct
                markdown = _normalize_source_links(markdown)
                logger.info("LLM writing complete")
        except Exception as e:
            logger.error(f"LLM writing failed, falling back to template: {e}")
            markdown = _build_fallback_markdown(extracted, today)
    else:
        logger.info("LLM not configured, using template")
        markdown = _build_fallback_markdown(extracted, today)

    # å°é¢å›¾ï¼šå– importance_score æœ€é«˜ä¸”æœ‰å›¾ç‰‡çš„æ¨æ–‡ï¼Œæ’å…¥åˆ°æ ‡é¢˜è¡Œä¹‹å
    cover_item = next(
        (it for it in sorted(extracted, key=lambda x: x.importance_score, reverse=True)
         if it.metadata.get("image")),
        None,
    )
    if cover_item:
        img_md = f"\n![cover]({cover_item.metadata['image']})\n"
        # æ’å…¥åˆ°ç¬¬ä¸€ä¸ª h1 æ ‡é¢˜è¡Œä¹‹å
        lines = markdown.split("\n")
        for i, line in enumerate(lines):
            if line.startswith("# "):
                lines.insert(i + 1, img_md)
                break
        markdown = "\n".join(lines)
        logger.info(f"Cover image added from @{cover_item.author}: {cover_item.metadata['image']}")

    # ç”¨ LLM ç”Ÿæˆä¸€å¥è¯ descriptionï¼ˆåŒè¯­ï¼‰
    top3 = sorted(extracted, key=lambda x: x.importance_score, reverse=True)[:3]
    description = await _generate_description(top3, today)

    article = GeneratedArticle(
        title=f"AI Daily â€” {today}",
        date=today,
        description=description,
        sections=sections,
        markdown_content=markdown,
        item_count=len(extracted),
    )

    logger.info(f"Writing done: {article.title}, {article.item_count} items")
    return {"article": article}
