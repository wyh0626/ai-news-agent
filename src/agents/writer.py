"""Writer Agent - å°†æå–åçš„ç»“æ„åŒ–ä¿¡æ¯ç¼–æ’ä¸ºå¯è¯»çš„æ¯æ—¥ç®€æŠ¥"""

import logging
from collections import defaultdict
from datetime import datetime, timezone

from langchain_openai import ChatOpenAI

from src.config import settings
from src.graph.state import PipelineState
from src.models import ArticleSection, ExtractedItem, GeneratedArticle

logger = logging.getLogger(__name__)

WRITER_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI é¢†åŸŸæ–°é—»ç¼–è¾‘ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–°é—»æ¡ç›®ï¼Œæ’°å†™ä¸€ä»½ä¸­æ–‡æ¯æ—¥ AI æ–°é—»ç®€æŠ¥ã€‚

âš ï¸ ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸‹æ–¹æ¨¡æ¿æ ¼å¼è¾“å‡ºï¼Œä¸å¾—å¢å‡ç« èŠ‚ã€ä¸å¾—æ”¹å˜æ ‡é¢˜å±‚çº§ã€ä¸å¾—æ”¹å˜åˆ—è¡¨æ ¼å¼ã€‚

â”â”â”â”â”â” è¾“å‡ºæ¨¡æ¿ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ â”â”â”â”â”â”

# AI æ—¥æŠ¥ â€” {date}

> å…±æ”¶å½• {total_count} æ¡ AI é¢†åŸŸèµ„è®¯

## ğŸ”¥ ä»Šæ—¥ç„¦ç‚¹

ä»é‡ç‚¹æ–°é—»ä¸­é€‰å‡ºæœ€é‡è¦çš„ 2-3 æ¡ä½œä¸ºç„¦ç‚¹ï¼Œç”¨ ### ç¼–å·æ ‡é¢˜å±•å¼€ï¼š

### 1. æ–°é—»æ ‡é¢˜

2-3 å¥è¯çš„æ‘˜è¦åˆ†æï¼ŒåŒ…å«èƒŒæ™¯ä¸å½±å“ã€‚[åŸå§‹é“¾æ¥-æ¥æº](url)

### 2. æ–°é—»æ ‡é¢˜

2-3 å¥è¯çš„æ‘˜è¦åˆ†æã€‚[åŸå§‹é“¾æ¥-æ¥æº](url)

## ğŸ“° é‡ç‚¹æŠ¥é“

å°†å‰©ä½™é‡ç‚¹æ–°é—»æŒ‰è¯é¢˜åˆ†ç»„ï¼Œæ¯ç»„ä¸€ä¸ª ### æ ‡é¢˜ï¼Œç»„å†…ç”¨åˆ—è¡¨ï¼š

### è¯é¢˜åˆ†ç±»å

- **æ–°é—»æ ‡é¢˜** â€” ä¸€å¥è¯æ‘˜è¦ + ç®€è¦åˆ†æã€‚[åŸå§‹é“¾æ¥-æ¥æº](url)
- **æ–°é—»æ ‡é¢˜** â€” ä¸€å¥è¯æ‘˜è¦ã€‚[åŸå§‹é“¾æ¥-æ¥æº](url)

## âš¡ å¿«è®¯é€Ÿè§ˆ

æ‰€æœ‰å¿«è®¯ç”¨ç»Ÿä¸€çš„å•è¡Œåˆ—è¡¨ï¼Œä¸åˆ†ç»„ï¼š

- **æ–°é—»æ ‡é¢˜** â€” ä¸€å¥è¯æ¦‚æ‹¬ã€‚[åŸå§‹é“¾æ¥-æ¥æº](url)
- **æ–°é—»æ ‡é¢˜** â€” ä¸€å¥è¯æ¦‚æ‹¬ã€‚[åŸå§‹é“¾æ¥-æ¥æº](url)

---

*æœ¬æŠ¥å‘Šç”± AI News Agent è‡ªåŠ¨ç”Ÿæˆ | {date}*

â”â”â”â”â”â” æ¨¡æ¿ç»“æŸ â”â”â”â”â”â”

å†™ä½œè§„åˆ™ï¼š
1. ã€Œä»Šæ—¥ç„¦ç‚¹ã€åªæ”¾ 2-3 æ¡ï¼Œç”¨ ### ç¼–å·æ ‡é¢˜ï¼Œæ¯æ¡ 2-3 å¥åˆ†æ
2. ã€Œé‡ç‚¹æŠ¥é“ã€æŒ‰è¯é¢˜åˆ†ç»„ï¼ˆå¦‚"å¼€æºæ¨¡å‹""AI å®‰å…¨""å·¥å…·ä¸æ¡†æ¶"ç­‰ï¼‰ï¼Œç»„å†…ç”¨ - **æ ‡é¢˜** â€” æ‘˜è¦ æ ¼å¼
3. ã€Œå¿«è®¯é€Ÿè§ˆã€æ¯æ¡ä¸€è¡Œï¼Œ- **æ ‡é¢˜** â€” ä¸€å¥è¯ æ ¼å¼
4. æ¯æ¡æ–°é—»éƒ½å¿…é¡»é™„ [åŸå§‹é“¾æ¥-æ¥æº](url)ï¼Œå…¶ä¸­â€œæ¥æºâ€å–è‡ªç´ æä¸­çš„ source å­—æ®µï¼ˆå¦‚ xã€redditã€hackernewsã€arxivã€githubã€rssï¼‰
5. æ¥è‡ªä¸åŒæ•°æ®æºçš„åŒç±»æ–°é—»å¯åˆå¹¶å™è¿°
6. è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¢å‘ AI ä»ä¸šè€…
7. è¯·ç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼Œä¸è¦åŒ…è£¹åœ¨ä»£ç å—ä¸­

## é‡ç‚¹æ–°é—»ç´ æ (importance â‰¥ {threshold}ï¼Œå…± {featured_count} æ¡)
{featured_json}

## å¿«è®¯ç´ æ (å…± {brief_count} æ¡)
{brief_json}"""


def _build_llm() -> ChatOpenAI:
    kwargs = {
        "model": settings.openai_model,
        "max_tokens": 32768,
    }
    if settings.openai_api_key:
        kwargs["api_key"] = settings.openai_api_key
    if settings.openai_base_url:
        kwargs["base_url"] = settings.openai_base_url
    return ChatOpenAI(**kwargs)


def _group_by_topic(items: list[ExtractedItem]) -> dict[str, list[ExtractedItem]]:
    """æŒ‰ä¸»é¢˜èšç±»æ–°é—»"""
    groups: dict[str, list[ExtractedItem]] = defaultdict(list)
    for item in items:
        primary_topic = item.topics[0] if item.topics else "å…¶ä»–"
        groups[primary_topic].append(item)
    return dict(groups)


def _build_fallback_markdown(items: list[ExtractedItem], today: str) -> str:
    """LLM ä¸å¯ç”¨æ—¶çš„é™çº§æ¨¡æ¿æ¸²æŸ“ï¼Œä¸ LLM è¾“å‡ºæ ¼å¼ä¿æŒä¸€è‡´"""
    lines = [
        f"# AI æ—¥æŠ¥ â€” {today}",
        "",
        f"> å…±æ”¶å½• {len(items)} æ¡ AI é¢†åŸŸèµ„è®¯",
        "",
    ]

    # ä»Šæ—¥ç„¦ç‚¹ï¼šimportance >= 8 çš„å‰ 3 æ¡
    top_items = [i for i in items if i.importance_score >= 8][:3]
    if top_items:
        lines.append("## ğŸ”¥ ä»Šæ—¥ç„¦ç‚¹")
        lines.append("")
        for idx, item in enumerate(top_items, 1):
            summary = item.summary or item.title
            lines.append(f"### {idx}. {item.title}")
            lines.append("")
            src = item.source_type.value
            lines.append(f"{summary} [åŸå§‹é“¾æ¥-{src}]({item.url})")
            lines.append("")

    # é‡ç‚¹æŠ¥é“ï¼šæŒ‰è¯é¢˜åˆ†ç»„ï¼Œæ’é™¤å·²åœ¨ç„¦ç‚¹ä¸­å±•ç¤ºçš„
    top_ids = {id(i) for i in top_items}
    featured = [i for i in items if i.importance_score >= 7 and id(i) not in top_ids]
    if featured:
        groups = _group_by_topic(featured)
        lines.append("## ğŸ“° é‡ç‚¹æŠ¥é“")
        lines.append("")
        for topic, topic_items in groups.items():
            lines.append(f"### {topic}")
            lines.append("")
            for item in topic_items:
                summary = item.summary or item.title
                src = item.source_type.value
                lines.append(f"- **{item.title}** â€” {summary} [åŸå§‹é“¾æ¥-{src}]({item.url})")
            lines.append("")

    # å¿«è®¯é€Ÿè§ˆ
    brief = [i for i in items if i.importance_score < 7]
    if brief:
        lines.append("## âš¡ å¿«è®¯é€Ÿè§ˆ")
        lines.append("")
        for item in brief:
            summary = item.summary or item.title
            src = item.source_type.value
            lines.append(f"- **{item.title}** â€” {summary} [åŸå§‹é“¾æ¥-{src}]({item.url})")
        lines.append("")

    lines.append("---")
    lines.append("")
    lines.append(f"*æœ¬æŠ¥å‘Šç”± AI News Agent è‡ªåŠ¨ç”Ÿæˆ | {today}*")

    return "\n".join(lines)


def _normalize_source_links(markdown: str) -> str:
    """åå¤„ç†ï¼šç»Ÿä¸€æ‰€æœ‰é“¾æ¥æ ‡ç­¾ä¸º [åŸå§‹é“¾æ¥-æ¥æº](url) æ ¼å¼"""
    import re

    # åŸŸå â†’ æ¥æºæ ‡ç­¾æ˜ å°„
    domain_map = {
        "x.com": "x", "twitter.com": "x",
        "reddit.com": "reddit",
        "news.ycombinator.com": "hackernews", "hn.algolia.com": "hackernews",
        "arxiv.org": "arxiv",
        "github.com": "github",
        "huggingface.co": "huggingface",
    }

    def _get_source(url: str) -> str:
        for domain, label in domain_map.items():
            if domain in url:
                return label
        return "rss"

    # åŒ¹é…æ‰€æœ‰ markdown é“¾æ¥ [ä»»æ„æ–‡æœ¬](url)
    def replacer(m):
        text = m.group(1)
        url = m.group(2)
        source = _get_source(url)
        return f"[åŸå§‹é“¾æ¥-{source}]({url})"

    return re.sub(r'\[([^\]]*)\]\((https?://[^\)]+)\)', replacer, markdown)


def _items_to_json(items: list[ExtractedItem], brief: bool = False) -> str:
    """å°†æ–°é—»æ¡ç›®è½¬ä¸º JSON å­—ç¬¦ä¸²ï¼›brief=True æ—¶åªä¿ç•™å¿…è¦å­—æ®µä»¥èŠ‚çœ token"""
    import json

    data = []
    for item in items:
        if brief:
            data.append({
                "title": item.title,
                "source": item.source_type.value,
                "url": item.url,
            })
        else:
            data.append({
                "title": item.title,
                "summary": item.summary,
                "topics": item.topics,
                "importance_score": item.importance_score,
                "source": item.source_type.value,
                "url": item.url,
            })
    return json.dumps(data, ensure_ascii=False, indent=2)


def _split_featured_brief(
    items: list[ExtractedItem],
) -> tuple[list[ExtractedItem], list[ExtractedItem]]:
    """æŒ‰ importance_score å°†æ–°é—»åˆ†ä¸ºé‡ç‚¹å’Œç®€è¦ä¸¤å±‚

    items å·²æŒ‰ importance_score é™åºæ’åˆ—ï¼ˆç”± Extractor ä¿è¯ï¼‰
    """
    top_k = settings.top_k_featured
    featured = items[:top_k]
    brief = items[top_k:]
    return featured, brief


async def writer_node(state: PipelineState) -> dict:
    """Writer èŠ‚ç‚¹ï¼šç”Ÿæˆæ¯æ—¥æ–°é—»ç®€æŠ¥ï¼ˆé‡ç‚¹å±•å¼€ + å¿«è®¯é€Ÿè§ˆï¼‰"""
    extracted = state.get("extracted_items", [])
    if not extracted:
        logger.warning("æ²¡æœ‰æå–æ•°æ®ï¼Œè·³è¿‡æ’°ç¨¿")
        return {"article": None}

    today = datetime.now(tz=timezone.utc).strftime("%Y-%m-%d")
    featured, brief = _split_featured_brief(extracted)
    threshold = featured[-1].importance_score if featured else 0

    logger.info(
        f"å¼€å§‹æ’°å†™ {today} æ—¥æŠ¥: "
        f"é‡ç‚¹ {len(featured)} æ¡ + å¿«è®¯ {len(brief)} æ¡ "
        f"(åˆ†ç•Œçº¿ importance â‰¥ {threshold:.1f})"
    )

    groups = _group_by_topic(extracted)
    sections = [ArticleSection(topic=t, items=items) for t, items in groups.items()]

    # å°è¯•ç”¨ LLM ç”Ÿæˆ
    markdown = ""
    if settings.openai_api_key:
        try:
            llm = _build_llm()
            prompt = WRITER_PROMPT.format(
                date=today,
                total_count=len(extracted),
                featured_count=len(featured),
                brief_count=len(brief),
                threshold=threshold,
                featured_json=_items_to_json(featured),
                brief_json=_items_to_json(brief, brief=True),
            )
            resp = await llm.ainvoke(prompt)
            markdown = resp.content
            # å®Œæ•´æ€§æ£€æŸ¥ï¼šLLM å¯èƒ½æˆªæ–­ï¼Œç¼ºå°‘ç« èŠ‚åˆ™é™çº§
            required_sections = ["ä»Šæ—¥ç„¦ç‚¹", "å¿«è®¯é€Ÿè§ˆ"]
            missing = [s for s in required_sections if s not in markdown]
            if missing:
                logger.warning(f"LLM è¾“å‡ºä¸å®Œæ•´ï¼Œç¼ºå°‘ç« èŠ‚: {missing}ï¼Œé™çº§ä¸ºæ¨¡æ¿ç”Ÿæˆ")
                markdown = _build_fallback_markdown(extracted, today)
            else:
                # LLM è¾“å‡ºçš„é“¾æ¥æ ‡ç­¾å¯èƒ½ä¸ç»Ÿä¸€ï¼Œç”¨åå¤„ç†ä¿®æ­£
                markdown = _normalize_source_links(markdown)
                logger.info("LLM æ’°ç¨¿å®Œæˆ")
        except Exception as e:
            logger.error(f"LLM æ’°ç¨¿å¤±è´¥ï¼Œé™çº§ä¸ºæ¨¡æ¿ç”Ÿæˆ: {e}")
            markdown = _build_fallback_markdown(extracted, today)
    else:
        logger.info("æœªé…ç½® LLMï¼Œä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ")
        markdown = _build_fallback_markdown(extracted, today)

    article = GeneratedArticle(
        title=f"AI æ—¥æŠ¥ â€” {today}",
        date=today,
        sections=sections,
        markdown_content=markdown,
        item_count=len(extracted),
    )

    logger.info(f"æ’°ç¨¿å®Œæˆ: {article.title}, {article.item_count} æ¡æ–°é—»")
    return {"article": article}
