"""Trend Analyzer - åŸºäºé•¿æœŸè®°å¿†çš„è¶‹åŠ¿åˆ†æå’Œä¸“é¢˜æ·±åº¦æŠ¥å‘Šï¼ˆv0.4ï¼‰"""

import json
import logging
from datetime import datetime, timezone

from langchain_openai import ChatOpenAI

from src.config import settings

logger = logging.getLogger(__name__)

TREND_REPORT_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹è¿‡å» {days} å¤©çš„è¯é¢˜çƒ­åº¦æ•°æ®å’Œè¿‘æœŸæ–°é—»ï¼Œ
æ’°å†™ä¸€ä»½ä¸­æ–‡ AI è¡Œä¸šè¶‹åŠ¿åˆ†ææŠ¥å‘Šã€‚

## è¯é¢˜çƒ­åº¦æ•°æ® (æŒ‰çƒ­åº¦æ’åº)
{topics_json}

## è¿‘æœŸé«˜åˆ†æ–°é—»
{articles_json}

è¦æ±‚ï¼š
1. ç”¨ Markdown æ ¼å¼
2. åˆ†æ 3-5 ä¸ªæ ¸å¿ƒè¶‹åŠ¿
3. æ¯ä¸ªè¶‹åŠ¿ç»™å‡ºæ•°æ®æ”¯æ’‘å’Œç®€è¦åˆ†æ
4. æœ€åç»™å‡ºæœªæ¥ä¸€å‘¨çš„å…³æ³¨å»ºè®®
5. è¯­è¨€ä¸“ä¸šã€æ•°æ®é©±åŠ¨ï¼Œé¢å‘ AI ä»ä¸šè€…

è¯·ç›´æ¥è¾“å‡º Markdown å†…å®¹ã€‚"""

DEEPDIVE_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚è¯·å›´ç»•è¯é¢˜ã€Œ{topic}ã€æ’°å†™ä¸€ä»½æ·±åº¦åˆ†ææŠ¥å‘Šã€‚

## ç›¸å…³æ–°é—»
{articles_json}

è¦æ±‚ï¼š
1. ç”¨ Markdown æ ¼å¼
2. æ¦‚è¿°è¯é¢˜èƒŒæ™¯å’Œé‡è¦æ€§
3. æ¢³ç†å…³é”®äº‹ä»¶æ—¶é—´çº¿
4. åˆ†æå„æ–¹ï¼ˆå…¬å¸/ç ”ç©¶è€…ï¼‰çš„åŠ¨å‘
5. å±•æœ›æœªæ¥å‘å±•æ–¹å‘
6. è¯­è¨€ä¸“ä¸šï¼Œå¼•ç”¨å…·ä½“æ¥æº

è¯·ç›´æ¥è¾“å‡º Markdown å†…å®¹ã€‚"""


def _build_llm() -> ChatOpenAI:
    kwargs = {"model": settings.openai_model, "max_tokens": 16384}
    if settings.openai_api_key:
        kwargs["api_key"] = settings.openai_api_key
    if settings.openai_base_url:
        kwargs["base_url"] = settings.openai_base_url
    return ChatOpenAI(**kwargs)


async def generate_trend_report(days: int = 7) -> str | None:
    """ç”Ÿæˆå‘¨è¶‹åŠ¿åˆ†ææŠ¥å‘Š"""
    try:
        from src.storage.postgres import get_postgres

        pg = await get_postgres()
        if not pg.available:
            logger.warning("PostgreSQL ä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š")
            return None

        # è·å–è¯é¢˜çƒ­åº¦æ•°æ®
        trending = await pg.get_trending_topics(days=days, limit=20)
        if not trending:
            logger.info("æ²¡æœ‰è¶³å¤Ÿçš„è¯é¢˜æ•°æ®ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š")
            return None

        topics_json = json.dumps(trending, ensure_ascii=False, indent=2, default=str)

        # è·å–è¿‘æœŸé«˜åˆ†æ–‡ç« 
        articles = []
        async with pg._pool.connection() as conn:
            rows = await conn.execute(
                """SELECT title, summary, topics, source_type, url, importance_score
                   FROM article_index
                   WHERE created_at >= NOW() - INTERVAL '%s days'
                   ORDER BY importance_score DESC
                   LIMIT 30""",
                (days,),
            )
            async for row in rows:
                articles.append({
                    "title": row[0], "summary": row[1], "topics": row[2],
                    "source": row[3], "url": row[4], "score": row[5],
                })

        articles_json = json.dumps(articles, ensure_ascii=False, indent=2, default=str)

        if not settings.openai_api_key:
            return _fallback_trend_report(trending, articles, days)

        llm = _build_llm()
        prompt = TREND_REPORT_PROMPT.format(
            days=days, topics_json=topics_json, articles_json=articles_json
        )
        resp = await llm.ainvoke(prompt)
        return resp.content

    except Exception as e:
        logger.error(f"è¶‹åŠ¿æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return None


async def generate_deepdive_report(topic: str) -> str | None:
    """ç”Ÿæˆä¸“é¢˜æ·±åº¦æŠ¥å‘Š"""
    try:
        from src.storage.postgres import get_postgres

        pg = await get_postgres()
        if not pg.available:
            return None

        # è·å–è¯¥è¯é¢˜ç›¸å…³æ–‡ç« 
        async with pg._pool.connection() as conn:
            rows = await conn.execute(
                """SELECT title, summary, url, source_type, importance_score, published_at
                   FROM article_index
                   WHERE %s = ANY(topics)
                   ORDER BY published_at DESC
                   LIMIT 20""",
                (topic,),
            )
            articles = []
            async for row in rows:
                articles.append({
                    "title": row[0], "summary": row[1], "url": row[2],
                    "source": row[3], "score": row[4], "date": str(row[5]),
                })

        if not articles:
            return None

        articles_json = json.dumps(articles, ensure_ascii=False, indent=2, default=str)

        if not settings.openai_api_key:
            return f"# ä¸“é¢˜: {topic}\n\nå…± {len(articles)} ç¯‡ç›¸å…³æ–‡ç« ã€‚éœ€è¦ LLM ç”Ÿæˆæ·±åº¦åˆ†æã€‚"

        llm = _build_llm()
        prompt = DEEPDIVE_PROMPT.format(topic=topic, articles_json=articles_json)
        resp = await llm.ainvoke(prompt)
        return resp.content

    except Exception as e:
        logger.error(f"ä¸“é¢˜æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return None


def _fallback_trend_report(trending: list, articles: list, days: int) -> str:
    """LLM ä¸å¯ç”¨æ—¶çš„é™çº§è¶‹åŠ¿æŠ¥å‘Š"""
    lines = [
        f"# ğŸ“Š AI è¡Œä¸šè¶‹åŠ¿å‘¨æŠ¥",
        f"",
        f"> è¿‡å» {days} å¤©çš„è¯é¢˜çƒ­åº¦åˆ†æ",
        "",
        "## çƒ­é—¨è¯é¢˜ Top 10",
        "",
    ]
    for i, t in enumerate(trending[:10], 1):
        lines.append(f"{i}. **{t['topic']}** â€” å‡ºç° {t['count']} æ¬¡ï¼Œæ´»è·ƒ {t['active_days']} å¤©")
    lines.append("")
    lines.append("## é‡è¦æ–°é—»")
    lines.append("")
    for a in articles[:10]:
        lines.append(f"- [{a['title']}]({a.get('url', '')})")
    lines.append("")
    lines.append("---")
    lines.append(f"*éœ€è¦é…ç½® LLM ä»¥è·å–æ·±åº¦åˆ†æ*")
    return "\n".join(lines)
