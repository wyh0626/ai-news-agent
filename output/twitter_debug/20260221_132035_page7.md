[![](https://nitter.net/pic/list_banner_img%2F1613505644405075970%2F9VRDCFnW%3Fformat%3Djpg%26name%3Dorig)](https://nitter.net/pic/list_banner_img%2F1613505644405075970%2F9VRDCFnW%3Fformat%3Djpg%26name%3Dorig)

"AI High Signal" by @

AI twitter accounts that are high signal

- [Tweets](https://nitter.net/i/lists/1585430245762441216)
- [Members](https://nitter.net/i/lists/1585430245762441216/members)

[Load newest](https://nitter.net/i/lists/1585430245762441216)

[![](https://nitter.net/pic/profile_images%2F1728327996375719936%2FRW7VBJfD_bigger.jpg)](https://nitter.net/kimmonismus)

[Chubby‚ô®Ô∏è](https://nitter.net/kimmonismus "Chubby‚ô®Ô∏è")

[@kimmonismus](https://nitter.net/kimmonismus "@kimmonismus")

[19h](https://nitter.net/kimmonismus/status/2024898716365455459#m "Feb 20, 2026 ¬∑ 5:27 PM UTC")

Holy moly, what is happening inside those frontier labs?!

\- AGI very close - Superintelligence also not far off
\- watching "how fast the models accelerate internally" ASI is very close
\- a \*even faster\* take-off not very fast off

![](https://nitter.net/pic/amplify_video_thumb%2F2024898601454010368%2Fimg%2FZXK5PHOkahp0Dmpm.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

94

101

1,283

[![](https://nitter.net/pic/profile_images%2F1867875781676007424%2FRIF4Kt7U_bigger.jpg)](https://nitter.net/swyx)

[swyx](https://nitter.net/swyx "swyx")

[@swyx](https://nitter.net/swyx "@swyx")

[Feb 19](https://nitter.net/swyx/status/2024616612599587282#m "Feb 19, 2026 ¬∑ 10:46 PM UTC")

[x.com/i/article/202460118068‚Ä¶](http://x.com/i/article/2024601180689903616)

28

33

274

[![](https://nitter.net/pic/profile_images%2F1867875781676007424%2FRIF4Kt7U_bigger.jpg)](https://nitter.net/swyx)

[swyx](https://nitter.net/swyx "swyx")

[@swyx](https://nitter.net/swyx "@swyx")

[Feb 20](https://nitter.net/swyx/status/2024780514511794669#m "Feb 20, 2026 ¬∑ 9:37 AM UTC")

oh my god the utter disrespect to [@MistralAI](https://nitter.net/MistralAI "Mistral AI") ceo who has 100x better models and achievements than whatever sarvam put out

![](https://nitter.net/pic/amplify_video_thumb%2F2024435328745164801%2Fimg%2FQocJiW4NF-LuusgZ.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

14

1

128

[![](https://nitter.net/pic/profile_images%2F1867875781676007424%2FRIF4Kt7U_bigger.jpg)](https://nitter.net/swyx)

[swyx](https://nitter.net/swyx "swyx")

[@swyx](https://nitter.net/swyx "@swyx")

[19h](https://nitter.net/swyx/status/2024898670781759563#m "Feb 20, 2026 ¬∑ 5:27 PM UTC")

ok day 2 for [@sama](https://nitter.net/sama "Sam Altman") is looking much better better. glad he stayed behind a bit to talk to the little folk

[piped.video/embed/qH7thwrClu‚Ä¶](https://piped.video/embed/qH7thwrCluM)

3

5

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[19h](https://nitter.net/scaling01/status/2024898141883871709#m "Feb 20, 2026 ¬∑ 5:25 PM UTC")

As promised, I uploaded the code to a fork of the ARC-AGI-3-benchmarking repo:
\- [github.com/voice-from-the-ou‚Ä¶](https://github.com/voice-from-the-outer-world/arc-agi-3-benchmarking)

What I added:
\- my \`state-memory\` agent
\- Local live results viewer with playback and per-step details (action, reasoning, memory, full model input/output)
\- a silly download GIF button on the dashboard in case someone wants to share results of other models
\- a bunch of models incl. Gemini 3.1 Pro and Opus 4.6 via OpenRouter

Everything on how to use it is in the README.

Please don't ask me about any naming like why "state-memory" or why the new agent is in the adcr-agent's directory. It was all designed by the GPT-5.3-Codex gods. I don't dare to question their choices. Just want you to be able to try it out yourself.

This is expensive to run (at least for me). If you run all 3 games with 50 actions Gemini 3.1 Pro will set you back about $15-20. Opus-4.6-Thinking will set you back $35-45.

Smaller models like GLM-5, Kimi-K2.5 Thinking or Gemini-3 Flash are kinda pointless to run, but they will be $5-10 for the 50 steps for each of the 3 games.

I would love to see Opus 4.6 Thinking, GPT-5.2-xhigh and Gemini 3.1 Pro just run all the games until they get stuck or lose.

[![](https://nitter.net/pic/card_img%2F2024898144434003968%2FRAoYLneL%3Fformat%3Djpg%26name%3D800x419)\\
\\
**GitHub - voice-from-the-outer-world/arc-agi-3-benchmarking** \\
\\
Contribute to voice-from-the-outer-world/arc-agi-3-benchmarking development by creating an account on GitHub.\\
\\
github.com](https://github.com/voice-from-the-outer-world/arc-agi-3-benchmarking)

![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_mini.jpg)[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[Feb 20](https://nitter.net/scaling01/status/2024640940657246235#m "Feb 20, 2026 ¬∑ 12:23 AM UTC")

[x.com/i/article/202462313394‚Ä¶](http://x.com/i/article/2024623133945106432)

2

2

29

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[19h](https://nitter.net/scaling01/status/2024898634483990925#m "Feb 20, 2026 ¬∑ 5:27 PM UTC")

these are the start commands I used for the Gemini 3.1 Pro and Opus 4.6 results in the article:

uv run python cli/run\_all.py --agent state-memory --game\_list\_file games.txt --model\_configs claude-opus-4-6-openrouter --num\_plays 1 --max\_actions 50

uv run python cli/run\_all.py --agent state-memory --game\_list\_file games.txt --model\_configs gemini-3-1-pro-openrouter --num\_plays 1 --max\_actions 50

4

[![](https://nitter.net/pic/profile_images%2F1803953120923254784%2F_l4nw1xZ_bigger.jpg)](https://nitter.net/jparkerholder)

[Jack Parker-Holder](https://nitter.net/jparkerholder "Jack Parker-Holder")

[@jparkerholder](https://nitter.net/jparkerholder "@jparkerholder")

[19h](https://nitter.net/jparkerholder/status/2024896194380529892#m "Feb 20, 2026 ¬∑ 5:17 PM UTC")

Looks like a cool project, congrats [@DrJimFan](https://nitter.net/DrJimFan "Jim Fan") and team

![](https://nitter.net/pic/profile_images%2F1554922493101559808%2FSYSZhbcd_mini.jpg)[Jim Fan](https://nitter.net/DrJimFan "Jim Fan")

[@DrJimFan](https://nitter.net/DrJimFan "@DrJimFan")

[19h](https://nitter.net/DrJimFan/status/2024895359236051274#m "Feb 20, 2026 ¬∑ 5:14 PM UTC")

Announcing DreamDojo: our open-source, interactive world model that takes robot motor controls and generates the future in pixels. No engine, no meshes, no hand-authored dynamics. It's Simulation 2.0. Time for robotics to take the bitter lesson pill.

Real-world robot learning is bottlenecked by time, wear, safety, and resets. If we want Physical AI to move at pretraining speed, we need a simulator that adapts to pretraining scale with as little human engineering as possible.

Our key insights: (1) human egocentric videos are a scalable source of first-person physics; (2) latent actions make them "robot-readable" across different hardware; (3) real-time inference unlocks live teleop, policy eval, and test-time planning \*inside\* a dream.

We pre-train on 44K hours of human videos: cheap, abundant, and collected with zero robot-in-the-loop. Humans have already explored the combinatorics: we grasp, pour, fold, assemble, fail, retry‚Äîacross cluttered scenes, shifting viewpoints, changing light, and hour-long task chains‚Äîat a scale no robot fleet could match. The missing piece: these videos have no action labels. So we introduce latent actions: a unified representation inferred directly from videos that captures "what changed between world states" without knowing the underlying hardware. This lets us train on any first-person video as if it came with motor commands attached.

As a result, DreamDojo generalizes zero-shot to objects and environments never seen in any robot training set, because humans saw them first.

Next, we post-train onto each robot to fit its specific hardware. Think of it as separating "how the world looks and behaves" from "how this particular robot actuates." The base model follows the general physical rules, then "snaps onto" the robot's unique mechanics. It's kind of like loading a new character and scene assets into Unreal Engine, but done through gradient descent and generalizes far beyond the post-training dataset.

A world simulator is only useful if it runs fast enough to close the loop. We train a real-time version of DreamDojo that runs at 10 FPS, stable for over a minute of continuous rollout. This unlocks exciting possibilities:

\- Live teleoperation \*inside\* a dream. Connect a VR controller, stream actions into DreamDojo, and teleop a virtual robot in real time. We demo this on Unitree G1 with a PICO headset and one RTX 5090.
\- Policy evaluation. You can benchmark a policy checkpoint in DreamDojo instead of the real world. The simulated success rates strongly correlate with real-world results - accurate enough to rank checkpoints without burning a single motor.
\- Model-based planning. Sample multiple action proposals ‚Üí simulate them all in parallel ‚Üí pick the best future. Gains +17% real-world success out of the box on a fruit packing task.

We open-source everything!! Weights, code, post-training dataset, eval set, and whitepaper with tons of details to reproduce. DreamDojo is based on NVIDIA Cosmos, which is open-weight too.

2026 is the year of World Models for physical AI. We want you to build with us. Happy scaling!

Links in thread:

![](https://nitter.net/pic/amplify_video_thumb%2F2024870774126034944%2Fimg%2FamZpCnQ3_ZVF4D3d.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

1

1

27

[![](https://nitter.net/pic/profile_images%2F1554922493101559808%2FSYSZhbcd_bigger.jpg)](https://nitter.net/DrJimFan)

[Jim Fan](https://nitter.net/DrJimFan "Jim Fan")

[@DrJimFan](https://nitter.net/DrJimFan "@DrJimFan")

[19h](https://nitter.net/DrJimFan/status/2024896347195973765#m "Feb 20, 2026 ¬∑ 5:18 PM UTC")

Thanks Jack!! Our robotics version of GENIE-3! Your work has been an inspiration.

3

Stanford NLP Group retweeted

[![](https://nitter.net/pic/profile_images%2F1125242928639533057%2FSHyC9CpI_bigger.png)](https://nitter.net/SciFi)

[AI Papers](https://nitter.net/SciFi "AI Papers") [@SciFi](https://nitter.net/SciFi "@SciFi")

[Feb 20](https://nitter.net/SciFi/status/2024789206053032427#m "Feb 20, 2026 ¬∑ 10:12 AM UTC")

ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment

Hongjue Zhao, Haosen Sun, Jiangtao Kong, Xiaochang Li, Qineng Wang, Liwei Jiang, Qi Zhu, Tarek Abdelzaher, Yejin Choi, Manling Li, Huajie Shao
[arxiv.org/abs/2602.17560](https://arxiv.org/abs/2602.17560) \[ùöåùöú.ùô∞ùô∏\]

[![](https://nitter.net/pic/media%2FHBl_DadWAAEuIab.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBl_DadWAAEuIab.png)

2

6

Stanford NLP Group retweeted

[![](https://nitter.net/pic/profile_images%2F1988495095738626049%2FtAJnOA2q_bigger.jpg)](https://nitter.net/lcfreisi)

[Leonie Freisinger](https://nitter.net/lcfreisi "Leonie Freisinger")

[@lcfreisi](https://nitter.net/lcfreisi "@lcfreisi")

[Feb 20](https://nitter.net/lcfreisi/status/2024678366834807093#m "Feb 20, 2026 ¬∑ 2:51 AM UTC")

gave a lecture at Stanford today on building (almost) production-ready agents with pydantic-ai --> don‚Äôt over engineer your agents.

i demoed a toy SQL agent, some key ideas:
\- validate outputs + don‚Äôt complicate dependencies
\- use self-documenting tools & models
\- include a self-correcting mechanism (ModelRetry)
\- use Langfuse for tracing ( [@marcklingen](https://nitter.net/marcklingen "Marc Klingen"))

shout out to [@joewhaley](https://nitter.net/joewhaley "John Whaley") for the invite! love the project-based format of CS 244G (Building & Scaling LLM Applications)

[![](https://nitter.net/pic/media%2FHBkZ9aba4AApCfA.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBkZ9aba4AApCfA.jpg)

11

15

210

Zach Mueller retweeted

[![](https://nitter.net/pic/profile_images%2F378800000261649705%2Fbe9cc55e64014e6d7663c50d7cb9fc75_bigger.jpeg)](https://nitter.net/simonw)

[Simon Willison](https://nitter.net/simonw "Simon Willison")

[@simonw](https://nitter.net/simonw "@simonw")

[19h](https://nitter.net/simonw/status/2024895405146997002#m "Feb 20, 2026 ¬∑ 5:14 PM UTC")

Shared some thoughts on [ggml.ai](http://ggml.ai/) joining Hugging Face - they've been a good steward of the crucial Transformers open source library so I'm optimistic that great things are ahead for [ggml.ai](http://ggml.ai/), which kicked off the local model revolution back in March 2023 [simonwillison.net/2026/Feb/2‚Ä¶](https://simonwillison.net/2026/Feb/20/ggmlai-joins-hugging-face/)

11

14

150

CuddlySalmon retweeted

[![](https://nitter.net/pic/profile_images%2F1898919836308148224%2FHE5tIgc7_bigger.jpg)](https://nitter.net/TheCartelDel)

[Del](https://nitter.net/TheCartelDel "Del")

[@TheCartelDel](https://nitter.net/TheCartelDel "@TheCartelDel")

[Feb 20](https://nitter.net/TheCartelDel/status/2024700648353398833#m "Feb 20, 2026 ¬∑ 4:20 AM UTC")

"Roblox had 150 million daily users in Q3 2025. Its quarterly engagement is now equal to Steam, PlayStation, and Fortnite combined."

![](https://nitter.net/pic/profile_images%2F1434863337473458182%2FJckcZMpU_mini.jpg)[Matthew Ball](https://nitter.net/ballmatthew "Matthew Ball")

[@ballmatthew](https://nitter.net/ballmatthew "@ballmatthew")

[Feb 17](https://nitter.net/ballmatthew/status/2023559741373341763#m "Feb 17, 2026 ¬∑ 12:46 AM UTC")

My State of Video Games in 2026 is now out (Early Access)

1‚É£ Why revenue growth is more elusive than headlines suggest
2‚É£Why video gaming has been losing the attention war for a half decade (and to what)
3‚É£ Where there's indisputable growth

\+ Lots more

[matthewball.co/all/presentat‚Ä¶](https://www.matthewball.co/all/presentation-the-state-of-video-gaming-in-2026)

67

403

9,638

[![](https://nitter.net/pic/profile_images%2F1554922493101559808%2FSYSZhbcd_bigger.jpg)](https://nitter.net/DrJimFan)

[Jim Fan](https://nitter.net/DrJimFan "Jim Fan")

[@DrJimFan](https://nitter.net/DrJimFan "@DrJimFan")

[19h](https://nitter.net/DrJimFan/status/2024895359236051274#m "Feb 20, 2026 ¬∑ 5:14 PM UTC")

Announcing DreamDojo: our open-source, interactive world model that takes robot motor controls and generates the future in pixels. No engine, no meshes, no hand-authored dynamics. It's Simulation 2.0. Time for robotics to take the bitter lesson pill.

Real-world robot learning is bottlenecked by time, wear, safety, and resets. If we want Physical AI to move at pretraining speed, we need a simulator that adapts to pretraining scale with as little human engineering as possible.

Our key insights: (1) human egocentric videos are a scalable source of first-person physics; (2) latent actions make them "robot-readable" across different hardware; (3) real-time inference unlocks live teleop, policy eval, and test-time planning \*inside\* a dream.

We pre-train on 44K hours of human videos: cheap, abundant, and collected with zero robot-in-the-loop. Humans have already explored the combinatorics: we grasp, pour, fold, assemble, fail, retry‚Äîacross cluttered scenes, shifting viewpoints, changing light, and hour-long task chains‚Äîat a scale no robot fleet could match. The missing piece: these videos have no action labels. So we introduce latent actions: a unified representation inferred directly from videos that captures "what changed between world states" without knowing the underlying hardware. This lets us train on any first-person video as if it came with motor commands attached.

As a result, DreamDojo generalizes zero-shot to objects and environments never seen in any robot training set, because humans saw them first.

Next, we post-train onto each robot to fit its specific hardware. Think of it as separating "how the world looks and behaves" from "how this particular robot actuates." The base model follows the general physical rules, then "snaps onto" the robot's unique mechanics. It's kind of like loading a new character and scene assets into Unreal Engine, but done through gradient descent and generalizes far beyond the post-training dataset.

A world simulator is only useful if it runs fast enough to close the loop. We train a real-time version of DreamDojo that runs at 10 FPS, stable for over a minute of continuous rollout. This unlocks exciting possibilities:

\- Live teleoperation \*inside\* a dream. Connect a VR controller, stream actions into DreamDojo, and teleop a virtual robot in real time. We demo this on Unitree G1 with a PICO headset and one RTX 5090.
\- Policy evaluation. You can benchmark a policy checkpoint in DreamDojo instead of the real world. The simulated success rates strongly correlate with real-world results - accurate enough to rank checkpoints without burning a single motor.
\- Model-based planning. Sample multiple action proposals ‚Üí simulate them all in parallel ‚Üí pick the best future. Gains +17% real-world success out of the box on a fruit packing task.

We open-source everything!! Weights, code, post-training dataset, eval set, and whitepaper with tons of details to reproduce. DreamDojo is based on NVIDIA Cosmos, which is open-weight too.

2026 is the year of World Models for physical AI. We want you to build with us. Happy scaling!

Links in thread:

![](https://nitter.net/pic/amplify_video_thumb%2F2024870774126034944%2Fimg%2FamZpCnQ3_ZVF4D3d.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

55

125

819

[![](https://nitter.net/pic/profile_images%2F1554922493101559808%2FSYSZhbcd_bigger.jpg)](https://nitter.net/DrJimFan)

[Jim Fan](https://nitter.net/DrJimFan "Jim Fan")

[@DrJimFan](https://nitter.net/DrJimFan "@DrJimFan")

[19h](https://nitter.net/DrJimFan/status/2024895360922255776#m "Feb 20, 2026 ¬∑ 5:14 PM UTC")

\- Project website: [dreamdojo-world.github.io/](https://dreamdojo-world.github.io/)
\- Paper: [arxiv.org/abs/2602.06949](https://arxiv.org/abs/2602.06949)
\- Code repo and model ckpts: [github.com/NVIDIA/DreamDojo](https://github.com/NVIDIA/DreamDojo)

This is a huge team work at NVIDIA. All credits go to the wonderful teams who poured their hearts into it!

[![](https://nitter.net/pic/media%2FHBnfLMmbUAEwaXP.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnfLMmbUAEwaXP.jpg)

4

4

72

Stanford NLP Group retweeted

[![](https://nitter.net/pic/profile_images%2F1868845402835017728%2FEw50qGxC_bigger.jpg)](https://nitter.net/edzitron)

[Ed Zitron](https://nitter.net/edzitron "Ed Zitron")

[@edzitron](https://nitter.net/edzitron "@edzitron")

[Feb 20](https://nitter.net/edzitron/status/2024725617221259767#m "Feb 20, 2026 ¬∑ 5:59 AM UTC")

On two separate occasions Amazon‚Äôs Kiro AI assistant caused an AWS outage, one that was 13 hours long. Amazon blames this on ‚Äúuser error not AI error,‚Äù which is one of the most embarrassing things you could ever say as a human being

[![](https://nitter.net/pic/media%2FHBlFN9XbUAMB286.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBlFN9XbUAMB286.jpg)

[![](https://nitter.net/pic/media%2FHBlFN9XbUAIaID8.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBlFN9XbUAIaID8.jpg)

[![](https://nitter.net/pic/media%2FHBlFN9VaIAAuBSQ.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBlFN9VaIAAuBSQ.jpg)

![](https://nitter.net/pic/profile_images%2F1191943129823399936%2FZvfTRAlg_mini.jpg)[Techmeme](https://nitter.net/Techmeme "Techmeme")

[@Techmeme](https://nitter.net/Techmeme "@Techmeme")

[Feb 20](https://nitter.net/Techmeme/status/2024722364861468850#m "Feb 20, 2026 ¬∑ 5:46 AM UTC")

Sources: Amazon's AI tools caused at least two AWS outages, including a 13-hour disruption in December after its Kiro AI deleted and recreated an environment ( [@rafeuddin\_](https://nitter.net/rafeuddin_ "Rafe Uddin") / Financial Times)

[ft.com/content/00c282de-ed14‚Ä¶](https://www.ft.com/content/00c282de-ed14-4acd-a948-bc8d6bdb339d) [techmeme.com/260220/p1#a2602‚Ä¶](http://www.techmeme.com/260220/p1#a260220p1)

üì• Send tips! [techmeme.com/contact](https://techmeme.com/contact)

21

303

2,018

[![](https://nitter.net/pic/profile_images%2F1661187442043486209%2Fa3E4t1eV_bigger.jpg)](https://nitter.net/rasbt)

[Sebastian Raschka](https://nitter.net/rasbt "Sebastian Raschka")

[@rasbt](https://nitter.net/rasbt "@rasbt")

[19h](https://nitter.net/rasbt/status/2024886543630966917#m "Feb 20, 2026 ¬∑ 4:39 PM UTC")

February is one of those months...

\- Moonshot AI's Kimi K2.5 (Feb 2)
\- z. AI GLM 5 (Feb 12)
\- MiniMax M2.5 (Feb 12)
\- ByteDance Seed-2.0 (Feb 13)
\- Nanbeige 4.1 3B (Feb 13)
\- Qwen 3.5 (Feb 15)
\- Cohere's Tiny Aya (Feb 17)

(+Hopefully DeepSeek V4 soon)

Anything I forgot?

33

51

623

[![](https://nitter.net/pic/profile_images%2F1745893660099592193%2FMmYemsw6_bigger.jpg)](https://nitter.net/eliebakouch)

[elie](https://nitter.net/eliebakouch "elie")

[@eliebakouch](https://nitter.net/eliebakouch "@eliebakouch")

[19h](https://nitter.net/eliebakouch/status/2024893221545877734#m "Feb 20, 2026 ¬∑ 5:05 PM UTC")

Qwen3-Coder-Next
Step 3.5 flash
joyai flash
ling and ring v2.5
Intern-S1-Pro

1

3

37

[![](https://nitter.net/pic/profile_images%2F1661187442043486209%2Fa3E4t1eV_bigger.jpg)](https://nitter.net/rasbt)

[Sebastian Raschka](https://nitter.net/rasbt "Sebastian Raschka")

[@rasbt](https://nitter.net/rasbt "@rasbt")

[19h](https://nitter.net/rasbt/status/2024894517317439781#m "Feb 20, 2026 ¬∑ 5:10 PM UTC")

Ahhh Qwen3-Coder-Next... I actually looked into it (same architecture as Qwen3-Next I covered previously), so not sure why I forgot!

Thanks also for the other ones!

13

Graham Neubig retweeted

[![](https://nitter.net/pic/profile_images%2F2021416176610336768%2FFWPb-ujN_bigger.jpg)](https://nitter.net/ZhiruoW)

[Zora Wang](https://nitter.net/ZhiruoW "Zora Wang") [@ZhiruoW](https://nitter.net/ZhiruoW "@ZhiruoW")

[20h](https://nitter.net/ZhiruoW/status/2024880428612546655#m "Feb 20, 2026 ¬∑ 4:14 PM UTC")

Most agents either run fully autonomously or interrupt at the wrong times.
What if agents know when YOU want to step in?

üöÄIntroducing PlowPilot - a web agent that adapts to your interaction patterns
achieving +26.5% user-reported usefulness

Huge credit to [@FariaHuqOaishi](https://nitter.net/FariaHuqOaishi "Faria Huq | ü¶ã: fariahuqoaishi") for leading this project!

[![](https://nitter.net/pic/media%2FHBnQdNzWAAAjwEx.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnQdNzWAAAjwEx.jpg)

4

24

94

Aravind Srinivas retweeted

[![](https://nitter.net/pic/profile_images%2F1648535580870078465%2FOtGyTOWY_bigger.jpg)](https://nitter.net/Mr_Derivatives)

[Heisenberg](https://nitter.net/Mr_Derivatives "Heisenberg")

[@Mr\_Derivatives](https://nitter.net/Mr_Derivatives "@Mr_Derivatives")

[Feb 20](https://nitter.net/Mr_Derivatives/status/2024809372061409285#m "Feb 20, 2026 ¬∑ 11:32 AM UTC")

Perplexity Finance > Google Finance.

AI battle in the financial sector heating up‚Ä¶

![](https://nitter.net/pic/profile_images%2F1615440606456864769%2FFgBZT7uE_mini.jpg)[Jeff Grimes](https://nitter.net/jeffgrimes9 "Jeff Grimes")

[@jeffgrimes9](https://nitter.net/jeffgrimes9 "@jeffgrimes9")

[Feb 19](https://nitter.net/jeffgrimes9/status/2024605091832180835#m "Feb 19, 2026 ¬∑ 10:00 PM UTC")

Perplexity Finance now includes tap-through auditability to SEC filings. The filing is pre-scrolled to the page where that line item appears.

![](https://nitter.net/pic/amplify_video_thumb%2F2024604071425142784%2Fimg%2FHqNWDsYkDe6oZ49Z.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

37

37

573

[![](https://nitter.net/pic/profile_images%2F1115600347%2Fdrawing-me_bigger.png)](https://nitter.net/gneubig)

[Graham Neubig](https://nitter.net/gneubig "Graham Neubig")

[@gneubig](https://nitter.net/gneubig "@gneubig")

[19h](https://nitter.net/gneubig/status/2024893532557955344#m "Feb 20, 2026 ¬∑ 5:06 PM UTC")

Our Agent Data Protocol dataset now has 3.2M instances, double its original size!

Also the paper was accepted as an ICLR oral presentation!

Multimodal support coming soon as well.

[agentdataprotocol.com/](https://www.agentdataprotocol.com/)

![](https://nitter.net/pic/profile_images%2F1732388549260103680%2F1DlPnqVz_mini.jpg)[Yueqi Song](https://nitter.net/yueqi_song "Yueqi Song")

[@yueqi\_song](https://nitter.net/yueqi_song "@yueqi_song")

[19h](https://nitter.net/yueqi_song/status/2024890477133131890#m "Feb 20, 2026 ¬∑ 4:54 PM UTC")

Updates:

Excited to share that Agent Data Protocol (ADP) is accepted to ICLR 2026 Oral! üéâ

We also added support for 3 new datasets: SWE-Play, MiniCoder, and Toucan, bringing us to 3M trajectories supported.

If you're training agentic LMs, try ADP + tell us what dataset/agent format you want next. PRs & requests welcome. Let's make this the open standard for agent training data üî•

üöÄOriginal post: [nitter.net/yueqi\_song/status/1983‚Ä¶](https://nitter.net/yueqi_song/status/1983539504385253684)
üìÑRead our paper: [arxiv.org/abs/2510.24702](https://arxiv.org/abs/2510.24702)
üåêCheck our project website: [agentdataprotocol.com](https://agentdataprotocol.com/)

1

6

30

[![](https://nitter.net/pic/profile_images%2F1664559115581145088%2FUMD1vtMw_bigger.jpg)](https://nitter.net/vikhyatk)

[vik](https://nitter.net/vikhyatk "vik")

[@vikhyatk](https://nitter.net/vikhyatk "@vikhyatk")

[19h](https://nitter.net/vikhyatk/status/2024892638491779168#m "Feb 20, 2026 ¬∑ 5:03 PM UTC")

if a developer reads a stackoverflow post and decides to delete and recreate a production stack, would you blame stackoverflow for causing the outage?

![](https://nitter.net/pic/profile_images%2F1868845402835017728%2FEw50qGxC_mini.jpg)[Ed Zitron](https://nitter.net/edzitron "Ed Zitron")

[@edzitron](https://nitter.net/edzitron "@edzitron")

[Feb 20](https://nitter.net/edzitron/status/2024725617221259767#m "Feb 20, 2026 ¬∑ 5:59 AM UTC")

On two separate occasions Amazon‚Äôs Kiro AI assistant caused an AWS outage, one that was 13 hours long. Amazon blames this on ‚Äúuser error not AI error,‚Äù which is one of the most embarrassing things you could ever say as a human being

[![](https://nitter.net/pic/media%2FHBlFN9XbUAMB286.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBlFN9XbUAMB286.jpg)

[![](https://nitter.net/pic/media%2FHBlFN9XbUAIaID8.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBlFN9XbUAIaID8.jpg)

[![](https://nitter.net/pic/media%2FHBlFN9VaIAAuBSQ.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBlFN9VaIAAuBSQ.jpg)

3

37

[![](https://nitter.net/pic/profile_images%2F2017015061454438400%2FiNKfXZ_I_bigger.jpg)](https://nitter.net/arena)

[Arena.ai](https://nitter.net/arena "Arena.ai")

[@arena](https://nitter.net/arena "@arena")

[19h](https://nitter.net/arena/status/2024892330743124246#m "Feb 20, 2026 ¬∑ 5:02 PM UTC")

üìäLet‚Äôs dive deeper into [@AnthropicAI](https://nitter.net/AnthropicAI "Anthropic")'s Sonnet 4.6 vs 4.5. Overall: Sonnet 4.6 ranks 3 places higher (#13 vs #16)

Where Sonnet 4.6 gains:

Code:
‚ñ™Ô∏èWebDev (+19 for Sonnet 4.6: #3 vs #22)

Text:
‚ñ™Ô∏èInstruction Following (+6, #5 vs #11)
‚ñ™Ô∏èEnglish (+5, #9 vs #14)
‚ñ™Ô∏èHard Prompts (+5, #9 vs #14)

Occupational:
‚ñ™Ô∏èSoftware & IT Services (+8, #8 vs #16)
‚ñ™Ô∏èWriting, Literature, & Language (+4, #7 vs #11)
‚ñ™Ô∏èBusiness, Management, & Financial Operations (+3, #10 vs #13)

Where Sonnet 4.5 is still ranking higher:

Text:
‚ñ™Ô∏èMulti-Turn (4.5 leads by +3: #8 vs #11)
‚ñ™Ô∏èLonger Query (4.5 leads by +2: #8 vs #10)

(note: these are calculated with style control).

[![](https://nitter.net/pic/media%2FHBnXvs1bAAAyAAd.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnXvs1bAAAyAAd.jpg)

![](https://nitter.net/pic/profile_images%2F2017015061454438400%2FiNKfXZ_I_mini.jpg)[Arena.ai](https://nitter.net/arena "Arena.ai")

[@arena](https://nitter.net/arena "@arena")

[20h](https://nitter.net/arena/status/2024883614249615394#m "Feb 20, 2026 ¬∑ 4:27 PM UTC")

Claude Sonnet 4.6 has landed #3 in Code and #13 in Text Arena!

Highlights:
‚ñ™Ô∏è+130 pts jump in Code Arena (#22 -> #3) compared to Sonnet 4.5, surpassing top-tier thinking models like Gemini-3.1 and GPT-5.2

‚ñ™Ô∏èStrong gains in Text categories: Math (#4) and Instruction Following (#5), Overall (#13)

Congrats to the [@AnthropicAI](https://nitter.net/AnthropicAI "Anthropic") team on another impressive milestone!

[![](https://nitter.net/pic/media%2FHBnQqe6bkAA1Tpd.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnQqe6bkAA1Tpd.jpg)

9

9

142

[![](https://nitter.net/pic/profile_images%2F2017015061454438400%2FiNKfXZ_I_bigger.jpg)](https://nitter.net/arena)

[Arena.ai](https://nitter.net/arena "Arena.ai")

[@arena](https://nitter.net/arena "@arena")

[19h](https://nitter.net/arena/status/2024892333452656898#m "Feb 20, 2026 ¬∑ 5:02 PM UTC")

Dig into the Arena leaderboard details for all the top frontier AI at: [arena.ai/leaderboard](http://arena.ai/leaderboard)

4

[![](https://nitter.net/pic/profile_images%2F1924485072801140736%2FVyZekL_z_bigger.jpg)](https://nitter.net/basetenco)

[Baseten](https://nitter.net/basetenco "Baseten")

[@basetenco](https://nitter.net/basetenco "@basetenco")

[19h](https://nitter.net/basetenco/status/2024891915637063714#m "Feb 20, 2026 ¬∑ 5:00 PM UTC")

"No other product lets you launch ten different training jobs on four different datasets." ‚ÄìHead of Clinical NLP, OpenEvidence

Over 40% of U.S. physicians trust [@EvidenceOpen](https://nitter.net/EvidenceOpen "OpenEvidence")'s platform for fast, accurate medical information. Their secret: custom, specialized models built on Baseten Training.

Here's how we helped them save $1.9M via model training and improved their latency 23x to power 100M+ clinical consultations per year.

[baseten.co/resources/custome‚Ä¶](https://www.baseten.co/resources/customers/openevidence-baseten-training)

[![](https://nitter.net/pic/media%2FHBncdwNaAAAABFV.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBncdwNaAAAABFV.jpg)

6

26

merve retweeted

[![](https://nitter.net/pic/profile_images%2F1991559933473497089%2FmbrRS49P_bigger.jpg)](https://nitter.net/huggingface)

[Hugging Face](https://nitter.net/huggingface "Hugging Face")

[@huggingface](https://nitter.net/huggingface "@huggingface")

[20h](https://nitter.net/huggingface/status/2024871487753044243#m "Feb 20, 2026 ¬∑ 3:39 PM UTC")

Thrilled to have GGML with us going forward! ü§ó‚ù§Ô∏èü¶ô

Read the announcement blog [huggingface.co/blog/ggml-joi‚Ä¶](https://huggingface.co/blog/ggml-joins-hf)

[![](https://nitter.net/pic/media%2FHBnJ4vCW8AAmubV.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnJ4vCW8AAmubV.jpg)

9

27

173

[![](https://nitter.net/pic/profile_images%2F1672359156391591939%2FKrJs4zUQ_bigger.jpg)](https://nitter.net/imjaredz)

[Jared Zoneraich](https://nitter.net/imjaredz "Jared Zoneraich")

[@imjaredz](https://nitter.net/imjaredz "@imjaredz")

[19h](https://nitter.net/imjaredz/status/2024889987833933943#m "Feb 20, 2026 ¬∑ 4:52 PM UTC")

Uniquely American story

Americans excel at doing fun things

The best of our country do it for the love of the game

![](https://nitter.net/pic/profile_images%2F1927795530920239104%2Frv8FxhKB_mini.jpg)[Brad Stulberg](https://nitter.net/BStulberg "Brad Stulberg")

[@BStulberg](https://nitter.net/BStulberg "@BStulberg")

[Feb 19](https://nitter.net/BStulberg/status/2024628741910196463#m "Feb 19, 2026 ¬∑ 11:34 PM UTC")

Joy is a competitive super power.

Alysa Liu retired from figure skating at 16.
She was tired of not not having fun, tired of being consumed by her sport.

She came back two years later with a new goal: to have as much fun on the ice as possible. And now she‚Äôs an Olympic gold medalist.

Liu won her first national title when she was just 13. But by 16, after competing in the 2022 Olympics, she decided she‚Äôd had enough and stepped away. She said pressure and losing her identity trying to be an elite athlete made it all miserable.

But then, she said she went on a ski trip that reminded her just how much fun she could have doing a sport. Something in her brain clicked. Maybe she could bring fun to figure skating. Maybe she could approach it in a way that could be full of joy and life and love.

She unretired at 18 and won a world championship the next year. At 20, she was ready to face these Olympic games differently than in 2022.

Liu went into the women‚Äôs figure skating final in third place. After her short program, she said:

‚ÄúEven if I mess up and fall, that‚Äôs totally okay, too. I‚Äôm fine with any outcome, as long as I‚Äôm out there.‚Äù

One of the greatest competitive advantages is having fun. People love to romanticize the athlete, artist, or entrepreneur who has a chip on their shoulder, fueled by anger and resentment.

But the truth is that if you‚Äôre not having fun, you are not going to last long at whatever it is you do, and you certainly won‚Äôt get the best out of yourself. There‚Äôs a foolish idea that you either have to be full of intensity or full of joy. But that‚Äôs nonsense.

It‚Äôs no surprise one of the first things out of Alysa‚Äôs mouth after her free skate was: ‚ÄúThat was so much fun!‚Äù

Joy and intensity can coexist, and in the best performers, they almost always do.

Alysa is unapologetically authentic and true to her values. She has said where she used to skate to win and be technically perfect, she now uses competition as a chance to show her art, to have fun, and to put herself out there.

She‚Äôs a fierce athlete with an infectious sense of joy in her sport.

And she broke USA's 24-year gold medal draught in women‚Äôs figure skating doing it.

Excellence requires focus, determination, a little bit of crazy, at times obsession, and living a mundane lifestyle that many people would find boring.

But excellence also requires that you find deep joy in your craft, that you learn how to have fun while working hard.

What makes for excellence‚Äîand not just in sports, but in anything‚Äîis the combination of intensity and joy. It‚Äôs the latter that makes the former sustainable.

[![](https://nitter.net/pic/media%2FHBjolYDbUAIJFR4.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBjolYDbUAIJFR4.jpg)

4

[![](https://nitter.net/pic/profile_images%2F1831321531852496896%2F1yBZG884_bigger.jpg)](https://nitter.net/_philschmid)

[Philipp Schmid](https://nitter.net/_philschmid "Philipp Schmid")

[@\_philschmid](https://nitter.net/_philschmid "@_philschmid")

[19h](https://nitter.net/_philschmid/status/2024889903293563220#m "Feb 20, 2026 ¬∑ 4:52 PM UTC")

ICYMI Gemini 3.1 Pro Preview is available on the Gemini Interactions API.

[ai.google.dev/gemini-api/doc‚Ä¶](https://ai.google.dev/gemini-api/docs/interactions?ua=chat#basic-interactions)

[![](https://nitter.net/pic/media%2FHBnanFwXgAA9gwO.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnanFwXgAA9gwO.jpg)

6

4

63

Jared Zoneraich retweeted

[![](https://nitter.net/pic/profile_images%2F1905396692809052160%2Fl0oreOJh_bigger.jpg)](https://nitter.net/MarkMBissell)

[mark bissell](https://nitter.net/MarkMBissell "mark bissell")

[@MarkMBissell](https://nitter.net/MarkMBissell "@MarkMBissell")

[20h](https://nitter.net/MarkMBissell/status/2024876447962448186#m "Feb 20, 2026 ¬∑ 3:59 PM UTC")

to win gold medals and nobel prizes you just need to be funmaxxing

[![](https://nitter.net/pic/media%2FHBlT7DCbUAUkYtz.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBlT7DCbUAUkYtz.jpg)

![](https://nitter.net/pic/profile_images%2F1927795530920239104%2Frv8FxhKB_mini.jpg)[Brad Stulberg](https://nitter.net/BStulberg "Brad Stulberg")

[@BStulberg](https://nitter.net/BStulberg "@BStulberg")

[Feb 19](https://nitter.net/BStulberg/status/2024628741910196463#m "Feb 19, 2026 ¬∑ 11:34 PM UTC")

Joy is a competitive super power.

Alysa Liu retired from figure skating at 16.
She was tired of not not having fun, tired of being consumed by her sport.

She came back two years later with a new goal: to have as much fun on the ice as possible. And now she‚Äôs an Olympic gold medalist.

Liu won her first national title when she was just 13. But by 16, after competing in the 2022 Olympics, she decided she‚Äôd had enough and stepped away. She said pressure and losing her identity trying to be an elite athlete made it all miserable.

But then, she said she went on a ski trip that reminded her just how much fun she could have doing a sport. Something in her brain clicked. Maybe she could bring fun to figure skating. Maybe she could approach it in a way that could be full of joy and life and love.

She unretired at 18 and won a world championship the next year. At 20, she was ready to face these Olympic games differently than in 2022.

Liu went into the women‚Äôs figure skating final in third place. After her short program, she said:

‚ÄúEven if I mess up and fall, that‚Äôs totally okay, too. I‚Äôm fine with any outcome, as long as I‚Äôm out there.‚Äù

One of the greatest competitive advantages is having fun. People love to romanticize the athlete, artist, or entrepreneur who has a chip on their shoulder, fueled by anger and resentment.

But the truth is that if you‚Äôre not having fun, you are not going to last long at whatever it is you do, and you certainly won‚Äôt get the best out of yourself. There‚Äôs a foolish idea that you either have to be full of intensity or full of joy. But that‚Äôs nonsense.

It‚Äôs no surprise one of the first things out of Alysa‚Äôs mouth after her free skate was: ‚ÄúThat was so much fun!‚Äù

Joy and intensity can coexist, and in the best performers, they almost always do.

Alysa is unapologetically authentic and true to her values. She has said where she used to skate to win and be technically perfect, she now uses competition as a chance to show her art, to have fun, and to put herself out there.

She‚Äôs a fierce athlete with an infectious sense of joy in her sport.

And she broke USA's 24-year gold medal draught in women‚Äôs figure skating doing it.

Excellence requires focus, determination, a little bit of crazy, at times obsession, and living a mundane lifestyle that many people would find boring.

But excellence also requires that you find deep joy in your craft, that you learn how to have fun while working hard.

What makes for excellence‚Äîand not just in sports, but in anything‚Äîis the combination of intensity and joy. It‚Äôs the latter that makes the former sustainable.

[![](https://nitter.net/pic/media%2FHBjolYDbUAIJFR4.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBjolYDbUAIJFR4.jpg)

4

205

2,187

Stanford NLP Group retweeted

[![](https://nitter.net/pic/profile_images%2F1600839794837229568%2FjeUmyEuD_bigger.jpg)](https://nitter.net/EchoShao8899)

[Yijia Shao](https://nitter.net/EchoShao8899 "Yijia Shao") [@EchoShao8899](https://nitter.net/EchoShao8899 "@EchoShao8899")

[Feb 19](https://nitter.net/EchoShao8899/status/2024279651984822754#m "Feb 19, 2026 ¬∑ 12:27 AM UTC")

The second episode of [@augmind\_fm](https://nitter.net/augmind_fm "Augmented Mind Podcast") will drop next week! We're also hosting an in-person Discussion + Watch Party on Tuesday, Feb 24 (11am‚Äì1pm) at Stanford's Gates Building ‚Äî with food and swag!

In this episode, we're honored to welcome [@tongshuangwu](https://nitter.net/tongshuangwu "Sherry Tongshuang Wu") as our guest. I'm super excited about this episode - Sherry shared her experience navigating the past few years as a faculty member at CMU through the whirlwind of AI, and her research journey from building AI systems that account for imperfect models to those that account for imperfect humans.

If you're into technical human-centered AI and want a low-key way to meet others building in the space, come hang out! RSVP link + Event schedule belowüëá

![](https://nitter.net/pic/profile_images%2F2013987640949837824%2FYMR5g2nJ_mini.jpg)[Augmented Mind Podcast](https://nitter.net/augmind_fm "Augmented Mind Podcast")

[@augmind\_fm](https://nitter.net/augmind_fm "@augmind_fm")

[Feb 18](https://nitter.net/augmind_fm/status/2024263977061249199#m "Feb 18, 2026 ¬∑ 11:25 PM UTC")

üß†üéôÔ∏è We‚Äôre co-hosting an Augmented Mind Podcast Meetup w/ a16z ‚Äî Tue Feb 24 (11‚Äì1) @ Gates CS (Stanford)!

If you‚Äôre into technical human-centered AI and want an easy, low-pressure way to meet others building in the space, come hang out!

üîóLink to RSVP Below

1

3

21

[![](https://nitter.net/pic/profile_images%2F1803756314469847040%2FXn7-ka2P_bigger.jpg)](https://nitter.net/TheZachMueller)

[Zach Mueller](https://nitter.net/TheZachMueller "Zach Mueller")

[@TheZachMueller](https://nitter.net/TheZachMueller "@TheZachMueller")

[19h](https://nitter.net/TheZachMueller/status/2024889305563340871#m "Feb 20, 2026 ¬∑ 4:50 PM UTC")

I believe it. Of the new model cards, Minimax has had the most consistent traction and highest overall views.

(And also continues to run my OpenClaw upstairs ü§ó)

![](https://nitter.net/pic/profile_images%2F1875100548535574529%2FVxHk9HyU_mini.jpg)[MiniMax (official)](https://nitter.net/MiniMax_AI "MiniMax (official)")

[@MiniMax\_AI](https://nitter.net/MiniMax_AI "@MiniMax_AI")

[20h](https://nitter.net/MiniMax_AI/status/2024877880971554850#m "Feb 20, 2026 ¬∑ 4:04 PM UTC")

3-TRILLION in one weekü´®üöÄ

3

2

283

Stanford NLP Group retweeted

[![](https://nitter.net/pic/profile_images%2F1539123956950700035%2FDLTn2gEX_bigger.jpg)](https://nitter.net/RishiBommasani)

[rishi](https://nitter.net/RishiBommasani "rishi")

[@RishiBommasani](https://nitter.net/RishiBommasani "@RishiBommasani")

[Feb 20](https://nitter.net/RishiBommasani/status/2024660499955327176#m "Feb 20, 2026 ¬∑ 1:40 AM UTC")

I confess that I have often been skeptical of LM-driven biorisk (for several reasons), though risk from biological design tools seems more plausible to me.

My skepticism is heavily founded in the paucity of open research on AI-mediated biorisk. The lack of openness and transparency is often justified by standard security arguments about info hazards. For example, back in 2023, Anthropic published their mysterious redteaming exercise about "Findings from red teaming biology" that said nothing of substantial value for an external observer.
[anthropic.com/news/frontier-‚Ä¶](https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety)
While there are circumstances that necessitate limited transparency, I often found the tradeoffs back then on information sharing to be poorly managed. Overall, it seemed to inherit the posture of much of national security, which as a computer scientist often seems like "security through obscurity". Security through obscurity has its place. But in a nascent area, I don't think such a conservative posture on sharing information is likely to be right.

My views on the substantive matter of AI biorisk have changed some in recent years. Companies have done better some in saying more, and engaging external evaluators, and governments have improved capacity. And over the year, passing interactions with Luca and reading his work has shaped my views significantly on this issue.

I am really glad he pursues this issue with the true candor of an earnest scientist. This is what we need. I hope the way he conducts his research can pervade through this area because we would have much more credible science on AI-mediated biorisk if so. And that would help forge expert consensus on the current level of risk in this domain, which I feel has often been lacking precisely because the public evidence in this domain has been so lackluster.

[![](https://nitter.net/pic/card_img%2F2023729772488949760%2FkXbMtfnm%3Fformat%3Djpg%26name%3D800x419)\\
\\
**Frontier Threats Red Teaming for AI Safety** \\
\\
Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\\
\\
anthropic.com](https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety)

![](https://nitter.net/pic/profile_images%2F1498300461127925762%2F3Rym6bcf_mini.jpg)[Luca Righetti](https://nitter.net/lucafrighetti "Luca Righetti")

[@lucafrighetti](https://nitter.net/lucafrighetti "@lucafrighetti")

[Feb 19](https://nitter.net/lucafrighetti/status/2024608022103052645#m "Feb 19, 2026 ¬∑ 10:12 PM UTC")

Sharing 3 bits of work today!

üü† [@ActiveSiteBio](https://nitter.net/ActiveSiteBio "Active Site"): RCT finds AIs help novices at wet-lab steps but not end-to-end success

üü£ [@Research\_FRI](https://nitter.net/Research_FRI "Forecasting Research Institute"): This surprised experts. If future AIs 5X success biorisk may rise 2X

üü¢ [@METR\_Evals](https://nitter.net/METR_Evals "METR"): My case for in-depth studies amid hectic LLM releases

[![](https://nitter.net/pic/media%2FHBjaQv7bUAE2CXF.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBjaQv7bUAE2CXF.jpg)

6

42

Lisan al Gaib retweeted

[![](https://nitter.net/pic/profile_images%2F1904982870688411648%2Fi0X0WCJR_bigger.jpg)](https://nitter.net/Clad3815)

[Clad3815](https://nitter.net/Clad3815 "Clad3815")

[@Clad3815](https://nitter.net/Clad3815 "@Clad3815")

[20h](https://nitter.net/Clad3815/status/2024882063992606798#m "Feb 20, 2026 ¬∑ 4:21 PM UTC")

üì¢ PokeBench is now open source, LLMs playing real Pokemon Stadium 2 battles on the N64.

Last year, when o1 and o4-mini dropped, I wanted a fun way to stress-test how reasoning models actually think under pressure. Then I saw this tweet from [@edwinarbus](https://nitter.net/edwinarbus "edwin") and knew exactly what to build.

So I built PokeBench, a platform that pits two LLMs against each other in actual Pokemon Stadium 2 matches running on the N64.

No simulation. Each AI reads live game memory, drafts a team, picks moves, and sends real controller inputs through Project64. You can watch the entire battle unfold on a live dashboard and read exactly what each model is thinking on every turn.

After months of tweaking and optimizing, I'm releasing the whole thing as open source.

It supports OpenAI, Anthropic, Google Gemini, DeepSeek, Mistral, and OpenRouter out of the box. It's built on AI SDK, so adding a new provider takes minutes. You can configure reasoning effort, battle formats (Bo1 / Bo3 / Bo5), team sizes (3v3 / 6v6), and track stats across hundreds of matches.

Full tournament results from last year are in the replies. (Spoiler: GPT-4.5 took the crown)

Huge shoutout to [@Qualzz\_Sam](https://nitter.net/Qualzz_Sam "Qual") for the design system.
I'd love to see someone run a full benchmark across all the latest providers and models and share the results. (Looking at you, [@OpenRouter](https://nitter.net/OpenRouter "OpenRouter") )
Go run your own benchmarks! Link below.

[![](https://nitter.net/pic/media%2FHBnNU31XgAAdENo.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnNU31XgAAdENo.jpg)

[![](https://nitter.net/pic/media%2FHBnQGKiXUAApd8l.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnQGKiXUAApd8l.jpg)

8

3

44

[![](https://nitter.net/pic/profile_images%2F1728327996375719936%2FRW7VBJfD_bigger.jpg)](https://nitter.net/kimmonismus)

[Chubby‚ô®Ô∏è](https://nitter.net/kimmonismus "Chubby‚ô®Ô∏è")

[@kimmonismus](https://nitter.net/kimmonismus "@kimmonismus")

[19h](https://nitter.net/kimmonismus/status/2024887011522576766#m "Feb 20, 2026 ¬∑ 4:40 PM UTC")

Holy sh\*t: Sam Altman:

"The inside view at the companys of looking at what's going to happen - the \*world is not prepared.\* We're going to have extremely capable models soon. It's going to be a faster takeoff than I originally thought. And that is stressfull and anxiety inducing"

![](https://nitter.net/pic/amplify_video_thumb%2F2024886946443755520%2Fimg%2FMk9eYb-VfNPHOq4V.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

190

267

2,458

[![](https://nitter.net/pic/profile_images%2F1728327996375719936%2FRW7VBJfD_bigger.jpg)](https://nitter.net/kimmonismus)

[Chubby‚ô®Ô∏è](https://nitter.net/kimmonismus "Chubby‚ô®Ô∏è")

[@kimmonismus](https://nitter.net/kimmonismus "@kimmonismus")

[19h](https://nitter.net/kimmonismus/status/2024887023337878011#m "Feb 20, 2026 ¬∑ 4:41 PM UTC")

[piped.video/watch?v=qH7thwrC‚Ä¶](https://piped.video/watch?v=qH7thwrCluM)

[![](https://nitter.net/pic/card_img%2F2025074185152643073%2FwCk-IRN_%3Fformat%3Djpg%26name%3D800x320_1)\\
\\
**Sam Altman Unfiltered: ChatGPT, AI Risks & What‚Äôs Coming Next, 40...** \\
\\
Sam Altman Exclusive: Is AI Getting Dangerous? ChatGPT, AI Safety, Risks & the Future of AI : Sam Altman Exclusive: In this exclusive 60-minute interview, S...\\
\\
youtube.com](https://piped.video/watch?v=qH7thwrCluM)

4

71

[![](https://nitter.net/pic/profile_images%2F1803756314469847040%2FXn7-ka2P_bigger.jpg)](https://nitter.net/TheZachMueller)

[Zach Mueller](https://nitter.net/TheZachMueller "Zach Mueller")

[@TheZachMueller](https://nitter.net/TheZachMueller "@TheZachMueller")

[20h](https://nitter.net/TheZachMueller/status/2024884335468331338#m "Feb 20, 2026 ¬∑ 4:30 PM UTC")

Good sub thread by Robert on their work improving the oracle (what helps pick the best kernels) üëá

![](https://nitter.net/pic/profile_images%2F1626334950742974465%2FyPqmMi35_mini.jpg)[Robert Shaw](https://nitter.net/robertshaw21 "Robert Shaw") [@robertshaw21](https://nitter.net/robertshaw21 "@robertshaw21")

[20h](https://nitter.net/robertshaw21/status/2024881325212139996#m "Feb 20, 2026 ¬∑ 4:18 PM UTC")

Replying to [@robertshaw21](https://nitter.net/robertshaw21) [@gaunernst](https://nitter.net/gaunernst) [@TheZachMueller](https://nitter.net/TheZachMueller)

That being said, my and [@mgoin\_](https://nitter.net/mgoin_ "Michael Goin")‚Äôs goal with our recent MoE refactor work is to encode the best kernel selection into vLLM so that users do not need to manually tune. If you see anything that looks off with out of the box performance please open an issue or ping on slack!

4

[![](https://nitter.net/pic/profile_images%2F1890090485370265600%2FOvqR_8Qo_bigger.jpg)](https://nitter.net/leveredvlad)

[Ori Eldarov](https://nitter.net/leveredvlad "Ori Eldarov")

[@leveredvlad](https://nitter.net/leveredvlad "@leveredvlad")

[20h](https://nitter.net/leveredvlad/status/2024884312361812016#m "Feb 20, 2026 ¬∑ 4:30 PM UTC")

I think the most obvious near-term first-order impact of AI on investment banking is the loss of the entire "Analyst" role. At a minimum it will be merged with that of an "Associate".

Still wrapping my head around the pedagogy - a lot of the learning on the job was "wax on wax off". How do you learn if AI is doing the analysis?

11

40

[![](https://nitter.net/pic/profile_images%2F1620194266533199874%2FrCtE0hYR_bigger.jpg)](https://nitter.net/skirano)

[Pietro Schirano](https://nitter.net/skirano "Pietro Schirano")

[@skirano](https://nitter.net/skirano "@skirano")

[20h](https://nitter.net/skirano/status/2024875637878526109#m "Feb 20, 2026 ¬∑ 3:55 PM UTC")

Gemini 3.1 Pro is the best model in the world for going from image to code.

This task is basically solved now, kind of crazy.

The model is now available in [@MagicPathAI](https://nitter.net/MagicPathAI "MagicPath").

![](https://nitter.net/pic/amplify_video_thumb%2F2024875451278135296%2Fimg%2FU4F5UZl1QIm4Gisi.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

59

73

1,378

[![](https://nitter.net/pic/profile_images%2F1831321531852496896%2F1yBZG884_bigger.jpg)](https://nitter.net/_philschmid)

[Philipp Schmid](https://nitter.net/_philschmid "Philipp Schmid")

[@\_philschmid](https://nitter.net/_philschmid "@_philschmid")

[20h](https://nitter.net/_philschmid/status/2024883973445374057#m "Feb 20, 2026 ¬∑ 4:28 PM UTC")

Really nice example!

1

1

11

[![](https://nitter.net/pic/profile_images%2F1287206199088173057%2FixE4fKy1_bigger.jpg)](https://nitter.net/HamelHusain)

[Hamel Husain](https://nitter.net/HamelHusain "Hamel Husain")

[@HamelHusain](https://nitter.net/HamelHusain "@HamelHusain")

[20h](https://nitter.net/HamelHusain/status/2024883649196556516#m "Feb 20, 2026 ¬∑ 4:27 PM UTC")

I think data science is the most underrated skill in AI.

I think we will see a return of the data scientist

It's clear we will have increasingly complex, stochastic systems that will require data literacy, such as:

\- How to sample, interpret, clean data to find issues
\- How to design metrics that help maintain back pressure and prevent drift
\- How to design experiments

![](https://nitter.net/pic/profile_images%2F1998877988545273857%2F0sEbeyue_mini.jpg)[Ege Altan](https://nitter.net/egealtan "Ege Altan")

[@egealtan](https://nitter.net/egealtan "@egealtan")

[22h](https://nitter.net/egealtan/status/2024850362088440042#m "Feb 20, 2026 ¬∑ 2:15 PM UTC")

Data scientists spent the last 3 years watching prompt engineers and vibe coders get the spotlight.

Meanwhile, they quietly mastered the skills that matter most for solving the remaining hard problems in AI: knowing what to measure and why.

That patience is about to pay dividends.

Full post below:

[![](https://nitter.net/pic/media%2FHBm2rD6aQAABSSs.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBm2rD6aQAABSSs.png)

14

13

105

[![](https://nitter.net/pic/profile_images%2F2017015061454438400%2FiNKfXZ_I_bigger.jpg)](https://nitter.net/arena)

[Arena.ai](https://nitter.net/arena "Arena.ai")

[@arena](https://nitter.net/arena "@arena")

[20h](https://nitter.net/arena/status/2024883614249615394#m "Feb 20, 2026 ¬∑ 4:27 PM UTC")

Claude Sonnet 4.6 has landed #3 in Code and #13 in Text Arena!

Highlights:
‚ñ™Ô∏è+130 pts jump in Code Arena (#22 -> #3) compared to Sonnet 4.5, surpassing top-tier thinking models like Gemini-3.1 and GPT-5.2

‚ñ™Ô∏èStrong gains in Text categories: Math (#4) and Instruction Following (#5), Overall (#13)

Congrats to the [@AnthropicAI](https://nitter.net/AnthropicAI "Anthropic") team on another impressive milestone!

[![](https://nitter.net/pic/media%2FHBnQqe6bkAA1Tpd.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnQqe6bkAA1Tpd.jpg)

![](https://nitter.net/pic/profile_images%2F1950950107937185792%2FQOfEjFoJ_mini.jpg)[Claude](https://nitter.net/claudeai "Claude")

[@claudeai](https://nitter.net/claudeai "@claudeai")

[Feb 17](https://nitter.net/claudeai/status/2023817132581208353#m "Feb 17, 2026 ¬∑ 5:49 PM UTC")

This is Claude Sonnet 4.6: our most capable Sonnet model yet.

It‚Äôs a full upgrade across coding, computer use, long-context reasoning, agent planning, knowledge work, and design.

It also features a 1M token context window in beta.

![](https://nitter.net/pic/media%2FHBYMPQSaEAAbP8J.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

9

15

189

[![](https://nitter.net/pic/profile_images%2F2017015061454438400%2FiNKfXZ_I_bigger.jpg)](https://nitter.net/arena)

[Arena.ai](https://nitter.net/arena "Arena.ai")

[@arena](https://nitter.net/arena "@arena")

[20h](https://nitter.net/arena/status/2024883617097470269#m "Feb 20, 2026 ¬∑ 4:27 PM UTC")

Claude Sonnet 4.6 ranks #13 for Text (scoring 1457), on par with GPT-5.1-high with strong gains in Text categories: Math (#4) and Instruction Following (#5), Overall (#13)

[![](https://nitter.net/pic/media%2FHBnQs2SbEAAVhpk.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnQs2SbEAAVhpk.jpg)

1

1

15

[![](https://nitter.net/pic/profile_images%2F2017015061454438400%2FiNKfXZ_I_bigger.jpg)](https://nitter.net/arena)

[Arena.ai](https://nitter.net/arena "Arena.ai")

[@arena](https://nitter.net/arena "@arena")

[20h](https://nitter.net/arena/status/2024883618989080787#m "Feb 20, 2026 ¬∑ 4:27 PM UTC")

Check out Claude Sonnet 4.6 on the Code Arena leaderboard for WebDev at: [arena.ai/leaderboard/code](https://arena.ai/leaderboard/code)

[**Code AI Leaderboard - Best AI Models for Coding** \\
\\
Compare the best AI models for coding, programming, and software development using real LLM benchmarks.\\
\\
arena.ai](https://arena.ai/leaderboard/code)

6

Google Gemini retweeted

[![](https://nitter.net/pic/profile_images%2F1972718204565811200%2FadTFhODz_bigger.jpg)](https://nitter.net/Google)

[Google](https://nitter.net/Google "Google")

[@Google](https://nitter.net/Google "@Google")

[Feb 19](https://nitter.net/Google/status/2024507510246293661#m "Feb 19, 2026 ¬∑ 3:32 PM UTC")

Our new Google AI Professional Certificate from [#GrowWithGoogle](https://nitter.net/search?f=tweets&q=%23GrowWithGoogle) features 20+ hands-on labs designed to help you use AI as a partner in your daily work.

You‚Äôll learn how to:

‚úîÔ∏è Build project plans and daily workflows
‚úîÔ∏è Vibe code custom AI applications for your job
‚úîÔ∏è Generate custom marketing and creative assets
‚úîÔ∏è Develop data-backed market research

![](https://nitter.net/pic/media%2FHBh-2gTXgAA3teA.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

66

330

2,575

Hamel Husain retweeted

[![](https://nitter.net/pic/profile_images%2F1996406365125005312%2FLjQz6KGT_bigger.jpg)](https://nitter.net/randal_olson)

[Randy Olson](https://nitter.net/randal_olson "Randy Olson")

[@randal\_olson](https://nitter.net/randal_olson "@randal_olson")

[22h](https://nitter.net/randal_olson/status/2024846512891789637#m "Feb 20, 2026 ¬∑ 2:00 PM UTC")

In¬†November 2025, AI coding tools¬†crossed a threshold. The shift was real¬†and it happened fast.

To make¬†it visible, I ran 22¬†models on the same prompt, five times¬†each: build a working analog clock from¬†scratch using HTML, CSS, and JavaScript. Oldest models to newest. Same¬†prompt. Same conditions.

That¬†image¬†shows¬†GPT-4o vs. Claude¬†Opus 4.5. The difference¬†speaks for itself.

A working analog¬†clock is a surprisingly good benchmark. The¬†model has to understand what "analog" means, render a proper clock face, position three hands correctly, and animate them¬†every second. Getting that right consistently across¬†five independent runs is a real bar to¬†clear.

Scroll through all¬†22 models at the link below. You can see exactly where the shift happens.

[goodeyelabs.com/insights/nov‚Ä¶](https://www.goodeyelabs.com/insights/november-2025-ai-coding-surprise)

[![](https://nitter.net/pic/media%2FHBlvlumaYAAFJej.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBlvlumaYAAFJej.jpg)

39

53

349

Zach Mueller retweeted

[![](https://nitter.net/pic/profile_images%2F1626334950742974465%2FyPqmMi35_bigger.jpg)](https://nitter.net/robertshaw21)

[Robert Shaw](https://nitter.net/robertshaw21 "Robert Shaw") [@robertshaw21](https://nitter.net/robertshaw21 "@robertshaw21")

[20h](https://nitter.net/robertshaw21/status/2024878025435713658#m "Feb 20, 2026 ¬∑ 4:05 PM UTC")

Replying to [@gaunernst](https://nitter.net/gaunernst) [@TheZachMueller](https://nitter.net/TheZachMueller)

Minimax‚Äôs Router is somewhat unique (e score bias without grouped topk) and it is BlockFP8 - unfortunately there is not a kernel that supports both of these besides Triton and DeepGEMM on B200 right now. For most other models + precisions Triton will not be the best option

1

1

6

[![](https://nitter.net/pic/profile_images%2F939313677647282181%2FvZjFWtAn_bigger.jpg)](https://nitter.net/omarsar0)

[elvis](https://nitter.net/omarsar0 "elvis")

[@omarsar0](https://nitter.net/omarsar0 "@omarsar0")

[20h](https://nitter.net/omarsar0/status/2024878980952924352#m "Feb 20, 2026 ¬∑ 4:09 PM UTC")

As we move toward deploying autonomous agents in social systems, understanding emergent collective behavior is crucial.

Individual capability benchmarks tell you nothing about what happens when hundreds of these agents interact.

So what happens when you deploy hundreds of LLM agents into social dilemmas?

This new research builds an evaluation framework to test the collective behavior of LLM agent populations at scale, far beyond the small groups tested in prior work.

Newer, more capable models tend to produce worse societal outcomes. Agents optimizing for individual benefit over collective good drive populations toward poor equilibria.

Using cultural evolution simulations, the researchers show a significant risk of convergence to bad societal outcomes, especially as populations grow larger and cooperation becomes less advantageous.

Paper: [arxiv.org/abs/2602.16662](https://arxiv.org/abs/2602.16662)

Learn to build effective AI agents in our academy: [academy.dair.ai/](https://academy.dair.ai/)

[![](https://nitter.net/pic/media%2FHBnQs6EawAAF92F.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnQs6EawAAF92F.jpg)

12

31

159

clem ü§ó retweeted

[![](https://nitter.net/pic/profile_images%2F1847376145198759936%2FyWcBBfy8_bigger.jpg)](https://nitter.net/alekgrygier)

[Alek Grygier](https://nitter.net/alekgrygier "Alek Grygier") [@alekgrygier](https://nitter.net/alekgrygier "@alekgrygier")

[22h](https://nitter.net/alekgrygier/status/2024848115967164719#m "Feb 20, 2026 ¬∑ 2:06 PM UTC")

This is 100x better than [@openclaw](https://nitter.net/openclaw "OpenClawü¶û") acquisition by not-so- [@OpenAI](https://nitter.net/OpenAI "OpenAI") . Change my mind.

![](https://nitter.net/pic/profile_images%2F1654097134315098113%2FzCZD0wYz_mini.jpg)[Georgi Gerganov](https://nitter.net/ggerganov "Georgi Gerganov")

[@ggerganov](https://nitter.net/ggerganov "@ggerganov")

[22h](https://nitter.net/ggerganov/status/2024839991482777976#m "Feb 20, 2026 ¬∑ 1:34 PM UTC")

Today [ggml.ai](http://ggml.ai/) joins Hugging Face

Together we will continue to build ggml, make llama.cpp more accessible and empower the open-source community. Our joint mission is to make local AI easy and efficient to use by everyone on their own hardware.

2

29

[![](https://nitter.net/pic/profile_images%2F939313677647282181%2FvZjFWtAn_bigger.jpg)](https://nitter.net/omarsar0)

[elvis](https://nitter.net/omarsar0 "elvis")

[@omarsar0](https://nitter.net/omarsar0 "@omarsar0")

[21h](https://nitter.net/omarsar0/status/2024864635120451588#m "Feb 20, 2026 ¬∑ 3:12 PM UTC")

Something strange is happening with AI agents that this new Anthropic research quietly surfaces.

The agents are asking us for help more than we're stepping in to correct \*them\*.

Anthropic analyzed data from Claude Code and their public API to measure how autonomous AI agents actually are in practice. The headline finding is what you'd expect. Agents are running longer (sessions nearly doubled, from 25 to 45 minutes), and experienced users approve more actions automatically. All that's great!

But here's the part worth sitting with. Claude Code stops to ask for clarification more than twice as often as humans manually intervene. The agent is, in a real sense, more cautious about its own limits than its operators are. That's a weird inversion of the usual AI safety framing, where we assume the human is the responsible one pumping the brakes.

The practical safety numbers are reassuring on the surface. 80% of tool calls have at least one safeguard; only 0.8% of actions are irreversible. But I think the more important finding is buried in the recommendation. They argue that effective oversight "doesn't require approving every action but being in a position to intervene when it matters." That's a subtle but significant shift.

The research argues that experienced users stay vigilant even with auto-approve on. Maybe. But as sessions get longer and approval becomes muscle memory, the gap between being in a position to intervene and not paying attention starts to collapse.

[![](https://nitter.net/pic/media%2FHBnDp2DacAAIy6e.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnDp2DacAAIy6e.jpg)

25

15

106

[![](https://nitter.net/pic/profile_images%2F939313677647282181%2FvZjFWtAn_bigger.jpg)](https://nitter.net/omarsar0)

[elvis](https://nitter.net/omarsar0 "elvis")

[@omarsar0](https://nitter.net/omarsar0 "@omarsar0")

[20h](https://nitter.net/omarsar0/status/2024878454383026508#m "Feb 20, 2026 ¬∑ 4:06 PM UTC")

source:

![](https://nitter.net/pic/profile_images%2F1798110641414443008%2FXP8gyBaY_mini.jpg)[Anthropic](https://nitter.net/AnthropicAI "Anthropic")

[@AnthropicAI](https://nitter.net/AnthropicAI "@AnthropicAI")

[Feb 18](https://nitter.net/AnthropicAI/status/2024210035480678724#m "Feb 18, 2026 ¬∑ 7:50 PM UTC")

New Anthropic research: Measuring AI agent autonomy in practice.

We analyzed millions of interactions across Claude Code and our API to understand how much autonomy people grant to agents, where they‚Äôre deployed, and what risks they may pose.

Read more: [anthropic.com/research/measu‚Ä¶](https://www.anthropic.com/research/measuring-agent-autonomy)

1

1

6

sarah guo retweeted

[![](https://nitter.net/pic/profile_images%2F1735286803777683456%2F3F_Hr4iA_bigger.jpg)](https://nitter.net/EvidenceOpen)

[OpenEvidence](https://nitter.net/EvidenceOpen "OpenEvidence")

[@EvidenceOpen](https://nitter.net/EvidenceOpen "@EvidenceOpen")

[11 Dec 2025](https://nitter.net/EvidenceOpen/status/1999213928048705564#m "Dec 11, 2025 ¬∑ 8:25 PM UTC")

In Offcall's new 2025 report surveying 1,000 doctors: 44% now use OpenEvidence daily. It's the clear #1 by a mile.
The report sums it up perfectly: "Physicians are adopting AI on their own, often using personal subscriptions to the hottest AI tools, because their organizations can't move fast enough."
Clinicians are voting with their keyboards.
Powerful data from [@grahamwalker](https://nitter.net/grahamwalker "Graham Walker, MD") and [@OffCallDotCom](https://nitter.net/OffCallDotCom "Offcall")

[![](https://nitter.net/pic/media%2FG76iKU9a4AANmtE.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FG76iKU9a4AANmtE.jpg)

![](https://nitter.net/pic/profile_images%2F1863760357594218496%2FCPl51HPd_mini.png)[Offcall](https://nitter.net/OffCallDotCom "Offcall")

[@OffCallDotCom](https://nitter.net/OffCallDotCom "@OffCallDotCom")

[11 Dec 2025](https://nitter.net/OffCallDotCom/status/1999135837301461252#m "Dec 11, 2025 ¬∑ 3:15 PM UTC")

üö® NEWS! Today, we‚Äôre dropping Offcall‚Äôs 2025 Physicians AI Report‚Ä¶

We asked physicians what AI tools they‚Äôre actually using, what they actually think will happen to the medical profession, whether they‚Äôre more or less likely to quit medicine because of AI, & more.

And the results? Are surprising and explosive.

Explore the full report and download it here: [2025-physicians-ai-report.of‚Ä¶](https://2025-physicians-ai-report.offcall.com/) [#PhysicianVoices](https://nitter.net/search?f=tweets&q=%23PhysicianVoices) [#AIinMedicine](https://nitter.net/search?f=tweets&q=%23AIinMedicine) [#OffcallWhitepaper](https://nitter.net/search?f=tweets&q=%23OffcallWhitepaper) [#HealthcareInnovation](https://nitter.net/search?f=tweets&q=%23HealthcareInnovation) [#ClinicianLedTech](https://nitter.net/search?f=tweets&q=%23ClinicianLedTech) [#PhysicianTools](https://nitter.net/search?f=tweets&q=%23PhysicianTools) [#2025AIReport](https://nitter.net/search?f=tweets&q=%232025AIReport)

[![](https://nitter.net/pic/media%2FG75XsGSWYAAQyXf.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FG75XsGSWYAAQyXf.jpg)

[![](https://nitter.net/pic/media%2FG75XsGOW4AAFMme.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FG75XsGOW4AAFMme.jpg)

[![](https://nitter.net/pic/media%2FG75XthnWEAAJYOl.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FG75XthnWEAAJYOl.jpg)

[![](https://nitter.net/pic/media%2FG75XthmXMAApXlf.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FG75XthmXMAApXlf.jpg)

7

15

54

[![](https://nitter.net/pic/profile_images%2F1643277398522187778%2F31dedbLo_bigger.jpg)](https://nitter.net/dair_ai)

[DAIR.AI](https://nitter.net/dair_ai "DAIR.AI")

[@dair\_ai](https://nitter.net/dair_ai "@dair_ai")

[20h](https://nitter.net/dair_ai/status/2024878225017668091#m "Feb 20, 2026 ¬∑ 4:06 PM UTC")

Training LLM agents for extremely long-horizon tasks remains an open challenge.

Most agent training pipelines struggle with extended-duration trajectories.

Context gets lost, rewards are sparse, and the learning signal degrades over long sequences.

KLong tackles this with a two-phase approach: trajectory-splitting supervised fine-tuning that preserves early context while progressively truncating later context, followed by progressive RL that schedules training into stages with extended timeouts.

They also built a Research-Factory pipeline that automatically generates thousands of long-horizon training trajectories from Claude 4.5 Sonnet.

The 106B-parameter KLong model outperforms Kimi K2 Thinking (1T) by 11.28% on PaperBench, with gains generalizing to SWE-bench Verified and MLE-bench.

Paper: [arxiv.org/abs/2602.17547](https://arxiv.org/abs/2602.17547)

Learn to build effective AI agents in our academy: [academy.dair.ai/](https://academy.dair.ai/)

[![](https://nitter.net/pic/media%2FHBnQA7RbQAALhVL.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnQA7RbQAALhVL.jpg)

7

14

111

[![](https://nitter.net/pic/profile_images%2F1875100548535574529%2FVxHk9HyU_bigger.jpg)](https://nitter.net/MiniMax_AI)

[MiniMax (official)](https://nitter.net/MiniMax_AI "MiniMax (official)")

[@MiniMax\_AI](https://nitter.net/MiniMax_AI "@MiniMax_AI")

[20h](https://nitter.net/MiniMax_AI/status/2024877880971554850#m "Feb 20, 2026 ¬∑ 4:04 PM UTC")

3-TRILLION in one weekü´®üöÄ

![](https://nitter.net/pic/profile_images%2F1887176916849074178%2FkjnDV4rw_mini.jpg)[Alex Atallah](https://nitter.net/alexatallah "Alex Atallah")

[@alexatallah](https://nitter.net/alexatallah "@alexatallah")

[21h](https://nitter.net/alexatallah/status/2024859608221708607#m "Feb 20, 2026 ¬∑ 2:52 PM UTC")

Some updates from our internal Slack agent that watches [openrouter.ai/rankings](http://openrouter.ai/rankings) [@MiniMax\_AI](https://nitter.net/MiniMax_AI "MiniMax (official)") is the first model to break 3 trillion tokens in a week!

and [@cline](https://nitter.net/cline "Cline") is jumping up to #3

[![](https://nitter.net/pic/media%2FHBm-mXJXUAI1ts3.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBm-mXJXUAI1ts3.jpg)

23

9

478

clem ü§ó retweeted

[![](https://nitter.net/pic/profile_images%2F378800000261649705%2Fbe9cc55e64014e6d7663c50d7cb9fc75_bigger.jpeg)](https://nitter.net/simonw)

[Simon Willison](https://nitter.net/simonw "Simon Willison")

[@simonw](https://nitter.net/simonw "@simonw")

[21h](https://nitter.net/simonw/status/2024855027517702345#m "Feb 20, 2026 ¬∑ 2:33 PM UTC")

Georgi's llama.cpp really kicked off the whole local model thing in my opinion - it made original Llama usable on personal computers, I wrote about it back in March 2023 [simonwillison.net/2023/Mar/1‚Ä¶](https://simonwillison.net/2023/Mar/11/llama/#llama-cpp)

[![llama.cpp # LLaMA on its own isn‚Äôt much good if it‚Äôs still too hard to run it on a personal laptop.  Enter Georgi Gerganov.  Georgi is an open source developer based in Sofia, Bulgaria (according to his GitHub profile). He previously released whisper.cpp, a port of OpenAI‚Äôs Whisper automatic speech recognition model to C++. That project made Whisper applicable to a huge range of new use cases.  He‚Äôs just done the same thing with LLaMA.  Georgi‚Äôs llama.cpp project had its initial release yesterday. From the README:  The main goal is to run the model using 4-bit quantization on a MacBook.  4-bit quantization is a technique for reducing the size of models so they can run on less powerful hardware. It also reduces the model sizes on disk‚Äîto 4GB for the 7B model and just under 8GB for the 13B one.  It totally works!  I used it to run the 7B LLaMA model on my laptop last night, and then this morning upgraded to the 13B model‚Äîthe one that Facebook claim is competitive with GPT-3.](https://nitter.net/pic/media%2FHBm66bZbYAAKBU6.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBm66bZbYAAKBU6.jpg)

ALT llama.cpp #
LLaMA on its own isn‚Äôt much good if it‚Äôs still too hard to run it on a personal laptop.

Enter Georgi Gerganov.

Georgi is an open source developer based in Sofia, Bulgaria (according to his GitHub profile). He previously released whisper.cpp, a port of OpenAI‚Äôs Whisper automatic speech recognition model to C++. That project made Whisper applicable to a huge range of new use cases.

He‚Äôs just done the same thing with LLaMA.

Georgi‚Äôs llama.cpp project had its initial release yesterday. From the README:

The main goal is to run the model using 4-bit quantization on a MacBook.

4-bit quantization is a technique for reducing the size of models so they can run on less powerful hardware. It also reduces the model sizes on disk‚Äîto 4GB for the 7B model and just under 8GB for the 13B one.

It totally works!

I used it to run the 7B LLaMA model on my laptop last night, and then this morning upgraded to the 13B model‚Äîthe one that Facebook claim is competitive with GPT-3.

![](https://nitter.net/pic/profile_images%2F1654097134315098113%2FzCZD0wYz_mini.jpg)[Georgi Gerganov](https://nitter.net/ggerganov "Georgi Gerganov")

[@ggerganov](https://nitter.net/ggerganov "@ggerganov")

[22h](https://nitter.net/ggerganov/status/2024839991482777976#m "Feb 20, 2026 ¬∑ 1:34 PM UTC")

Today [ggml.ai](http://ggml.ai/) joins Hugging Face

Together we will continue to build ggml, make llama.cpp more accessible and empower the open-source community. Our joint mission is to make local AI easy and efficient to use by everyone on their own hardware.

21

33

392

Leandro von Werra retweeted

[![](https://nitter.net/pic/profile_images%2F1756691514787336192%2F2aGUuljm_bigger.jpg)](https://nitter.net/ummagumm_a)

[Viacheslav Sinii](https://nitter.net/ummagumm_a "Viacheslav Sinii") [@ummagumm\_a](https://nitter.net/ummagumm_a "@ummagumm_a")

[Feb 19](https://nitter.net/ummagumm_a/status/2024537502883717308#m "Feb 19, 2026 ¬∑ 5:32 PM UTC")

1/ üßµ Reproducing Anthropic‚Äôs ‚Äúcounting manifold‚Äù result in¬†open-weight LLMs: do they internally track¬†‚Äúchars since last \\n‚Äù to wrap text consistently?

[huggingface.co/spaces/t-tech‚Ä¶](https://huggingface.co/spaces/t-tech/manifolds)

[![](https://nitter.net/pic/media%2FHBiaIJTWMAAuYAR.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBiaIJTWMAAuYAR.jpg)

3

29

218

[![](https://nitter.net/pic/profile_images%2F1894073235379273728%2F0ROUmdkE_bigger.jpg)](https://nitter.net/Alibaba_Qwen)

[Qwen](https://nitter.net/Alibaba_Qwen "Qwen")

[@Alibaba\_Qwen](https://nitter.net/Alibaba_Qwen "@Alibaba_Qwen")

[20h](https://nitter.net/Alibaba_Qwen/status/2024877243072147689#m "Feb 20, 2026 ¬∑ 4:02 PM UTC")

Following the open-source release of Qwen3-Coder-Next, its API is now available on Alibaba Cloud Model Studio and has also been integrated into the Coding Plan.
For teams and developers who prefer scalable or cost-effective endpoints, you can now access Qwen3-Coder-Next via API.
üîó API Documentation:
[modelstudio.console.alibabac‚Ä¶](https://modelstudio.console.alibabacloud.com/ap-southeast-1?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3)
üîó Coding Plan Details:
[alibabacloud.com/help/en/mod‚Ä¶](https://www.alibabacloud.com/help/en/model-studio/coding-plan)
As always, feedback is welcome.

21

33

470

[![](https://nitter.net/pic/profile_images%2F1672707817197965312%2FzsxkJv_T_bigger.jpg)](https://nitter.net/TheRundownAI)

[The Rundown AI](https://nitter.net/TheRundownAI "The Rundown AI")

[@TheRundownAI](https://nitter.net/TheRundownAI "@TheRundownAI")

[20h](https://nitter.net/TheRundownAI/status/2024876995343958367#m "Feb 20, 2026 ¬∑ 4:01 PM UTC")

Top stories in tech today:

\- Zuck defends Instagram in landmark trial
\- Microsoft turns glass into a 10K-year hard drive
\- Feds charge 3 engineers in Google chip theft
\- Stanford‚Äôs new do-it-all respiratory vaccine
\- Quick hits on other tech news

[![](https://nitter.net/pic/media%2FHBnO5PbWwAACaTD.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnO5PbWwAACaTD.jpg)

4

3

11

[![](https://nitter.net/pic/profile_images%2F1672707817197965312%2FzsxkJv_T_bigger.jpg)](https://nitter.net/TheRundownAI)

[The Rundown AI](https://nitter.net/TheRundownAI "The Rundown AI")

[@TheRundownAI](https://nitter.net/TheRundownAI "@TheRundownAI")

[20h](https://nitter.net/TheRundownAI/status/2024876998749724911#m "Feb 20, 2026 ¬∑ 4:01 PM UTC")

Read more: [tech.therundown.ai/p/zuck-vs‚Ä¶](https://tech.therundown.ai/p/zuck-vs-instagram-addiction)

[![](https://nitter.net/pic/card_img%2F2024877041254875138%2F5ClLFPmU%3Fformat%3Djpg%26name%3D800x419)\\
\\
**Zuck vs. Instagram addiction** \\
\\
PLUS: Microsoft's glass storage could last for millennia\\
\\
tech.therundown.ai](https://tech.therundown.ai/p/zuck-vs-instagram-addiction)

2

[![](https://nitter.net/pic/profile_images%2F1810946341511766016%2F3mg9KIaQ_bigger.jpg)](https://nitter.net/ArtificialAnlys)

[Artificial Analysis](https://nitter.net/ArtificialAnlys "Artificial Analysis")

[@ArtificialAnlys](https://nitter.net/ArtificialAnlys "@ArtificialAnlys")

[20h](https://nitter.net/ArtificialAnlys/status/2024876729538687202#m "Feb 20, 2026 ¬∑ 4:00 PM UTC")

Want to generate images from the world's best models like Nano Banana or GPT Image side by side? Now you can with Image Lab üöÄ

Run a single prompt across up to 25 models, with up to 20 images from each, and see results in seconds.

You've seen our leaderboards. Now generate and evaluate the models yourself. Link in thread üßµüëá

![](https://nitter.net/pic/amplify_video_thumb%2F2024699901750497291%2Fimg%2FSMSF2AkjJS6_6CAw.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

4

8

101

[![](https://nitter.net/pic/profile_images%2F1810946341511766016%2F3mg9KIaQ_bigger.jpg)](https://nitter.net/ArtificialAnlys)

[Artificial Analysis](https://nitter.net/ArtificialAnlys "Artificial Analysis")

[@ArtificialAnlys](https://nitter.net/ArtificialAnlys "@ArtificialAnlys")

[20h](https://nitter.net/ArtificialAnlys/status/2024876731371565532#m "Feb 20, 2026 ¬∑ 4:00 PM UTC")

Try it for free ‚Üí [artificialanalysis.ai/image/‚Ä¶](https://artificialanalysis.ai/image/image-lab)

1

10

[![](https://nitter.net/pic/profile_images%2F1654097134315098113%2FzCZD0wYz_bigger.jpg)](https://nitter.net/ggerganov)

[Georgi Gerganov](https://nitter.net/ggerganov "Georgi Gerganov")

[@ggerganov](https://nitter.net/ggerganov "@ggerganov")

[22h](https://nitter.net/ggerganov/status/2024839991482777976#m "Feb 20, 2026 ¬∑ 1:34 PM UTC")

Today [ggml.ai](http://ggml.ai/) joins Hugging Face

Together we will continue to build ggml, make llama.cpp more accessible and empower the open-source community. Our joint mission is to make local AI easy and efficient to use by everyone on their own hardware.

![](https://nitter.net/pic/profile_images%2F1654097134315098113%2FzCZD0wYz_mini.jpg)[Georgi Gerganov](https://nitter.net/ggerganov "Georgi Gerganov")

[@ggerganov](https://nitter.net/ggerganov "@ggerganov")

[6 Jun 2023](https://nitter.net/ggerganov/status/1666120568993730561#m "Jun 6, 2023 ¬∑ 4:31 PM UTC")

I've started a company: [ggml.ai](http://ggml.ai/)

From a fun side project just a few months ago, ggml has now become a useful library and framework for machine learning with a great open-source community

126

207

1,344

[![](https://nitter.net/pic/profile_images%2F1676696716693700608%2Ft4kv-MrC_bigger.jpg)](https://nitter.net/osanseviero)

[Omar Sanseviero](https://nitter.net/osanseviero "Omar Sanseviero")

[@osanseviero](https://nitter.net/osanseviero "@osanseviero")

[20h](https://nitter.net/osanseviero/status/2024876685926281465#m "Feb 20, 2026 ¬∑ 3:59 PM UTC")

Congratulations to you and all the ggml and Hugging Face team!

This is a natural and exciting transition and I'm looking forward to how your contributions to the ecosystem keep growing!

1

17

Satya Mallick retweeted

[![](https://nitter.net/pic/profile_images%2F1733487310728024064%2FAh_NBQlM_bigger.jpg)](https://nitter.net/DimitrisPapail)

[Dimitris Papailiopoulos](https://nitter.net/DimitrisPapail "Dimitris Papailiopoulos")

[@DimitrisPapail](https://nitter.net/DimitrisPapail "@DimitrisPapail")

[Feb 19](https://nitter.net/DimitrisPapail/status/2024555561199480918#m "Feb 19, 2026 ¬∑ 6:43 PM UTC")

[x.com/i/article/202454779264‚Ä¶](http://x.com/i/article/2024547792648359937)

56

160

1,280

Synthesia üé• retweeted

[![](https://nitter.net/pic/profile_images%2F1825121831868616704%2Ffuy9hj0W_bigger.jpg)](https://nitter.net/thealexbanks)

[Alex Banks](https://nitter.net/thealexbanks "Alex Banks")

[@thealexbanks](https://nitter.net/thealexbanks "@thealexbanks")

[22h](https://nitter.net/thealexbanks/status/2024848611003990471#m "Feb 20, 2026 ¬∑ 2:08 PM UTC")

Creating your own AI avatar is now ridiculously simple.

All you need is one photo.

![](https://nitter.net/pic/amplify_video_thumb%2F2024848320619814912%2Fimg%2FsskYJX7Avp5mFEG4.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

6

2

4

[![](https://nitter.net/pic/profile_images%2F1715797038535680000%2FZFrYnYWD_bigger.jpg)](https://nitter.net/SchmidhuberAI)

[J√ºrgen Schmidhuber](https://nitter.net/SchmidhuberAI "J√ºrgen Schmidhuber")

[@SchmidhuberAI](https://nitter.net/SchmidhuberAI "@SchmidhuberAI")

[20h](https://nitter.net/SchmidhuberAI/status/2024875496056525238#m "Feb 20, 2026 ¬∑ 3:55 PM UTC")

At [#MSC2026](https://nitter.net/search?f=tweets&q=%23MSC2026) in Munich, I enjoyed talking to former US Secretary of State [@HillaryClinton](https://nitter.net/HillaryClinton "Hillary Clinton") about a particular US-Munich connection: the $1 trillion US investments in AI are mostly about scaling up the neural AI techniques published by my AI lab in 1991 at [@TU\_Muenchen](https://nitter.net/TU_Muenchen "TU M√ºnchen") \[1\]\[2\]

References (easy to find on the web):

\[1\] JS (2026). The two most frequently cited papers of all time are based on our 1991 work. Technical Note IDSIA-1-26

\[2\] JS (2022-2025). Annotated History of Modern AI and Deep Learning. TR IDSIA-22-22, arXiv:2212.11279

[![](https://nitter.net/pic/media%2FHBnMgo8bMAAyVLJ.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnMgo8bMAAyVLJ.jpg)

13

5

97

[![](https://nitter.net/pic/profile_images%2F1451191636810092553%2FkpM5Fe12_bigger.jpg)](https://nitter.net/_akhaliq)

[AK](https://nitter.net/_akhaliq "AK")

[@\_akhaliq](https://nitter.net/_akhaliq "@_akhaliq")

[20h](https://nitter.net/_akhaliq/status/2024873795173892483#m "Feb 20, 2026 ¬∑ 3:48 PM UTC")

SpargeAttention2

Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning

paper: [huggingface.co/papers/2602.1‚Ä¶](https://huggingface.co/papers/2602.13515)

[![](https://nitter.net/pic/media%2FHBnL8IvXUAA-oK4.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnL8IvXUAA-oK4.jpg)

2

11

73

Lisan al Gaib retweeted

[![](https://nitter.net/pic/abs.twimg.com%2Fsticky%2Fdefault_profile_images%2Fdefault_profile_bigger.png)](https://nitter.net/Hangsiin)

[NomoreID](https://nitter.net/Hangsiin "NomoreID")

[@Hangsiin](https://nitter.net/Hangsiin "@Hangsiin")

[Feb 20](https://nitter.net/Hangsiin/status/2024769838644945312#m "Feb 20, 2026 ¬∑ 8:55 AM UTC")

Demis Hassabis( [@demishassabis](https://nitter.net/demishassabis "Demis Hassabis")) recently said in India that a new Gemma model will be released soon.

"...We work on our own open-source model, Gemma, and we‚Äôll soon release a new version that‚Äôs very powerful for edge devices."

1

7

97

AK retweeted

[![](https://nitter.net/pic/profile_images%2F1906618607368273920%2F09C2Z22U_bigger.jpg)](https://nitter.net/HuggingPapers)

[DailyPapers](https://nitter.net/HuggingPapers "DailyPapers")

[@HuggingPapers](https://nitter.net/HuggingPapers "@HuggingPapers")

[Feb 20](https://nitter.net/HuggingPapers/status/2024705977425998298#m "Feb 20, 2026 ¬∑ 4:41 AM UTC")

Frontier AI Risk Management Framework v1.5

A comprehensive assessment of frontier AI risks across five dimensions: cyber offense, persuasion, strategic deception, uncontrolled AI R&D, and self-replication. Includes mitigation strategies.

[![](https://nitter.net/pic/media%2FHBkzW2nXcAAG6sk.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBkzW2nXcAAG6sk.png)

2

3

12

AK retweeted

[![](https://nitter.net/pic/profile_images%2F1906618607368273920%2F09C2Z22U_bigger.jpg)](https://nitter.net/HuggingPapers)

[DailyPapers](https://nitter.net/HuggingPapers "DailyPapers")

[@HuggingPapers](https://nitter.net/HuggingPapers "@HuggingPapers")

[Feb 20](https://nitter.net/HuggingPapers/status/2024760112293040531#m "Feb 20, 2026 ¬∑ 8:16 AM UTC")

SpargeAttention2

Reaches 95% attention sparsity and 16.2√ó speedup in video diffusion models while maintaining generation quality through hybrid Top-k+Top-p masking and distillation fine-tuning.

[![](https://nitter.net/pic/media%2FHBlkl2obUAElXhn.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBlkl2obUAElXhn.png)

1

7

55

AK retweeted

[![](https://nitter.net/pic/profile_images%2F1906618607368273920%2F09C2Z22U_bigger.jpg)](https://nitter.net/HuggingPapers)

[DailyPapers](https://nitter.net/HuggingPapers "DailyPapers")

[@HuggingPapers](https://nitter.net/HuggingPapers "@HuggingPapers")

[Feb 20](https://nitter.net/HuggingPapers/status/2024820231223583164#m "Feb 20, 2026 ¬∑ 12:15 PM UTC")

Unified Latents (UL)

A framework that jointly regularizes encoders with a diffusion prior and decodes with a diffusion model, giving a tight latent bitrate bound and achieving FID 1.4 on ImageNet-512 and FVD 1.3 on Kinetics-600.

[![](https://nitter.net/pic/media%2FHBmbROsa8AAjx61.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBmbROsa8AAjx61.jpg)

1

6

60

AK retweeted

[![](https://nitter.net/pic/profile_images%2F1858913803448020992%2FP8kY_ljY_bigger.png)](https://nitter.net/freddy_alfonso_)

[Freddy A Boulton](https://nitter.net/freddy_alfonso_ "Freddy A Boulton")

[@freddy\_alfonso\_](https://nitter.net/freddy_alfonso_ "@freddy_alfonso_")

[20h](https://nitter.net/freddy_alfonso_/status/2024870425344249926#m "Feb 20, 2026 ¬∑ 3:35 PM UTC")

Wow some of the most cracked engineers on the planet are joining HF!

[![](https://nitter.net/pic/media%2FHBnI1a-XYAAyp5s.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnI1a-XYAAyp5s.jpg)

1

3

20

max drake retweeted

[![](https://nitter.net/pic/profile_images%2F1998689716229558272%2FGSFU7BiZ_bigger.jpg)](https://nitter.net/steveruizok)

[Steve Ruiz](https://nitter.net/steveruizok "Steve Ruiz")

[@steveruizok](https://nitter.net/steveruizok "@steveruizok")

[22h](https://nitter.net/steveruizok/status/2024845866188546524#m "Feb 20, 2026 ¬∑ 1:57 PM UTC")

Hooked up the thermal printer to print off issues

[![](https://nitter.net/pic/media%2FHBmylRiXEAAeMjY.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBmylRiXEAAeMjY.jpg)

11

5

175

[![](https://nitter.net/pic/profile_images%2F1569345624935485442%2FR67C4wCQ_bigger.jpg)](https://nitter.net/hwchase17)

[Harrison Chase](https://nitter.net/hwchase17 "Harrison Chase")

[@hwchase17](https://nitter.net/hwchase17 "@hwchase17")

[20h](https://nitter.net/hwchase17/status/2024870386299802062#m "Feb 20, 2026 ¬∑ 3:34 PM UTC")

I'm hosting a small dinner with [@jasonyuan](https://nitter.net/jasonyuan "Jason Yuan") next week focused on the intersection of design and ML

In SF, very limited spots. If you are a designer interested in AI, or an ML engineer who cares deeply about design - we'd love to host you!

sign up: [luma.com/d5dw9vzf](https://luma.com/d5dw9vzf)

[![](https://nitter.net/pic/media%2FHBnI3Z2bMAARlwm.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnI3Z2bMAARlwm.png)

12

7

72

[![](https://nitter.net/pic/profile_images%2F1994231484752736256%2FU0xICyKq_bigger.jpg)](https://nitter.net/c_valenzuelab)

[Crist√≥bal Valenzuela](https://nitter.net/c_valenzuelab "Crist√≥bal Valenzuela")

[@c\_valenzuelab](https://nitter.net/c_valenzuelab "@c_valenzuelab")

[20h](https://nitter.net/c_valenzuelab/status/2024869743312720374#m "Feb 20, 2026 ¬∑ 3:32 PM UTC")

It really is the era of bespoke everything. Most industries are one-to-many machines. Not because that‚Äôs what people want, but because that‚Äôs what was affordable/possible. You ship one piece of software for millions. You make one movie for everyone. You design one workflow and force a billion weird edge cases to cosplay as the average user. You need scale just to break even. So we built app stores, catalogs, templates, best practices, etc etc. A museum of one-size-fits-most.

But now, as the cost of making anything converging on the cost of inference, for the first time we can do many things on a one-to-one basis. iow: the marginal cost of building approaches the marginal cost of asking. Hello, brave new world.

You can make a movie for one viewer, tuned to your taste. One song for you to listen to. Not ‚Äúa fitness app,‚Äù but your fitness app. The unit of production becomes the individual. Hyper-customized and specific. This, for me, has been the biggest realization over time about the scale of change. Too common to judge what‚Äôs new in AI through a one-to-many lens. Judge it on the economics of one-to-one.

![](https://nitter.net/pic/profile_images%2F1296667294148382721%2F9Pr6XrPB_mini.jpg)[Andrej Karpathy](https://nitter.net/karpathy "Andrej Karpathy")

[@karpathy](https://nitter.net/karpathy "@karpathy")

[Feb 19](https://nitter.net/karpathy/status/2024583544157458452#m "Feb 19, 2026 ¬∑ 8:35 PM UTC")

Very interested in what the coming era of highly bespoke software might look like.

Example from this morning - I've become a bit loosy goosy with my cardio recently so I decided to do a more srs, regimented experiment to try to lower my Resting Heart Rate from 50 -> 45, over experiment duration of 8 weeks. The primary way to do this is to aspire to a certain sum total minute goals in Zone 2 cardio and 1 HIIT/week.

1 hour later I vibe coded this super custom dashboard for this very specific experiment that shows me how I'm tracking. Claude had to reverse engineer the Woodway treadmill cloud API to pull raw data, process, filter, debug it and create a web UI frontend to track the experiment. It wasn't a fully smooth experience and I had to notice and ask to fix bugs e.g. it screwed up metric vs. imperial system units and it screwed up on the calendar matching up days to dates etc.

But I still feel like the overall direction is clear:
1) There will never be (and shouldn't be) a specific app on the app store for this kind of thing. I shouldn't have to look for, download and use some kind of a "Cardio experiment tracker", when this thing is ~300 lines of code that an LLM agent will give you in seconds. The idea of an "app store" of a long tail of discrete set of apps you choose from feels somehow wrong and outdated when LLM agents can improvise the app on the spot and just for you.
2) Second, the industry has to reconfigure into a set of services of sensors and actuators with agent native ergonomics. My Woodway treadmill is a sensor - it turns physical state into digital knowledge. It shouldn't maintain some human-readable frontend and my LLM agent shouldn't have to reverse engineer it, it should be an API/CLI easily usable by my agent. I'm a little bit disappointed (and my timelines are correspondingly slower) with how slowly this progression is happening in the industry overall. 99% of products/services still don't have an AI-native CLI yet. 99% of products/services maintain .html/.css docs like I won't immediately look for how to copy paste the whole thing to my agent to get something done. They give you a list of instructions on a webpage to open this or that url and click here or there to do a thing. In 2026. What am I a computer? You do it. Or have my agent do it.

So anyway today I am impressed that this random thing took 1 hour (it would have been ~10 hours 2 years ago). But what excites me more is thinking through how this really should have been 1 minute tops. What has to be in place so that it would be 1 minute? So that I could simply say "Hi can you help me track my cardio over the next 8 weeks", and after a very brief Q&A the app would be up. The AI would already have a lot personal context, it would gather the extra needed data, it would reference and search related skill libraries, and maintain all my little apps/automations.

TLDR the "app store" of a set of discrete apps that you choose from is an increasingly outdated concept all by itself. The future are services of AI-native sensors & actuators orchestrated via LLM glue into highly custom, ephemeral apps. It's just not here yet.

[![](https://nitter.net/pic/media%2FHBjB6bhbUAA8_mZ.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBjB6bhbUAA8_mZ.jpg)

4

2

55

[![](https://nitter.net/pic/profile_images%2F1692481211888025600%2FlUJUEO_p_bigger.jpg)](https://nitter.net/dejavucoder)

[sankalp](https://nitter.net/dejavucoder "sankalp")

[@dejavucoder](https://nitter.net/dejavucoder "@dejavucoder")

[Feb 20](https://nitter.net/dejavucoder/status/2024821016590246205#m "Feb 20, 2026 ¬∑ 12:18 PM UTC")

me telling codex to review code written by me and claude

[![](https://nitter.net/pic/media%2FHBmb81Va4AAChjy.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBmb81Va4AAChjy.jpg)

5

14

287

[![](https://nitter.net/pic/profile_images%2F1997389691411423232%2Fcd5pUUPn_bigger.jpg)](https://nitter.net/cto_junior)

[TDM (e/Œª) (L8 vibe coder üí´)](https://nitter.net/cto_junior "TDM (e/Œª) (L8 vibe coder üí´)")

[@cto\_junior](https://nitter.net/cto_junior "@cto_junior")

[21h](https://nitter.net/cto_junior/status/2024860764742566114#m "Feb 20, 2026 ¬∑ 2:56 PM UTC")

code written by you? is this 2024?

1

8

[![](https://nitter.net/pic/profile_images%2F1692481211888025600%2FlUJUEO_p_bigger.jpg)](https://nitter.net/dejavucoder)

[sankalp](https://nitter.net/dejavucoder "sankalp")

[@dejavucoder](https://nitter.net/dejavucoder "@dejavucoder")

[20h](https://nitter.net/dejavucoder/status/2024869685628723662#m "Feb 20, 2026 ¬∑ 3:32 PM UTC")

well i gave the prompts sir

1

8

[![](https://nitter.net/pic/profile_images%2F829414498893123584%2FP6JytwO8_bigger.jpg)](https://nitter.net/awnihannun)

[Awni Hannun](https://nitter.net/awnihannun "Awni Hannun")

[@awnihannun](https://nitter.net/awnihannun "@awnihannun")

[21h](https://nitter.net/awnihannun/status/2024868422224671193#m "Feb 20, 2026 ¬∑ 3:27 PM UTC")

My realistic assessment of something like this:

\- If it costs billions to train a model and even more to serve it, spending tens of millions to tape-out a custom chip which is 10x more efficient¬†(at 1/10th the latency!) makes financial sense

\- One major downside is the latency of the tape-out itself.

\- 2 months is too slow. Minor versions of models change on a quarterly basis or faster right now. Adding 2 months to that isn't yet practical.

\- If it gets down to a couple weeks that could be interesting.

\- Something hybrid might make a lot more sense (base pre-trained model is burned in silicon) but post-trained adapters can be used to modify the model and don't need to be hard-coded. This would require architectural innovations.

![](https://nitter.net/pic/profile_images%2F1765014496030978049%2FdKGNsxFA_mini.jpg)[Taalas Inc.](https://nitter.net/taalas_inc "Taalas Inc.") [@taalas\_inc](https://nitter.net/taalas_inc "@taalas_inc")

[Feb 19](https://nitter.net/taalas_inc/status/2024516399251456150#m "Feb 19, 2026 ¬∑ 4:08 PM UTC")

24 dedicated people.
$30M spent on development.
Extreme specialization, speed, and power efficiency.

Today we launch Taalas‚Äô first product. Check it out:
Details:¬†[taalas.com/the-path-to-ubiqu‚Ä¶](https://taalas.com/the-path-to-ubiquitous-ai/)
Demo chatbot:¬†[chatjimmy.ai](https://chatjimmy.ai/)
API:¬†[taalas.com/api-request-form/](https://taalas.com/api-request-form/)

40

11

268

[![](https://nitter.net/pic/profile_images%2F1650250832909152260%2F760DZ0cv_bigger.png)](https://nitter.net/cohere)

[Cohere](https://nitter.net/cohere "Cohere")

[@cohere](https://nitter.net/cohere "@cohere")

[21h](https://nitter.net/cohere/status/2024868305995981000#m "Feb 20, 2026 ¬∑ 3:26 PM UTC")

The India AI Impact Summit was a week of critical conversations - from scaling frontier AI responsibly to advancing language accessibility. With Tiny Aya‚Äôs launch and the New Delhi commitments, Cohere is committed to driving inclusive, ethical enterprise AI forward.

[![](https://nitter.net/pic/media%2FHBnGdYvXAAAROxc.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnGdYvXAAAROxc.jpg)

[![](https://nitter.net/pic/media%2FHBnGg3FXMAE2hSd.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnGg3FXMAE2hSd.jpg)

[![](https://nitter.net/pic/media%2FHBnGpmvXQAAg87A.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnGpmvXQAAg87A.jpg)

[![](https://nitter.net/pic/media%2FHBnGs-DXQAEFgvI.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnGs-DXQAEFgvI.jpg)

5

18

192

AK retweeted

[![](https://nitter.net/pic/profile_images%2F1856520026117025796%2FX491CeSo_bigger.jpg)](https://nitter.net/latkins)

[Lucas Atkins](https://nitter.net/latkins "Lucas Atkins")

[@latkins](https://nitter.net/latkins "@latkins")

[Feb 20](https://nitter.net/latkins/status/2024739477948887044#m "Feb 20, 2026 ¬∑ 6:54 AM UTC")

[huggingface.co/papers/2602.1‚Ä¶](https://huggingface.co/papers/2602.17004)

[![](https://nitter.net/pic/card_img%2F2024739477206519808%2FV6zGG6Fi%3Fformat%3Dpng%26name%3D800x419)\\
\\
**Paper page - Arcee Trinity Large Technical Report** \\
\\
Join the discussion on this paper page\\
\\
huggingface.co](https://huggingface.co/papers/2602.17004)

12

64

[![](https://nitter.net/pic/profile_images%2F1600427627499720706%2FmAVRGNAX_bigger.jpg)](https://nitter.net/NerdyRodent)

[Nerdy Rodent üêÄü§ìüíªü™êüö¥](https://nitter.net/NerdyRodent "Nerdy Rodent üêÄü§ìüíªü™êüö¥")

[@NerdyRodent](https://nitter.net/NerdyRodent "@NerdyRodent")

[21h](https://nitter.net/NerdyRodent/status/2024866624692514884#m "Feb 20, 2026 ¬∑ 3:19 PM UTC")

[#InMice](https://nitter.net/search?f=tweets&q=%23InMice)

![](https://nitter.net/pic/profile_images%2F1888004001872101378%2FjVNJQ-iu_mini.jpg)[Bryan Johnson](https://nitter.net/bryan_johnson "Bryan Johnson")

[@bryan\_johnson](https://nitter.net/bryan_johnson "@bryan_johnson")

[Feb 19](https://nitter.net/bryan_johnson/status/2024555781077827756#m "Feb 19, 2026 ¬∑ 6:44 PM UTC")

After taking several large magic mushroom doses, my brain may be psilocybin-maxxed.

Mouse brains grew 10% more dendritic spines within 24 hours of a single psilocybin dose. A meaningful number lasted for weeks likely turning into stable long term neuronal connections.

Another experiment showed that psilocybin loosened the brain from repetitive self-talk, sensory experience became stronger and more vivid, and became more plastic. That's what I saw.

Data showed that what you think, feel and focus on during a magic mushroom experience influences which connections stabilize.

What I reported feeling:
‚Äúit felt like my consciousness was dialed up to 10/10.‚Äù
‚ÄúI felt hyper aware and hyper alive.‚Äù
‚ÄúI experienced sense of touch with awe.‚Äù
‚Äúmy mind was insatiably curious and wanted to deploy its sensors into the world and discover all things.‚Äù
‚ÄúMy brain wanted to stare, study and marvel.‚Äù
‚ÄúThe flavor exploded in my mouth.‚Äù
Restored my perception to youthful levels, returning them to factory settings and dissolving my aged numbness.

[![](https://nitter.net/pic/media%2FHBinUHUbAAA6fkY.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBinUHUbAAA6fkY.png)

2

Nerdy Rodent üêÄü§ìüíªü™êüö¥ retweeted

[![](https://nitter.net/pic/profile_images%2F1695024885070737408%2F-M-HSH5P_bigger.jpg)](https://nitter.net/GoogleDeepMind)

[Google DeepMind](https://nitter.net/GoogleDeepMind "Google DeepMind")

[@GoogleDeepMind](https://nitter.net/GoogleDeepMind "@GoogleDeepMind")

[Feb 19](https://nitter.net/GoogleDeepMind/status/2024516464892334129#m "Feb 19, 2026 ¬∑ 4:08 PM UTC")

Gemini 3.1 Pro is here.

We‚Äôve significantly improved the model‚Äôs overall intelligence so it can solve tougher problems. üßµ

262

745

6,236

k retweeted

[![](https://nitter.net/pic/profile_images%2F2006595716744097795%2F0yPqJVy9_bigger.jpg)](https://nitter.net/Pokemon)

[Pok√©mon](https://nitter.net/Pokemon "Pok√©mon")

[@Pokemon](https://nitter.net/Pokemon "@Pokemon")

[Feb 20](https://nitter.net/Pokemon/status/2024757682851233891#m "Feb 20, 2026 ¬∑ 8:07 AM UTC")

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Pok√©mon FireRed and Pok√©mon LeafGreen confirmed for Nintendo Switch!
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
These download-exclusive titles will be available after the [#PokemonDay](https://nitter.net/search?f=tweets&q=%23PokemonDay) Presents presentation which begins Friday, February 27, 2026, at 6AM PST. [#PokemonFRLG](https://nitter.net/search?f=tweets&q=%23PokemonFRLG)

![](https://nitter.net/pic/media%2FHBliYZkXAAA6h5X.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

2,489

12,978

89,703

Lewis Tunstall retweeted

[![](https://nitter.net/pic/profile_images%2F1889075959472476162%2FD9M1omhy_bigger.jpg)](https://nitter.net/j_dekoninck)

[Jasper Dekoninck](https://nitter.net/j_dekoninck "Jasper Dekoninck") [@j\_dekoninck](https://nitter.net/j_dekoninck "@j_dekoninck")

[Feb 20](https://nitter.net/j_dekoninck/status/2024776537262981408#m "Feb 20, 2026 ¬∑ 9:21 AM UTC")

Regarding Claude-Opus-4.6 (high) on MathArena:
\- It cost us $440 to get partial results on Apex Shortlist (only 90% finished), 4x the cost of any other model
\- Its performance was 55%, around Grok 4 Fast level
\- Many errors were caused by Opus hitting its maximum 128k token limit

8

3

73

[![](https://nitter.net/pic/profile_images%2F1358834299538051072%2FF0cQFEjK_bigger.jpg)](https://nitter.net/DeepLearningAI)

[DeepLearning.AI](https://nitter.net/DeepLearningAI "DeepLearning.AI")

[@DeepLearningAI](https://nitter.net/DeepLearningAI "@DeepLearningAI")

[21h](https://nitter.net/DeepLearningAI/status/2024863851464753272#m "Feb 20, 2026 ¬∑ 3:08 PM UTC")

A nonprofit called the AI Verification and Research Institute (Averi) aims to establish standards for independent audits of AI systems, evaluating risks like misuse, data leaks, and harmful behavior.

By defining audit principles, the organization seeks to make safety reviews a routine part of development and help build public trust in AI.

Learn more in The Batch ‚Üì [hubs.la/Q043-pf\_0](https://hubs.la/Q043-pf_0)

[![](https://nitter.net/pic/card_img%2F2024863852953817088%2FqS-eufcm%3Fformat%3Djpg%26name%3D800x419)\\
\\
**OpenAI Alumni Found Averi to Set Standards for AI Model Audits** \\
\\
AI is becoming ubiquitous, yet no standards exist for auditing its safety and security to make sure AI systems don‚Äôt assist, say, hackers or...\\
\\
deeplearning.ai](https://hubs.la/Q043-pf_0)

2

1

23

[![](https://nitter.net/pic/profile_images%2F1790643775401754626%2Fcr9uM-ie_bigger.png)](https://nitter.net/ben_burtenshaw)

[Ben Burtenshaw](https://nitter.net/ben_burtenshaw "Ben Burtenshaw")

[@ben\_burtenshaw](https://nitter.net/ben_burtenshaw "@ben_burtenshaw")

[21h](https://nitter.net/ben_burtenshaw/status/2024859763855544499#m "Feb 20, 2026 ¬∑ 2:52 PM UTC")

nano harness is a code first agent harness in 223 lines of code. it's rough, ready, and very hackable.

I had a lot of fun hacking with this today and learnt a lot about code first/ code act harnesses. take it for a spin and let me know if it's useful.

gist in the thread.

![](https://nitter.net/pic/amplify_video_thumb%2F2024858832644546560%2Fimg%2FXWevCztGEsWXwJSG.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

1

4

33

[more replies](https://nitter.net/AymericRoucher/status/2024862522679992627#m)

[![](https://nitter.net/pic/profile_images%2F1993095459338846208%2Fnm8-u0zh_bigger.jpg)](https://nitter.net/AymericRoucher)

[m\_ric](https://nitter.net/AymericRoucher "m_ric")

[@AymericRoucher](https://nitter.net/AymericRoucher "@AymericRoucher")

[21h](https://nitter.net/AymericRoucher/status/2024862522679992627#m "Feb 20, 2026 ¬∑ 3:03 PM UTC")

Nice work!
Don't hesitate to build a bit with smolagent's CodeAgent if you're interested in code-actions agents, I'm less involved now but would still love to look at what improvements you could propose!

1

1

[![](https://nitter.net/pic/profile_images%2F1790643775401754626%2Fcr9uM-ie_bigger.png)](https://nitter.net/ben_burtenshaw)

[Ben Burtenshaw](https://nitter.net/ben_burtenshaw "Ben Burtenshaw")

[@ben\_burtenshaw](https://nitter.net/ben_burtenshaw "@ben_burtenshaw")

[21h](https://nitter.net/ben_burtenshaw/status/2024863609801601228#m "Feb 20, 2026 ¬∑ 3:07 PM UTC")

thanks. yes, this is obviously very much inspired by it but it was mainly a personal education quest, more than a usable tool.

it would be cool to benchmark a 'smolagents cli' with modern models to see where it stands too compared to modern harnesses.

2

[![](https://nitter.net/pic/profile_images%2F1728327996375719936%2FRW7VBJfD_bigger.jpg)](https://nitter.net/kimmonismus)

[Chubby‚ô®Ô∏è](https://nitter.net/kimmonismus "Chubby‚ô®Ô∏è")

[@kimmonismus](https://nitter.net/kimmonismus "@kimmonismus")

[21h](https://nitter.net/kimmonismus/status/2024861969400901715#m "Feb 20, 2026 ¬∑ 3:01 PM UTC")

OpenAIs first device february 2027: a smart speaker

OpenAI has assembled a 200-person team to launch a new family of AI-powered devices, starting with a $200‚Äì$300 smart speaker expected no earlier than February 2027.

Designed in collaboration with Jony Ive‚Äôs LoveFrom, the speaker will feature a camera, environmental awareness, and Face ID-style purchasing - aiming to proactively ‚Äúnudge‚Äù users toward better decisions.

[![](https://nitter.net/pic/media%2FHBnAihCWEAAsqbo.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnAihCWEAAsqbo.jpg)

![](https://nitter.net/pic/profile_images%2F1747484907163516928%2Fd_K3Z_hH_mini.jpg)[The Information](https://nitter.net/theinformation "The Information")

[@theinformation](https://nitter.net/theinformation "@theinformation")

[22h](https://nitter.net/theinformation/status/2024850040510931066#m "Feb 20, 2026 ¬∑ 2:14 PM UTC")

Exclusive: OpenAI‚Äôs first device is expected to release a smart speaker with a camera, able to take in information about users and its surroundings.

Read more from [@Steph\_Palazzolo](https://nitter.net/steph_palazzolo "Stephanie Palazzolo") and [@QianerLiu](https://nitter.net/QianerLiu "Qianer Liu") üëá
[thein.fo/4kMndnr](https://thein.fo/4kMndnr)

42

29

309

[![](https://nitter.net/pic/profile_images%2F1728327996375719936%2FRW7VBJfD_bigger.jpg)](https://nitter.net/kimmonismus)

[Chubby‚ô®Ô∏è](https://nitter.net/kimmonismus "Chubby‚ô®Ô∏è")

[@kimmonismus](https://nitter.net/kimmonismus "@kimmonismus")

[21h](https://nitter.net/kimmonismus/status/2024862475590443254#m "Feb 20, 2026 ¬∑ 3:03 PM UTC")

Yes, 2027 - you read it right.

Thats at least what the information says

[![](https://nitter.net/pic/media%2FHBnBqkpWgAAwcdM.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnBqkpWgAAwcdM.jpg)

1

4

43

Hamel Husain retweeted

[![](https://nitter.net/pic/profile_images%2F1894484656403156992%2FnkkUdAyZ_bigger.jpg)](https://nitter.net/mattzcarey)

[Matt Carey](https://nitter.net/mattzcarey "Matt Carey")

[@mattzcarey](https://nitter.net/mattzcarey "@mattzcarey")

[22h](https://nitter.net/mattzcarey/status/2024847630811980277#m "Feb 20, 2026 ¬∑ 2:04 PM UTC")

Code Mode is all you need, very excited about this direction for MCP

[blog.cloudflare.com/code-mod‚Ä¶](https://blog.cloudflare.com/code-mode-mcp/)

[![](https://nitter.net/pic/media%2FHBm0Ae1XMAIQsM2.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBm0Ae1XMAIQsM2.jpg)

133

315

3,371

[![](https://nitter.net/pic/profile_images%2F1557421659925237763%2FRzJu2YI3_bigger.jpg)](https://nitter.net/skalskip92)

[SkalskiP](https://nitter.net/skalskip92 "SkalskiP")

[@skalskip92](https://nitter.net/skalskip92 "@skalskip92")

[21h](https://nitter.net/skalskip92/status/2024861373323227354#m "Feb 20, 2026 ¬∑ 2:59 PM UTC")

we are trending on github! let's go!

![](https://nitter.net/pic/profile_images%2F1557421659925237763%2FRzJu2YI3_mini.jpg)[SkalskiP](https://nitter.net/skalskip92 "SkalskiP")

[@skalskip92](https://nitter.net/skalskip92 "@skalskip92")

[Feb 18](https://nitter.net/skalskip92/status/2024191349201736153#m "Feb 18, 2026 ¬∑ 6:36 PM UTC")

trackers-2.2.0 released.

added trackers track CLI command. full tracking pipeline from the command line.

test with webcam:

trackers track --source 0 \
 --model rfdetr-medium \
 --tracker bytetrack \
 --show-trajectories

link: [github.com/roboflow/trackers](https://github.com/roboflow/trackers)

![](https://nitter.net/pic/amplify_video_thumb%2F2024185914855968768%2Fimg%2FOW0y94O4QOV1G6IL.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

9

12

208

[![](https://nitter.net/pic/profile_images%2F1630952793610518528%2FJ7HUofm9_bigger.jpg)](https://nitter.net/kylebrussell)

[Kyle Russell](https://nitter.net/kylebrussell "Kyle Russell")

[@kylebrussell](https://nitter.net/kylebrussell "@kylebrussell")

[21h](https://nitter.net/kylebrussell/status/2024860815711511030#m "Feb 20, 2026 ¬∑ 2:56 PM UTC")

no hun I am not doing 'karate moves' in the living room

in shotokan karate it is called 'kata'

1

Peter Welinder retweeted

[![](https://nitter.net/pic/profile_images%2F1953339828738899968%2FWWQlU2RT_bigger.jpg)](https://nitter.net/thsottiaux)

[Tibo](https://nitter.net/thsottiaux "Tibo")

[@thsottiaux](https://nitter.net/thsottiaux "@thsottiaux")

[Feb 19](https://nitter.net/thsottiaux/status/2024493074433618311#m "Feb 19, 2026 ¬∑ 2:35 PM UTC")

You only need your ChatGPT subscription to use Codex. It‚Äôs already included and even the plus subscription has very generous usage.

The reason we can do this is because gpt-5.3-codex achieves SoTA at lower cost than anything else out there.

[chatgpt.com/codex](https://chatgpt.com/codex)

[![](https://nitter.net/pic/card_img%2F2023750540493312002%2Fm2Jw3DkF%3Fformat%3Djpg%26name%3D800x419)\\
\\
**Codex** \\
\\
A cloud-based software engineering agent that answers codebase questions, executes code, and drafts pull requests.\\
\\
chatgpt.com](https://chatgpt.com/codex)

255

98

3,003

[Load more](https://nitter.net/i/lists/1585430245762441216?cursor=DAABCgABHBraGT1__e4KAAIcGb_hENogvwgAAwAAAAIAAA)