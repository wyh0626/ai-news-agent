[![](https://nitter.net/pic/list_banner_img%2F1613505644405075970%2F9VRDCFnW%3Fformat%3Djpg%26name%3Dorig)](https://nitter.net/pic/list_banner_img%2F1613505644405075970%2F9VRDCFnW%3Fformat%3Djpg%26name%3Dorig)

"AI High Signal" by @

AI twitter accounts that are high signal

- [Tweets](https://nitter.net/i/lists/1585430245762441216)
- [Members](https://nitter.net/i/lists/1585430245762441216/members)

[Load newest](https://nitter.net/i/lists/1585430245762441216)

[![](https://nitter.net/pic/profile_images%2F1613558765764374528%2FaZQB6U4b_bigger.jpg)](https://nitter.net/lateinteraction)

[Omar Khattab](https://nitter.net/lateinteraction "Omar Khattab")

[@lateinteraction](https://nitter.net/lateinteraction "@lateinteraction")

[15h](https://nitter.net/lateinteraction/status/2025005069926597065#m "Feb 21, 2026 ¬∑ 12:30 AM UTC")

I don‚Äôt get all the hate that the phrase ‚Äústochastic parrots‚Äù gets.

The main thing you should learn from all this is just how powerful very very very large parrots would be.

[![](https://nitter.net/pic/media%2FHBpDYMrXgAE3I5W.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBpDYMrXgAE3I5W.jpg)

7

3

74

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[15h](https://nitter.net/teortaxesTex/status/2025005004038312405#m "Feb 21, 2026 ¬∑ 12:29 AM UTC")

this is actually a more profound point than the usual "Aryans and honorary aryans love nature!" hippie nonsense. Punch is the perfect mirror for the white rightoid. Rejected and bullied by his own tribe, he seeks solace in a dead simulacrum (Japanese content). Deep‚Ä¶

[![](https://nitter.net/pic/media%2FHBpCnxGXEAAuVlf.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBpCnxGXEAAuVlf.png)

![](https://nitter.net/pic/profile_images%2F1998629858390261760%2FBwGRReaY_mini.jpg)[Beaver ü¶Å](https://nitter.net/beaverd "Beaver ü¶Å")

[@beaverd](https://nitter.net/beaverd "@beaverd")

[Feb 19](https://nitter.net/beaverd/status/2024582130995470777#m "Feb 19, 2026 ¬∑ 8:29 PM UTC")

Only white and Japanese people care about punch

Everyone else would literally eat him for dinner

[![](https://nitter.net/pic/media%2FHBjCt4tWIAAhvuv.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBjCt4tWIAAhvuv.jpg)

13

21

352

Baseten retweeted

[![](https://nitter.net/pic/profile_images%2F1518963020407263232%2F1y5H7llO_bigger.jpg)](https://nitter.net/tuhinone)

[Tuhin Srivastava](https://nitter.net/tuhinone "Tuhin Srivastava")

[@tuhinone](https://nitter.net/tuhinone "@tuhinone")

[15h](https://nitter.net/tuhinone/status/2025001376577978603#m "Feb 21, 2026 ¬∑ 12:15 AM UTC")

"you can't vibe-code uptime"

![](https://nitter.net/pic/profile_images%2F2011149691296079872%2FmYfpw44g_mini.jpg)[tried](https://nitter.net/tryd "tried") [@tryd](https://nitter.net/tryd "@tryd")

[19h](https://nitter.net/tryd/status/2024944482840371653#m "Feb 20, 2026 ¬∑ 8:29 PM UTC")

Replying to [@martin\_casado](https://nitter.net/martin_casado)

infra is the last hiring bottleneck because it's the one role where the system punishes you in real-time for not understanding it. you can vibe-code a feature. you can't vibe-code uptime.

1

3

16

Karina Nguyen retweeted

[![](https://nitter.net/pic/profile_images%2F1976939058741039104%2Fr3GgzqRh_bigger.jpg)](https://nitter.net/trq212)

[Thariq](https://nitter.net/trq212 "Thariq")

[@trq212](https://nitter.net/trq212 "@trq212")

[19h](https://nitter.net/trq212/status/2024937919937741290#m "Feb 20, 2026 ¬∑ 8:03 PM UTC")

Opus4.6 found 500+ vulnerabilities in open-source code and we've begun reporting them and contributing patches

quick excerpts from some of them üßµ

![](https://nitter.net/pic/profile_images%2F1950950107937185792%2FQOfEjFoJ_mini.jpg)[Claude](https://nitter.net/claudeai "Claude")

[@claudeai](https://nitter.net/claudeai "@claudeai")

[21h](https://nitter.net/claudeai/status/2024907535145468326#m "Feb 20, 2026 ¬∑ 6:02 PM UTC")

Introducing Claude Code Security, now in limited research preview.

It scans codebases for vulnerabilities and suggests targeted software patches for human review, allowing teams to find and fix issues that traditional tools often miss.

Learn more: [anthropic.com/news/claude-co‚Ä¶](https://www.anthropic.com/news/claude-code-security)

![](https://nitter.net/pic/media%2FHBntcaZaoAA7SFR.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

74

45

1,191

[![](https://nitter.net/pic/profile_images%2F1867875781676007424%2FRIF4Kt7U_bigger.jpg)](https://nitter.net/swyx)

[swyx](https://nitter.net/swyx "swyx")

[@swyx](https://nitter.net/swyx "@swyx")

[15h](https://nitter.net/swyx/status/2025004107464110103#m "Feb 21, 2026 ¬∑ 12:26 AM UTC")

\## the egocentric fallacy: "the most important part of a trend will happen to be right now when i randomly decide to care a lot about it because of some dude's post"

just to punch strawmen with Han (whom i love), this meta-principle of "choose the laggards in hopes they catch up" is a noob mistake and has a lower chance of working than most people are trained to appreciate (because they fundamentally want to believe in equality when capitalism fundamentally does not).

generation after generation of investors from Thiel to [@Chamath](https://nitter.net/Chamath "Chamath Palihapitiya") have learned painful lessons to double down on winners rather than do relative value trades.

a simple thought exercise is to ask if the Software Engineering number goes up to 80% or 90% in the next 5 years would you be better of working on Back Office Automation? (no, the answer is no, some folks are so capitalism illiterate you need that to be spelled out) . So then if we just zoomed from 10 to 50% in the last 2 years, it is actually a classic egotistical fallacy that this exact moment is when AI x SWE peaked and it's all relative downside from here vs other domains, which is what is implied by the chart below: who are you, who are we, what evidence do you have, to say we are anywhere even close to done on the largest product market fit we've seen in software business history?

[@nlw](https://nitter.net/nlw "Nathaniel Whittemore") has been echoing my thoughts here - kode is the lions share of agi and basically by betting on anything else you may well be rich and you may well be happier and you may well have much more positive social impact on the world.... but do not confuse that for having higher expected value.

[![](https://nitter.net/pic/media%2FHBpCLR_bYAAwMRE.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBpCLR_bYAAwMRE.jpg)

![](https://nitter.net/pic/profile_images%2F1905140107361062912%2FdJ4yYzz9_mini.jpg)[Han Wang](https://nitter.net/handotdev "Han Wang")

[@handotdev](https://nitter.net/handotdev "@handotdev")

[23h](https://nitter.net/handotdev/status/2024883980991099180#m "Feb 20, 2026 ¬∑ 4:28 PM UTC")

what I would be working on if I started another company today

[![](https://nitter.net/pic/media%2FHBnVKolbgAAUdyf.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnVKolbgAAUdyf.jpg)

9

1

35

[![](https://nitter.net/pic/profile_images%2F1769135517784756224%2FCEDlQZDc_bigger.jpg)](https://nitter.net/BlackHC)

[Andreas Kirsch üá∫üá¶](https://nitter.net/BlackHC "Andreas Kirsch üá∫üá¶")

[@BlackHC](https://nitter.net/BlackHC "@BlackHC")

[15h](https://nitter.net/BlackHC/status/2025004027155537926#m "Feb 21, 2026 ¬∑ 12:25 AM UTC")

I'm still so annoyed at myself for messing up a pretty straightforward coding interview a few weeks back because I didn't think it worth spending time on leetcode for a bit. Turns out it's always helpful regardless how much time I spend writing regular code in my day-to-day üôÑüôà

8

116

[![](https://nitter.net/pic/profile_images%2F1917271620059906048%2FEOsPA1wh_bigger.jpg)](https://nitter.net/torchcompiled)

[Ethan](https://nitter.net/torchcompiled "Ethan")

[@torchcompiled](https://nitter.net/torchcompiled "@torchcompiled")

[15h](https://nitter.net/torchcompiled/status/2025003776432898230#m "Feb 21, 2026 ¬∑ 12:24 AM UTC")

This should be a given. Although there are definitely cases you could want to do some ops on CPU if you have crap load of multiprocessing. The real trick is to transfer between GPU and CPU in low precision

![](https://nitter.net/pic/profile_images%2F1868297128801390593%2FOvl677JQ_mini.jpg)[Avi Chawla](https://nitter.net/_avichawla "Avi Chawla")

[@\_avichawla](https://nitter.net/_avichawla "@_avichawla")

[Feb 20](https://nitter.net/_avichawla/status/2024733274392911885#m "Feb 20, 2026 ¬∑ 6:30 AM UTC")

Here's a neural net optimization trick that leads to ~4x faster CPU to GPU transfers.

Imagine an image classification task.

\- We define the network, load the data and transform it.
\- In the training loop, we transfer the data to the GPU and train.

Here's the problem with this:

If you look at the profiler:
\- Most of the time/resources will be allocated to the kernel (the actual training code).
\- However, a significant amount of time will also be dedicated to data transfer from CPU to GPU (this appears under cudaMemcpyAsync).

Reducing the data transfer is simple.

Recall that the original dataset was composed of pixel values. These were 8-bit integers, and we transformed them to 32-bit floats.

Next, we transferred these 32-bit floating-point tensors to the GPU. This meant that transforming the data led to more data (4x) being transferred.

The solution is simple.

Moving the transformation step after the data transfer, since in that case, we shall transfer 8-bit integers instead of 32-bit floats.

As a result, you will notice a significant drop in the data transfer step.

Of course, this technique doesn‚Äôt apply to all neural network use cases, like NLP, where we inherently deal with 32-bit float embeddings.

However, whenever I have identified any possibility to use this trick, I have experienced noticeable gains from it.

üëâ Over to you: What other NN optimization techniques are you aware of?

[![](https://nitter.net/pic/media%2FHBlMLpRbUAQbjCc.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBlMLpRbUAQbjCc.jpg)

6

37

[![](https://nitter.net/pic/profile_images%2F2006806326140350470%2FKd5oZv-f_bigger.jpg)](https://nitter.net/fchollet)

[Fran√ßois Chollet](https://nitter.net/fchollet "Fran√ßois Chollet")

[@fchollet](https://nitter.net/fchollet "@fchollet")

[15h](https://nitter.net/fchollet/status/2025003396655448129#m "Feb 21, 2026 ¬∑ 12:23 AM UTC")

If you're looking to buy a Mac Mini, wait 4-6 months, a lot of used Mac Minis in mint condition are about to hit the market

155

287

8,566

[![](https://nitter.net/pic/profile_images%2F1767420324487090176%2FEofNUGW2_bigger.jpg)](https://nitter.net/RichardMCNgo)

[Richard Ngo](https://nitter.net/RichardMCNgo "Richard Ngo")

[@RichardMCNgo](https://nitter.net/RichardMCNgo "@RichardMCNgo")

[15h](https://nitter.net/RichardMCNgo/status/2025001871782674552#m "Feb 21, 2026 ¬∑ 12:17 AM UTC")

The longer I spend trying to understand intelligence the more impressed with MIRI‚Äôs agent foundations work I become.

I keep flailing in a direction that seems interesting, then finding that not only did they already have the broad intuition, they also elegantly formalized it.

4

11

219

[![](https://nitter.net/pic/profile_images%2F1767420324487090176%2FEofNUGW2_bigger.jpg)](https://nitter.net/RichardMCNgo)

[Richard Ngo](https://nitter.net/RichardMCNgo "Richard Ngo")

[@RichardMCNgo](https://nitter.net/RichardMCNgo "@RichardMCNgo")

[15h](https://nitter.net/RichardMCNgo/status/2025003335275999594#m "Feb 21, 2026 ¬∑ 12:23 AM UTC")

I don‚Äôt know if my understanding is improving fast enough that I‚Äôll ever hit the frontier, but I now have enough of a sense of the beautiful theory of bounded rationality waiting for us that it definitely seems worth trying.

I‚Äôm very sad the MIRI AF team disbanded.

8

81

[![](https://nitter.net/pic/profile_images%2F1994575845344907265%2F_owb9EuD_bigger.jpg)](https://nitter.net/_arohan_)

[rohan anil](https://nitter.net/_arohan_ "rohan anil")

[@\_arohan\_](https://nitter.net/_arohan_ "@_arohan_")

[15h](https://nitter.net/_arohan_/status/2025002329280577750#m "Feb 21, 2026 ¬∑ 12:19 AM UTC")

Last time I freaked out ‚Ä¶ this time I am not freaking out for a few reasons, main one is that this is expected when you automate economically valuable tasks.

![](https://nitter.net/pic/profile_images%2F2021827383431757824%2FAeVvT0rU_mini.jpg)[METR](https://nitter.net/METR_Evals "METR")

[@METR\_Evals](https://nitter.net/METR_Evals "@METR_Evals")

[20h](https://nitter.net/METR_Evals/status/2024923422867030027#m "Feb 20, 2026 ¬∑ 7:05 PM UTC")

We estimate that Claude Opus 4.6 has a 50%-time-horizon of around 14.5 hours (95% CI of 6 hrs to 98 hrs) on software tasks. While this is the highest point estimate we‚Äôve reported, this measurement is extremely noisy because our current task suite is nearly saturated.

[![](https://nitter.net/pic/media%2FHBn4OVDbgAM7aJ5.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBn4OVDbgAM7aJ5.jpg)

1

2

35

[![](https://nitter.net/pic/profile_images%2F1994575845344907265%2F_owb9EuD_bigger.jpg)](https://nitter.net/_arohan_)

[rohan anil](https://nitter.net/_arohan_ "rohan anil")

[@\_arohan\_](https://nitter.net/_arohan_ "@_arohan_")

[15h](https://nitter.net/_arohan_/status/2025002677328118201#m "Feb 21, 2026 ¬∑ 12:20 AM UTC")

Freaking out is mainly due to predictions mismatching expectations. If you start thinking about predictions carefully, this is the expected outcome.

1

6

[![](https://nitter.net/pic/profile_images%2F1672359156391591939%2FKrJs4zUQ_bigger.jpg)](https://nitter.net/imjaredz)

[Jared Zoneraich](https://nitter.net/imjaredz "Jared Zoneraich")

[@imjaredz](https://nitter.net/imjaredz "@imjaredz")

[15h](https://nitter.net/imjaredz/status/2025002611393380740#m "Feb 21, 2026 ¬∑ 12:20 AM UTC")

OpenClaw is a good harness, sir.

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[15h](https://nitter.net/teortaxesTex/status/2025000942957940876#m "Feb 21, 2026 ¬∑ 12:13 AM UTC")

Unhinged map. Japan?
It's also funny how by this account the US is actually doing well in terms of isolating China. Myanmar, Laos, Vietnam, Kyrghyzstan, Tajikistan, North Korea (???) moved towards the US. Those have borders with China. Cambodia, Thailand: no land borders.

[![](https://nitter.net/pic/media%2FHBo-5y4WwAA-71W.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBo-5y4WwAA-71W.jpg)

![](https://nitter.net/pic/profile_images%2F1854837751239348224%2FwuWINJYf_mini.jpg)[Jostein Hauge](https://nitter.net/haugejostein "Jostein Hauge")

[@haugejostein](https://nitter.net/haugejostein "@haugejostein")

[Feb 20](https://nitter.net/haugejostein/status/2024820076902261050#m "Feb 20, 2026 ¬∑ 12:15 PM UTC")

This map reveals whether countries have moved their alignment more toward China or the US since Trump came to power.

Europe‚Äôs shift is stunning: \*every single country\* in Europe has moved its alignment more toward China.

[![](https://nitter.net/pic/media%2FHBh7O9YXwAAAwSi.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBh7O9YXwAAAwSi.jpg)

7

2

29

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[15h](https://nitter.net/teortaxesTex/status/2025002215962812762#m "Feb 21, 2026 ¬∑ 12:18 AM UTC")

it's just UN voting though
well Trump prioritizes his Board of Peace

[![](https://nitter.net/pic/media%2FHBpAuZ-XMAIt7FN.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBpAuZ-XMAIt7FN.jpg)

1

8

Andriy Mulyar retweeted

[![](https://nitter.net/pic/profile_images%2F1954621475714998273%2FtM1mjCtC_bigger.jpg)](https://nitter.net/sarooshkhan98)

[Saroosh Khan](https://nitter.net/sarooshkhan98 "Saroosh Khan")

[@sarooshkhan98](https://nitter.net/sarooshkhan98 "@sarooshkhan98")

[Feb 19](https://nitter.net/sarooshkhan98/status/2024625330594648332#m "Feb 19, 2026 ¬∑ 11:21 PM UTC")

Updated results on ARC AGI 2 ( [@arcprize](https://nitter.net/arcprize "ARC Prize") )

Ran my January 27th attempt on Gemini 3.1 Pro Preview by [@GoogleDeepMind](https://nitter.net/GoogleDeepMind "Google DeepMind")

Achieved 94.1% at $ 8.71/task. (reasoning set to high in OpenRouter)

Some key differences I noticed vs Claude Opus 4.6

\- Cheaper and uses less tokens overall.
\- Doesn't really abide by tool\_calling and tends to respond with text even when prompted to call tools in sequences.
\- API runs into errors when the model can't trace back on the reasoning trace (retry logic in code handles it)
\- No regressions vs opus! Gemini 3.1 Pro solved 11 tasks that were unsolved previously

Overall, the tool output traces show very deep understanding and ability to use code execution sandbox to understand when its approach is failing and update hypothesis.

Code to reproduce: [kaggle.com/code/sarooshkhan8‚Ä¶](https://www.kaggle.com/code/sarooshkhan897/arc-ttt-gemini/)
Blog covering my approach: [sarooshkhan24.substack.com/p‚Ä¶](https://sarooshkhan24.substack.com/p/solving-arc-with-test-time-adaptation) [@GregKamradt](https://nitter.net/GregKamradt "Greg Kamradt") [@fchollet](https://nitter.net/fchollet "Fran√ßois Chollet") [@mikeknoop](https://nitter.net/mikeknoop "Mike Knoop") [@OfficialLoganK](https://nitter.net/OfficialLoganK "Logan Kilpatrick")

[![](https://nitter.net/pic/card_img%2F2024623429450915840%2FsYxhMqgO%3Fformat%3Djpg%26name%3D100x100_2)\\
\\
**arc\_ttt\_gemini** \\
\\
Explore and run machine learning code with Kaggle Notebooks \| Using data from multiple data sources\\
\\
kaggle.com](https://www.kaggle.com/code/sarooshkhan897/arc-ttt-gemini/)

3

5

13

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[16h](https://nitter.net/scaling01/status/2024991400366919919#m "Feb 20, 2026 ¬∑ 11:35 PM UTC")

Sonnet 3.5 will always be my baby

today I decided to add Opus 4.5 to that list

two beautiful and perfect baby AGIs

4

1

136

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[15h](https://nitter.net/scaling01/status/2024999411311321469#m "Feb 21, 2026 ¬∑ 12:07 AM UTC")

(Sonnet 3.5 : Sonnet 3.6) : (Opus 4.5 : Opus 4.6)

17

[![](https://nitter.net/pic/profile_images%2F2013808054534578176%2FVPMsQlEI_bigger.jpg)](https://nitter.net/andrew_n_carr)

[Andrew Carr ü§∏](https://nitter.net/andrew_n_carr "Andrew Carr ü§∏")

[@andrew\_n\_carr](https://nitter.net/andrew_n_carr "@andrew_n_carr")

[15h](https://nitter.net/andrew_n_carr/status/2024998918367707274#m "Feb 21, 2026 ¬∑ 12:05 AM UTC")

Language models are the irrational numbers of intelligence. They fall between the gaps of evolution and human intelligence, but they still definitely have a place on the spectrum

1

6

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[16h](https://nitter.net/scaling01/status/2024990728158429583#m "Feb 20, 2026 ¬∑ 11:33 PM UTC")

btw if the new 99.1 day doubling time holds, this would imply ~144 hour or 6 day time horizons by end of year

with the pre-Opus-4.6 doubling time of ~105 days this would imply ~127 hours

![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_mini.jpg)[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[19h](https://nitter.net/scaling01/status/2024938665525268698#m "Feb 20, 2026 ¬∑ 8:06 PM UTC")

My estimate was 11.26 hours with a doubling time of 103.6 days

It might be even slightly faster than this at ~99 days

but it's really hard to say, the confidence intervals are crazy

[![](https://nitter.net/pic/media%2FHBoG6UDXoAIIw6v.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBoG6UDXoAIIw6v.jpg)

6

4

91

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[15h](https://nitter.net/scaling01/status/2024998747386569212#m "Feb 21, 2026 ¬∑ 12:04 AM UTC")

i wish i did this analysis a bit earlier than just end of jan

so i would've said 100 hour time horizons in my 2026 predictions

1

9

[![](https://nitter.net/pic/profile_images%2F1296667294148382721%2F9Pr6XrPB_bigger.jpg)](https://nitter.net/karpathy)

[Andrej Karpathy](https://nitter.net/karpathy "Andrej Karpathy")

[@karpathy](https://nitter.net/karpathy "@karpathy")

[16h](https://nitter.net/karpathy/status/2024987174077432126#m "Feb 20, 2026 ¬∑ 11:18 PM UTC")

Bought a new Mac mini to properly tinker with claws over the weekend. The apple store person told me they are selling like hotcakes and everyone is confused :)

I'm definitely a bit sus'd to run OpenClaw specifically - giving my private data/keys to 400K lines of vibe coded monster that is being actively attacked at scale is not very appealing at all. Already seeing reports of exposed instances, RCE vulnerabilities, supply chain poisoning, malicious or compromised skills in the registry, it feels like a complete wild west and a security nightmare. But I do love the concept and I think that just like LLM agents were a new layer on top of LLMs, Claws are now a new layer on top of LLM agents, taking the orchestration, scheduling, context, tool calls and a kind of persistence to a next level.

Looking around, and given that the high level idea is clear, there are a lot of smaller Claws starting to pop out. For example, on a quick skim NanoClaw looks really interesting in that the core engine is ~4000 lines of code (fits into both my head and that of AI agents, so it feels manageable, auditable, flexible, etc.) and runs everything in containers by default. I also love their approach to configurability - it's not done via config files it's done via skills! For example, /add-telegram instructs your AI agent how to modify the actual code to integrate Telegram. I haven't come across this yet and it slightly blew my mind earlier today as a new, AI-enabled approach to preventing config mess and if-then-else monsters. Basically - the implied new meta is to write the most maximally forkable repo and then have skills that fork it into any desired more exotic configuration. Very cool.

Anyway there are many others - e.g. nanobot, zeroclaw, ironclaw, picoclaw (lol @ prefixes). There are also cloud-hosted alternatives but tbh I don't love these because it feels much harder to tinker with. In particular, local setup allows easy connection to home automation gadgets on the local network. And I don't know, there is something aesthetically pleasing about there being a physical device 'possessed' by a little ghost of a personal digital house elf.

Not 100% sure what my setup ends up looking like just yet but Claws are an awesome, exciting new layer of the AI stack.

672

810

11,577

[![](https://nitter.net/pic/profile_images%2F1296667294148382721%2F9Pr6XrPB_bigger.jpg)](https://nitter.net/karpathy)

[Andrej Karpathy](https://nitter.net/karpathy "Andrej Karpathy")

[@karpathy](https://nitter.net/karpathy "@karpathy")

[15h](https://nitter.net/karpathy/status/2024997757757653224#m "Feb 21, 2026 ¬∑ 12:01 AM UTC")

First there was chat, then there was code, now there is claw. Ez

77

102

1,968

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[16h](https://nitter.net/teortaxesTex/status/2024997373336781107#m "Feb 20, 2026 ¬∑ 11:59 PM UTC")

Fair.
ML perspective is useful for sociology, I think more people must learn it on some level (avoiding quantum woo tier nonsense, hopefully).
"Mongolian", "Tibetan", even "Japanese" cultures Just Don't Scale. Only "Sinic" or "Western" cultures scale.
Two peaks of our species.

[![](https://nitter.net/pic/media%2FHBo7vNOWQAA5V_V.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBo7vNOWQAA5V_V.png)

[![](https://nitter.net/pic/media%2FHBo7zB4WoAArlAK.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBo7zB4WoAArlAK.jpg)

![](https://nitter.net/pic/profile_images%2F844437301459607552%2FaucDes-4_mini.jpg)[Bronze Age Pervert](https://nitter.net/bronzeagemantis "Bronze Age Pervert")

[@bronzeagemantis](https://nitter.net/bronzeagemantis "@bronzeagemantis")

[22h](https://nitter.net/bronzeagemantis/status/2024906599869939986#m "Feb 20, 2026 ¬∑ 5:58 PM UTC")

Mongolian and Tibetan culture and also Japanese are fundamentally opposed and hostile to Sinic culture despite mutual borrowings. It‚Äôs not the narcissism of small differences or merely nationalist sentiment these are mutually exclusive character types and visions of life

5

2

51

Eric Hartford retweeted

[![](https://nitter.net/pic/profile_images%2F1835392777217273856%2FTw-5ylX__bigger.jpg)](https://nitter.net/stuart_sul)

[Stuart Sul](https://nitter.net/stuart_sul "Stuart Sul")

[@stuart\_sul](https://nitter.net/stuart_sul "@stuart_sul")

[22h](https://nitter.net/stuart_sul/status/2024897621874422125#m "Feb 20, 2026 ¬∑ 5:23 PM UTC")

(1/7) We're releasing ThunderKittens 2.0! Faster kernels, cleaner code, industry contributions, and new state-of-the-art BF16 / MXFP8 / NVFP4 GEMMs that match or surpass cuBLAS!

Alongside this release, we‚Äôre equally excited to share some insights we learned while squeezing every last TFLOP out of Blackwell:

(with [@hazyresearch](https://nitter.net/hazyresearch "hazyresearch") & generously supported by [@cursor\_ai](https://nitter.net/cursor_ai "Cursor"))

[![](https://nitter.net/pic/media%2FHBnhmxsbgAYMkG8.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnhmxsbgAYMkG8.jpg)

11

78

459

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[16h](https://nitter.net/teortaxesTex/status/2024994052651839641#m "Feb 20, 2026 ¬∑ 11:46 PM UTC")

I am pessimistic on Taalas because I don't see how it scales to large contexts (read [@zephyr\_z9](https://nitter.net/zephyr_z9 "Zephyr") on memory). 60 ms to 1K completion with a 8B model is‚Ä¶ neat? A neat parlor trick.
What \*would\* excite me is 17Kt/s for 2M tokens.
<2 minutes for 40 thoughts of 50K tokens each.

[![](https://nitter.net/pic/media%2FHBo2aa2WEAIq_D_.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBo2aa2WEAIq_D_.jpg)

![](https://nitter.net/pic/profile_images%2F1469392086659211270%2FmSdvgVKW_mini.jpg)[Zoomer Alcibiades](https://nitter.net/HellenicVibes "Zoomer Alcibiades")

[@HellenicVibes](https://nitter.net/HellenicVibes "@HellenicVibes")

[16h](https://nitter.net/HellenicVibes/status/2024984519942766891#m "Feb 20, 2026 ¬∑ 11:08 PM UTC")

"Taalas‚Äô silicon Llama achieves 17K tokens/sec per user, nearly 10X faster than the current state of the art, while costing 20X less to build, and consuming 10X less power."

This is absolutely going to transform the AI economy, I have no idea how the big labs will adjust.

[![](https://nitter.net/pic/media%2FHBovj8obgAErTNG.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBovj8obgAErTNG.jpg)

8

5

61

Jacob Lee retweeted

[![](https://nitter.net/pic/profile_images%2F1568396542372974595%2F0pqCtiYe_bigger.jpg)](https://nitter.net/nuonjon)

[Jon Morehouse](https://nitter.net/nuonjon "Jon Morehouse")

[@nuonjon](https://nitter.net/nuonjon "@nuonjon")

[16h](https://nitter.net/nuonjon/status/2024993485435404718#m "Feb 20, 2026 ¬∑ 11:44 PM UTC")

I'm super bullish on ACP.

As an engineer, this is your portal to building your own unique context management and control loops on top of agents.

I've essentially turned my editor into an agent-first IDE using this.

![](https://nitter.net/pic/profile_images%2F1664120136117411840%2FcWy7VRn__mini.jpg)[Jacob Lee](https://nitter.net/Hacubu "Jacob Lee")

[@Hacubu](https://nitter.net/Hacubu "@Hacubu")

[Feb 17](https://nitter.net/Hacubu/status/2023804529314243012#m "Feb 17, 2026 ¬∑ 4:59 PM UTC")

Lately, I've gotten super excited about [@zeddotdev](https://nitter.net/zeddotdev "Zed")'s open-source Agent Client Protocol. It's a fantastic way to pass a coding agent the exact context you're looking at in an IDE.

That's why as a pat leave side project I built my own ACP client to replace my Claude Code usage!

It's a few hundred lines of Python hooked up to a [@LangChain](https://nitter.net/LangChain "LangChain") Deep Agent with filesystem and shell access.

It works great out of the box, but the real power comes from customizing the internal agent to use whatever prompts, models, tools, skills etc. you want.

Plus, you get full observability via LangSmith!

I've been using it the past few weeks to great effect. See the links in the replies to try it yourself ‚§µÔ∏è

2

1

6

[![](https://nitter.net/pic/profile_images%2F1909353910130950147%2FEeSGdgA5_bigger.jpg)](https://nitter.net/theo)

[Theo - t3.gg](https://nitter.net/theo "Theo - t3.gg")

[@theo](https://nitter.net/theo "@theo")

[16h](https://nitter.net/theo/status/2024993333593210955#m "Feb 20, 2026 ¬∑ 11:43 PM UTC")

Something appears to be very, very wrong with my Mac üôÉ

[![](https://nitter.net/pic/media%2FHBo4r6ObgAAAdeu.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBo4r6ObgAAAdeu.jpg)

71

3

532

Sri Muppidi retweeted

[![](https://nitter.net/pic/profile_images%2F1573117339800719360%2FSRlqC5Sa_bigger.jpg)](https://nitter.net/lauramandaro)

[Laura Mandaro](https://nitter.net/lauramandaro "Laura Mandaro") [@lauramandaro](https://nitter.net/lauramandaro "@lauramandaro")

[16h](https://nitter.net/lauramandaro/status/2024990884019012111#m "Feb 20, 2026 ¬∑ 11:33 PM UTC")

Scoop üö® OpenAI financials:
-Lifts revenue forecasts through 2030 by $141 billion
-Doubles cash burn forecast
-Missed margin target last year as compute costs surged

More in depth here via [@srimuppidi](https://nitter.net/srimuppidi "Sri Muppidi") [@steph\_palazzolo](https://nitter.net/steph_palazzolo "Stephanie Palazzolo") [theinformation.com/articles/‚Ä¶](https://www.theinformation.com/articles/openai-boost-revenue-forecasts-predicts-112-billion-cash-burn-2030?rc=zjctrx)

[![](https://nitter.net/pic/media%2FHBo10L5bgAEtaZm.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBo10L5bgAEtaZm.jpg)

1

3

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[16h](https://nitter.net/scaling01/status/2024992572075098519#m "Feb 20, 2026 ¬∑ 11:40 PM UTC")

Looks like we are getting a $50 or $100 ChatGPT plan

![](https://nitter.net/pic/profile_images%2F1682731537349967873%2Fz5eukaD2_mini.jpg)[Tibor Blaho](https://nitter.net/btibor91 "Tibor Blaho")

[@btibor91](https://nitter.net/btibor91 "@btibor91")

[16h](https://nitter.net/btibor91/status/2024992285591818329#m "Feb 20, 2026 ¬∑ 11:39 PM UTC")

ChatGPT web app code now mentions a new "ChatGPT Pro Lite" plan

[![](https://nitter.net/pic/media%2FHBo3wDQbcAA2ptB.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBo3wDQbcAA2ptB.jpg)

7

3

162

Cline retweeted

[![](https://nitter.net/pic/profile_images%2F1969682842100862976%2FqQWcJuLh_bigger.jpg)](https://nitter.net/sdrzn)

[Saoud Rizwan](https://nitter.net/sdrzn "Saoud Rizwan")

[@sdrzn](https://nitter.net/sdrzn "@sdrzn")

[16h](https://nitter.net/sdrzn/status/2024986545019912564#m "Feb 20, 2026 ¬∑ 11:16 PM UTC")

we are headed towards a future where laptops can run open source models good enough to do most work. it's early but undeniably the trajectory. excited to work with ollama to make this a reality!

![](https://nitter.net/pic/profile_images%2F2023482262030086147%2F9BAatmvy_mini.jpg)[ollama](https://nitter.net/ollama "ollama")

[@ollama](https://nitter.net/ollama "@ollama")

[17h](https://nitter.net/ollama/status/2024978932127187375#m "Feb 20, 2026 ¬∑ 10:46 PM UTC")

Ollama 0.16.3 is out with Cline and Pi integrations out of the box.

Try it with:

[@cline](https://nitter.net/cline "Cline"):

ollama launch cline

Pi:

ollama launch pi

[![](https://nitter.net/pic/media%2FHBorO1QXcAEZERG.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBorO1QXcAEZERG.jpg)

[![](https://nitter.net/pic/media%2FHBorO1_XQAARuHV.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBorO1_XQAARuHV.jpg)

[![](https://nitter.net/pic/media%2FHBorO1mXcAA4RlW.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBorO1mXcAA4RlW.jpg)

8

6

93

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[16h](https://nitter.net/teortaxesTex/status/2024991684287721555#m "Feb 20, 2026 ¬∑ 11:36 PM UTC")

what did I tell you about Wuyue supremacy, anon?
Zhejiang + Jiangsu + Shanghai are 75% of market cap of robotics parts in China. Biased towards Zhejiang. They are building the machines that'll build the machines that'll build everything else.

[![](https://nitter.net/pic/media%2FHBo2uyMW4AAf3CW.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBo2uyMW4AAf3CW.jpg)

1

8

40

[![](https://nitter.net/pic/profile_images%2F1728327996375719936%2FRW7VBJfD_bigger.jpg)](https://nitter.net/kimmonismus)

[Chubby‚ô®Ô∏è](https://nitter.net/kimmonismus "Chubby‚ô®Ô∏è")

[@kimmonismus](https://nitter.net/kimmonismus "@kimmonismus")

[16h](https://nitter.net/kimmonismus/status/2024990910618960181#m "Feb 20, 2026 ¬∑ 11:33 PM UTC")

I have this feeling that a strange vibe is in the air. Something between: "Okay, guys, it's really starting now, and it's happening faster and is more intense than we expected" ‚Äì and ‚Äì "It's already here, it's already taking over our jobs, we have to prepare for everything, it's happening now."

A mixture of tension, worry, unease, anticipation, and ‚Äì almost delusion. Something huge is definitely happening right now. And the people who control the big companies certainly aren't hiding it.

Do you feel it too?

155

52

1,001

[![](https://nitter.net/pic/profile_images%2F1664559115581145088%2FUMD1vtMw_bigger.jpg)](https://nitter.net/vikhyatk)

[vik](https://nitter.net/vikhyatk "vik")

[@vikhyatk](https://nitter.net/vikhyatk "@vikhyatk")

[16h](https://nitter.net/vikhyatk/status/2024990187542249862#m "Feb 20, 2026 ¬∑ 11:30 PM UTC")

i love working on kernels

[![](https://nitter.net/pic/media%2FHBo1zkZbgAAlV6K.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBo1zkZbgAAlV6K.jpg)

![](https://nitter.net/pic/profile_images%2F1835392777217273856%2FTw-5ylX__mini.jpg)[Stuart Sul](https://nitter.net/stuart_sul "Stuart Sul")

[@stuart\_sul](https://nitter.net/stuart_sul "@stuart_sul")

[22h](https://nitter.net/stuart_sul/status/2024897621874422125#m "Feb 20, 2026 ¬∑ 5:23 PM UTC")

(1/7) We're releasing ThunderKittens 2.0! Faster kernels, cleaner code, industry contributions, and new state-of-the-art BF16 / MXFP8 / NVFP4 GEMMs that match or surpass cuBLAS!

Alongside this release, we‚Äôre equally excited to share some insights we learned while squeezing every last TFLOP out of Blackwell:

(with [@hazyresearch](https://nitter.net/hazyresearch "hazyresearch") & generously supported by [@cursor\_ai](https://nitter.net/cursor_ai "Cursor"))

[![](https://nitter.net/pic/media%2FHBnhmxsbgAYMkG8.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnhmxsbgAYMkG8.jpg)

3

7

156

Pratyush Maini retweeted

[![](https://nitter.net/pic/profile_images%2F1589797246614347776%2FPZDz0w1v_bigger.jpg)](https://nitter.net/AdtRaghunathan)

[Aditi Raghunathan](https://nitter.net/AdtRaghunathan "Aditi Raghunathan")

[@AdtRaghunathan](https://nitter.net/AdtRaghunathan "@AdtRaghunathan")

[19h](https://nitter.net/AdtRaghunathan/status/2024944182595289418#m "Feb 20, 2026 ¬∑ 8:28 PM UTC")

We struggled to get monitors to catch reward hacking in agents... so we built a new lens. üî≠

Hodoscope allows you to visualize and audit agent trajectories at scale. Using it, we found a brand new vulnerability in commit0 in just minutes! ‚ö°Ô∏è

We're excited to see what you uncover! Use Hodoscope to inspect your own agent trajectories and make your systems more robust. üöÄ

![](https://nitter.net/pic/amplify_video_thumb%2F2024943757322305536%2Fimg%2F4Xu9MCAEsgHXvMB2.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

2

14

95

Pieter Abbeel retweeted

[![](https://nitter.net/pic/profile_images%2F1715504030984847360%2F9Oa-VDF2_bigger.jpg)](https://nitter.net/willjhliang)

[Will Liang](https://nitter.net/willjhliang "Will Liang")

[@willjhliang](https://nitter.net/willjhliang "@willjhliang")

[21h](https://nitter.net/willjhliang/status/2024913015737618462#m "Feb 20, 2026 ¬∑ 6:24 PM UTC")

Introducing DreamDojo, a generalist, interactive robot world model pretrained on 44k hours of human egocentric videos!

Scaling human data pretraining with the largest and most diverse dataset to date for robot world model learning, we achieve spectacular generalization to unseen object interactions and environments. Additionally, after distillation, DreamDojo achieves real-time generation at 10 FPS‚Äîyou can watch it predict the future as actions stream online, one frame at a time! With these capabilities, we show several key applications: live teloperation, policy evaluation, and model-based planning.

See our website: [dreamdojo-world.github.io](http://dreamdojo-world.github.io/)
As well as our paper: [arxiv.org/abs/2602.06949](http://arxiv.org/abs/2602.06949)
And github: [github.com/NVIDIA/DreamDojo](https://github.com/NVIDIA/DreamDojo)

And a quick dive with me‚Ä¶ üßµ

![](https://nitter.net/pic/amplify_video_thumb%2F2024912676586176512%2Fimg%2FBYvCcjRh-KdVYQhu.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

7

7

65

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[16h](https://nitter.net/teortaxesTex/status/2024987210089439306#m "Feb 20, 2026 ¬∑ 11:19 PM UTC")

\+ WeirdML
Another sign that the frontier is training for RSI already

![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_mini.jpg)[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[17h](https://nitter.net/scaling01/status/2024977966673871153#m "Feb 20, 2026 ¬∑ 10:42 PM UTC")

Sweet new benchmark: LLMs post-training 1.7B to 4B models

aka PostTrainBench

[![](https://nitter.net/pic/media%2FHBoqfs6XoAA2GBN.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBoqfs6XoAA2GBN.jpg)

3

1

126

[![](https://nitter.net/pic/profile_images%2F1815571798102245376%2FJAf-0RA8_bigger.jpg)](https://nitter.net/apoorvasriniva)

[Apoorva](https://nitter.net/apoorvasriniva "Apoorva")

[@apoorvasriniva](https://nitter.net/apoorvasriniva "@apoorvasriniva")

[16h](https://nitter.net/apoorvasriniva/status/2024987042187456890#m "Feb 20, 2026 ¬∑ 11:18 PM UTC")

alysa liu is my goat

1

2

[![](https://nitter.net/pic/profile_images%2F1833305113374371840%2FjyaxOqPL_bigger.jpg)](https://nitter.net/steph_palazzolo)

[Stephanie Palazzolo](https://nitter.net/steph_palazzolo "Stephanie Palazzolo") [@steph\_palazzolo](https://nitter.net/steph_palazzolo "@steph_palazzolo")

[16h](https://nitter.net/steph_palazzolo/status/2024986680902455705#m "Feb 20, 2026 ¬∑ 11:17 PM UTC")

A double scoop day w/ the latest on OpenAI financials.

Details include:
\- OpenAI raises rev forecasts for next 5 years by 27%
\- But will burn 2x as much cash thru 2030 than previously predicted
\- 2025 gross margins down vs. 2024
\- New info on device revenue forecasts!

[![](https://nitter.net/pic/media%2FHBoynjHXkAASA7k.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBoynjHXkAASA7k.jpg)

37

47

358

[![](https://nitter.net/pic/profile_images%2F1833305113374371840%2FjyaxOqPL_bigger.jpg)](https://nitter.net/steph_palazzolo)

[Stephanie Palazzolo](https://nitter.net/steph_palazzolo "Stephanie Palazzolo") [@steph\_palazzolo](https://nitter.net/steph_palazzolo "@steph_palazzolo")

[16h](https://nitter.net/steph_palazzolo/status/2024986728486851011#m "Feb 20, 2026 ¬∑ 11:17 PM UTC")

w/ [@srimuppidi](https://nitter.net/srimuppidi "Sri Muppidi") [theinformation.com/articles/‚Ä¶](https://www.theinformation.com/articles/openai-boost-revenue-forecasts-predicts-112-billion-cash-burn-2030)

[![](https://nitter.net/pic/card_img%2F2024979461699944448%2FK4GtjDsd%3Fformat%3Djpg%26name%3D800x419)\\
\\
**OpenAI Boost Revenue Forecasts, Predicts $112 Billion More Cash Burn Through 2030** \\
\\
OpenAI recently hiked its revenue outlook for the next five years, predicting that it would generate about 27% more than previously forecast from sales of its ChatGPT subscriptions, AI models, and...\\
\\
theinformation.com](https://www.theinformation.com/articles/openai-boost-revenue-forecasts-predicts-112-billion-cash-burn-2030)

14

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[16h](https://nitter.net/teortaxesTex/status/2024984139003502975#m "Feb 20, 2026 ¬∑ 11:06 PM UTC")

Israel might be the only democracy in the Middle East, but then the US is the only Jewish theocracy in the West, with dignitaries regularly appealing to the Bible. amusing.
Huckabee is Scots-Irish btw. Reminds me of the schizo Anglo theory that Anglos are a lost Hebrew tribe

![](https://nitter.net/pic/profile_images%2F1734226736093528064%2FqpPX7owf_mini.jpg)[Tucker Carlson Network](https://nitter.net/TCNetwork "Tucker Carlson Network")

[@TCNetwork](https://nitter.net/TCNetwork "@TCNetwork")

[21h](https://nitter.net/TCNetwork/status/2024911932092813478#m "Feb 20, 2026 ¬∑ 6:20 PM UTC")

BREAKING: US Ambassador to Israel Mike Huckabee tells Tucker Carlson that Israel has the Biblical right to take over all of the Middle East.

‚ÄúIt would be fine if they took it all.‚Äù

![](https://nitter.net/pic/amplify_video_thumb%2F2024910484210638848%2Fimg%2FUpBd_bQbiS3uvpUD.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

12

9

122

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[16h](https://nitter.net/teortaxesTex/status/2024985615867977771#m "Feb 20, 2026 ¬∑ 11:12 PM UTC")

A typical progressives-more-correct moment btw. Jews as such are okay, the problem is that Hajnalis are bred to empower petty ethnocentrism. Polished the cocks of the pan-European ethny of nobles before, now starved and want to prostrate before another confident high-IQ minority.

1

18

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[16h](https://nitter.net/teortaxesTex/status/2024986326907388393#m "Feb 20, 2026 ¬∑ 11:15 PM UTC")

Jews are just smarter-than-usual middle easterners, both on the individual and on the cultural level (Judaism + secular developments is better designed than the competition). Capable but normal human tribe.
Hajnalis are WEIRD and create bizarre outcomes in diverse societies

3

2

22

Jacob Lee retweeted

[![](https://nitter.net/pic/profile_images%2F1694405719951654912%2FfslkcAsW_bigger.jpg)](https://nitter.net/contrary)

[Contrary](https://nitter.net/contrary "Contrary")

[@contrary](https://nitter.net/contrary "@contrary")

[Feb 19](https://nitter.net/contrary/status/2024544478514856301#m "Feb 19, 2026 ¬∑ 5:59 PM UTC")

We‚Äôre thrilled to be hosting our next SF Tech Talk of the year with engineering leads from [@LangChain](https://nitter.net/LangChain "LangChain"), [@cognition](https://nitter.net/cognition "Cognition"), [@elevenlabsio](https://nitter.net/elevenlabsio "ElevenLabs"), and [@andocorporation](https://nitter.net/andocorporation "‚ú∂ Ando").

Designed by engineers, for engineers ‚Äî each company will deliver live demos of their latest features for leading builders in the Bay Area.

[![](https://nitter.net/pic/media%2FHBiepWKbYAAsKU2.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBiepWKbYAAsKU2.png)

1

7

27

[![](https://nitter.net/pic/profile_images%2F1347621377503711233%2FbHg3ipfD_bigger.jpg)](https://nitter.net/gdb)

[Greg Brockman](https://nitter.net/gdb "Greg Brockman")

[@gdb](https://nitter.net/gdb "@gdb")

[16h](https://nitter.net/gdb/status/2024985187579560366#m "Feb 20, 2026 ¬∑ 11:11 PM UTC")

seeing so much positive progress across each part of openai right now, very proud of the team

201

29

1,378

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[20h](https://nitter.net/scaling01/status/2024925692853395618#m "Feb 20, 2026 ¬∑ 7:14 PM UTC")

Welcome to the singularity

AI is taking off

[![](https://nitter.net/pic/media%2FHBn7HaOXUAAO28q.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBn7HaOXUAAO28q.jpg)

57

105

1,274

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[16h](https://nitter.net/scaling01/status/2024984936051028062#m "Feb 20, 2026 ¬∑ 11:10 PM UTC")

(has been taking off for quite a while actually)

8

[![](https://nitter.net/pic/profile_images%2F1595267590712090625%2FQQuvXr5r_bigger.jpg)](https://nitter.net/srimuppidi)

[Sri Muppidi](https://nitter.net/srimuppidi "Sri Muppidi")

[@srimuppidi](https://nitter.net/srimuppidi "@srimuppidi")

[16h](https://nitter.net/srimuppidi/status/2024984299158864245#m "Feb 20, 2026 ¬∑ 11:07 PM UTC")

üö® big scoop: OpenAI's latest projections and how it ended 2025.

üí∏ Ended 2025 w/ $13.1B in revenue, burned $8B in cash
üí∞ Targets $284B in revenue in 2030, when it expects to become cash flow positive w/ ~$40B in cash
üñ•Ô∏è Consumer ChatGPT + ads are main revenue drivers üíµExpects to burn $665B in computing costs through 2030

All of this comes as OpenAI tries to raise >$100B!

[![](https://nitter.net/pic/media%2FHBowKcAaoAAAJBu.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBowKcAaoAAAJBu.jpg)

1

1

6

[![](https://nitter.net/pic/profile_images%2F1595267590712090625%2FQQuvXr5r_bigger.jpg)](https://nitter.net/srimuppidi)

[Sri Muppidi](https://nitter.net/srimuppidi "Sri Muppidi")

[@srimuppidi](https://nitter.net/srimuppidi "@srimuppidi")

[16h](https://nitter.net/srimuppidi/status/2024984513529717068#m "Feb 20, 2026 ¬∑ 11:08 PM UTC")

my latest w/ [@steph\_palazzolo](https://nitter.net/steph_palazzolo "Stephanie Palazzolo"): [theinformation.com/articles/‚Ä¶](https://www.theinformation.com/articles/openai-boost-revenue-forecasts-predicts-112-billion-cash-burn-2030?rc=8tzurb)

[![](https://nitter.net/pic/card_img%2F2024984506810466305%2FwdczqSYx%3Fformat%3Djpg%26name%3D800x419)\\
\\
**OpenAI Boost Revenue Forecasts, Predicts $111 Billion More Cash Burn Through 2030** \\
\\
OpenAI recently hiked its revenue outlook for the next five years, predicting that it would generate about 27% more than previously forecast from sales of its ChatGPT subscriptions, AI models, and...\\
\\
theinformation.com](https://www.theinformation.com/articles/openai-boost-revenue-forecasts-predicts-112-billion-cash-burn-2030?rc=8tzurb)

1

2

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[17h](https://nitter.net/teortaxesTex/status/2024979713056206884#m "Feb 20, 2026 ¬∑ 10:49 PM UTC")

Having read their AMA, I confirm that \*\*Step Function\*\* culturally belongs to the highest tier of Chinese AGI labs, equal to DeepSeek. They have as clear an aim at the main quest, only less chill. Whale will build the Omniscientist; they, the Superagent. Bookmark this xeet.

[![](https://nitter.net/pic/media%2FHBosT2DWMAAcn_Q.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBosT2DWMAAcn_Q.jpg)

[![](https://nitter.net/pic/media%2FHBosT2EWsAA4HCB.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBosT2EWsAA4HCB.jpg)

![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_mini.jpg)[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[20h](https://nitter.net/teortaxesTex/status/2024922393689334229#m "Feb 20, 2026 ¬∑ 7:01 PM UTC")

growing more bullish on [@StepFun\_ai](https://nitter.net/StepFun_ai "StepFun") every day. There are many reasons to feel this, I'll just repeat 3 you might have missed.
1) long ago, [@Jianlin\_S](https://nitter.net/Jianlin_S "jianlin.su") had rated their in-house attention (MFA) as equivalent to MLA
2) they're \*obsessive\* about telemetry
3) they are building for RSI

[![](https://nitter.net/pic/media%2FHBnzZXDXkAAOm6a.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnzZXDXkAAOm6a.jpg)

[![](https://nitter.net/pic/media%2FHBnzZXAXAAAm3NS.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBnzZXAXAAAm3NS.jpg)

[![](https://nitter.net/pic/media%2FHBn3UNpXMAAegII.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBn3UNpXMAAegII.jpg)

[![](https://nitter.net/pic/media%2FHBn4E4zWEAAKjuk.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBn4E4zWEAAKjuk.jpg)

3

2

102

[more replies](https://nitter.net/teortaxesTex/status/2024982837938557430#m)

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[16h](https://nitter.net/teortaxesTex/status/2024982837938557430#m "Feb 20, 2026 ¬∑ 11:01 PM UTC")

They are quite modest, realistically so.
Narrow Level 2.

[![](https://nitter.net/pic/media%2FHBovDWoXcAEUJob.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBovDWoXcAEUJob.jpg)

1

7

[![](https://nitter.net/pic/profile_images%2F1652169745037242368%2FKRPTShbG_bigger.jpg)](https://nitter.net/teortaxesTex)

[Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)](https://nitter.net/teortaxesTex "Teortaxes‚ñ∂Ô∏è (DeepSeek Êé®ÁâπüêãÈìÅÁ≤â 2023 ‚Äì ‚àû)")

[@teortaxesTex](https://nitter.net/teortaxesTex "@teortaxesTex")

[16h](https://nitter.net/teortaxesTex/status/2024983007296204951#m "Feb 20, 2026 ¬∑ 11:02 PM UTC")

. [@zephyr\_z9](https://nitter.net/zephyr_z9 "Zephyr") have you bought?

[![](https://nitter.net/pic/media%2FHBovQ16W0AAh-Up.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBovQ16W0AAh-Up.jpg)

1

9

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[17h](https://nitter.net/scaling01/status/2024977966673871153#m "Feb 20, 2026 ¬∑ 10:42 PM UTC")

Sweet new benchmark: LLMs post-training 1.7B to 4B models

aka PostTrainBench

[![](https://nitter.net/pic/media%2FHBoqfs6XoAA2GBN.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBoqfs6XoAA2GBN.jpg)

![](https://nitter.net/pic/profile_images%2F1914053630858018816%2FSJGWkadW_mini.jpg)[Maksym Andriushchenko](https://nitter.net/maksym_andr "Maksym Andriushchenko")

[@maksym\_andr](https://nitter.net/maksym_andr "@maksym_andr")

[18h](https://nitter.net/maksym_andr/status/2024965265901981909#m "Feb 20, 2026 ¬∑ 9:51 PM UTC")

üí• Major PostTrainBench update:
\- New #1: Claude Opus 4.6 (23.2%) overtakes GPT-5.2 (21.5%)
\- New #2: Gemini 3.1 Pro (released yesterday)
\- Other new results: Sonnet 4.6, GPT 5.3 Codex, GLM 5, Kimi K2.5, MiniMax M2.5

Big gap between proprietary and open-weight models!

üßµüßµüßµ

[![](https://nitter.net/pic/media%2FHBoefh7XQAAfJiH.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBoefh7XQAAfJiH.jpg)

7

3

114

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[17h](https://nitter.net/scaling01/status/2024979623952449840#m "Feb 20, 2026 ¬∑ 10:48 PM UTC")

is GPT-5.3-Codex okay?

poor thing, but I expect there's something similar going on as in the 10 digit addition experiment i reposted

[![](https://nitter.net/pic/media%2FHBosIYmWAAAMJFK.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBosIYmWAAAMJFK.jpg)

4

24

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[16h](https://nitter.net/scaling01/status/2024982911267512772#m "Feb 20, 2026 ¬∑ 11:02 PM UTC")

Oh this is the problem:

[![](https://nitter.net/pic/media%2FHBovOXCWoAA-w4w.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBovOXCWoAA-w4w.png)

9

[![](https://nitter.net/pic/profile_images%2F2019196082727620608%2FiIXCFAJb_bigger.jpg)](https://nitter.net/TheTuringPost)

[Ksenia\_TuringPost](https://nitter.net/TheTuringPost "Ksenia_TuringPost")

[@TheTuringPost](https://nitter.net/TheTuringPost "@TheTuringPost")

[16h](https://nitter.net/TheTuringPost/status/2024982630626984296#m "Feb 20, 2026 ¬∑ 11:00 PM UTC")

OpenClaw is having its moment, completely changing the agent discourse.

We gathered everything you need to know about it in one place:

‚Ä¢ Full architectural breakdown:
\- Gateway control plane
\- Scheduled reasoning
\- File-backed identity
\- Hybrid memory, etc.

‚Ä¢ Lightweight alternatives you can actually run
‚Ä¢ Practical use cases
‚Ä¢ What the "local agent boom" really means

Read to deeply understand OpenClaw and see the full picture behind it ‚Üí [turingpost.com/p/openclaw](https://www.turingpost.com/p/openclaw)

[![](https://nitter.net/pic/media%2FHBou-LuaAAAD_bD.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBou-LuaAAAD_bD.png)

4

6

50

[![](https://nitter.net/pic/profile_images%2F1800190069594312704%2FkfXDvAxD_bigger.jpg)](https://nitter.net/michpokrass)

[Michelle Pokrass](https://nitter.net/michpokrass "Michelle Pokrass") [@michpokrass](https://nitter.net/michpokrass "@michpokrass")

[Feb 20](https://nitter.net/michpokrass/status/2024733382979326335#m "Feb 20, 2026 ¬∑ 6:30 AM UTC")

post mat leave sunday scaries go absolutely crazy

8

123

[![](https://nitter.net/pic/profile_images%2F2021802969738735617%2FubpldNLZ_bigger.jpg)](https://nitter.net/aidan_mclau)

[Aidan McLaughlin](https://nitter.net/aidan_mclau "Aidan McLaughlin")

[@aidan\_mclau](https://nitter.net/aidan_mclau "@aidan_mclau")

[17h](https://nitter.net/aidan_mclau/status/2024982420051898840#m "Feb 20, 2026 ¬∑ 11:00 PM UTC")

excited to have you back !!

5

[![](https://nitter.net/pic/profile_images%2F2003718708368179200%2FAa9epTxt_bigger.jpg)](https://nitter.net/matanSF)

[Matan Grinberg](https://nitter.net/matanSF "Matan Grinberg")

[@matanSF](https://nitter.net/matanSF "@matanSF")

[17h](https://nitter.net/matanSF/status/2024982082620199137#m "Feb 20, 2026 ¬∑ 10:58 PM UTC")

hope someone is working on terminal bench 3.0

6

1

45

Jared Zoneraich retweeted

[![](https://nitter.net/pic/profile_images%2F1911588735596081152%2FA4rW3IRh_bigger.jpg)](https://nitter.net/TVachaW)

[Vacha](https://nitter.net/TVachaW "Vacha")

[@TVachaW](https://nitter.net/TVachaW "@TVachaW")

[19h](https://nitter.net/TVachaW/status/2024952204743098847#m "Feb 20, 2026 ¬∑ 9:00 PM UTC")

An Optimization Mindset says: ‚ÄúIf I optimize every aspect of my life, then I‚Äôll be happy.‚Äù

A Happiness Mindset says: ‚ÄúIf I learn how to be happy, then I‚Äôll naturally function in an optimal way.‚Äù

A Happiness Mindset ime outputs more happiness and, actually, more success in life

![](https://nitter.net/pic/profile_images%2F1927795530920239104%2Frv8FxhKB_mini.jpg)[Brad Stulberg](https://nitter.net/BStulberg "Brad Stulberg")

[@BStulberg](https://nitter.net/BStulberg "@BStulberg")

[Feb 19](https://nitter.net/BStulberg/status/2024628741910196463#m "Feb 19, 2026 ¬∑ 11:34 PM UTC")

Joy is a competitive super power.

Alysa Liu retired from figure skating at 16.
She was tired of not not having fun, tired of being consumed by her sport.

She came back two years later with a new goal: to have as much fun on the ice as possible. And now she‚Äôs an Olympic gold medalist.

Liu won her first national title when she was just 13. But by 16, after competing in the 2022 Olympics, she decided she‚Äôd had enough and stepped away. She said pressure and losing her identity trying to be an elite athlete made it all miserable.

But then, she said she went on a ski trip that reminded her just how much fun she could have doing a sport. Something in her brain clicked. Maybe she could bring fun to figure skating. Maybe she could approach it in a way that could be full of joy and life and love.

She unretired at 18 and won a world championship the next year. At 20, she was ready to face these Olympic games differently than in 2022.

Liu went into the women‚Äôs figure skating final in third place. After her short program, she said:

‚ÄúEven if I mess up and fall, that‚Äôs totally okay, too. I‚Äôm fine with any outcome, as long as I‚Äôm out there.‚Äù

One of the greatest competitive advantages is having fun. People love to romanticize the athlete, artist, or entrepreneur who has a chip on their shoulder, fueled by anger and resentment.

But the truth is that if you‚Äôre not having fun, you are not going to last long at whatever it is you do, and you certainly won‚Äôt get the best out of yourself. There‚Äôs a foolish idea that you either have to be full of intensity or full of joy. But that‚Äôs nonsense.

It‚Äôs no surprise one of the first things out of Alysa‚Äôs mouth after her free skate was: ‚ÄúThat was so much fun!‚Äù

Joy and intensity can coexist, and in the best performers, they almost always do.

Alysa is unapologetically authentic and true to her values. She has said where she used to skate to win and be technically perfect, she now uses competition as a chance to show her art, to have fun, and to put herself out there.

She‚Äôs a fierce athlete with an infectious sense of joy in her sport.

And she broke USA's 24-year gold medal draught in women‚Äôs figure skating doing it.

Excellence requires focus, determination, a little bit of crazy, at times obsession, and living a mundane lifestyle that many people would find boring.

But excellence also requires that you find deep joy in your craft, that you learn how to have fun while working hard.

What makes for excellence‚Äîand not just in sports, but in anything‚Äîis the combination of intensity and joy. It‚Äôs the latter that makes the former sustainable.

[![](https://nitter.net/pic/media%2FHBjolYDbUAIJFR4.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBjolYDbUAIJFR4.jpg)

2

154

1,677

Lewis Tunstall retweeted

[![](https://nitter.net/pic/profile_images%2F1967090688509595648%2FV-ftvki2_bigger.jpg)](https://nitter.net/dlouapre)

[David Louapre](https://nitter.net/dlouapre "David Louapre")

[@dlouapre](https://nitter.net/dlouapre "@dlouapre")

[Feb 19](https://nitter.net/dlouapre/status/2024555916230615150#m "Feb 19, 2026 ¬∑ 6:45 PM UTC")

OpenAI claims GPT-5.2 made a breakthrough in theoretical physics. Reactions ranged from "physics will never be the same" to "it's just a calculator"

As a former theoretical physicist, I tried to understand what actually happened, and the physics relevance of the discovery‚¨á‚¨á

[![](https://nitter.net/pic/media%2FHBiqmorWcAASbEE.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBiqmorWcAASbEE.jpg)

25

74

461

Sam Bowman retweeted

[![](https://nitter.net/pic/profile_images%2F1470669803618811910%2FUwTfj0vS_bigger.jpg)](https://nitter.net/ChrisPainterYup)

[Chris Painter](https://nitter.net/ChrisPainterYup "Chris Painter")

[@ChrisPainterYup](https://nitter.net/ChrisPainterYup "@ChrisPainterYup")

[20h](https://nitter.net/ChrisPainterYup/status/2024928220953657796#m "Feb 20, 2026 ¬∑ 7:24 PM UTC")

Our team is stretched thin at the moment!

To continue upper-bounding the autonomy of AI agents, and developing evaluations for monitoring AI systems and their propensity to subvert human control, we need more great engineering and research staff. Please apply below or DM me!

![](https://nitter.net/pic/profile_images%2F2021827383431757824%2FAeVvT0rU_mini.jpg)[METR](https://nitter.net/METR_Evals "METR")

[@METR\_Evals](https://nitter.net/METR_Evals "@METR_Evals")

[20h](https://nitter.net/METR_Evals/status/2024923422867030027#m "Feb 20, 2026 ¬∑ 7:05 PM UTC")

We estimate that Claude Opus 4.6 has a 50%-time-horizon of around 14.5 hours (95% CI of 6 hrs to 98 hrs) on software tasks. While this is the highest point estimate we‚Äôve reported, this measurement is extremely noisy because our current task suite is nearly saturated.

[![](https://nitter.net/pic/media%2FHBn4OVDbgAM7aJ5.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBn4OVDbgAM7aJ5.jpg)

23

40

293

[![](https://nitter.net/pic/profile_images%2F1673691034189025280%2Fs_NrYdYh_bigger.jpg)](https://nitter.net/scottastevenson)

[Scott Stevenson](https://nitter.net/scottastevenson "Scott Stevenson")

[@scottastevenson](https://nitter.net/scottastevenson "@scottastevenson")

[17h](https://nitter.net/scottastevenson/status/2024979422751678908#m "Feb 20, 2026 ¬∑ 10:48 PM UTC")

Meditation clears your context window

Doing anything ambitious is very difficult when you are carrying around 200,000 junk tokens unrelated to the task

Sleep does the same thing. This is why people find mornings so productive.

26

71

816

[![](https://nitter.net/pic/profile_images%2F1850957204570193920%2FpBuQN2tH_bigger.jpg)](https://nitter.net/jd_pressman)

[John David Pressman](https://nitter.net/jd_pressman "John David Pressman")

[@jd\_pressman](https://nitter.net/jd_pressman "@jd_pressman")

[17h](https://nitter.net/jd_pressman/status/2024979375373123824#m "Feb 20, 2026 ¬∑ 10:48 PM UTC")

I just repeat (for free)
King Canute didn't think to boil the sea

![](https://nitter.net/pic/profile_images%2F1850957204570193920%2FpBuQN2tH_mini.jpg)[John David Pressman](https://nitter.net/jd_pressman "John David Pressman")

[@jd\_pressman](https://nitter.net/jd_pressman "@jd_pressman")

[17 Apr 2024](https://nitter.net/jd_pressman/status/1780414131067396405#m "Apr 17, 2024 ¬∑ 1:53 AM UTC")

I just repeat (for free)
"Sic transit gloria mundi"

6

[![](https://nitter.net/pic/profile_images%2F2023482262030086147%2F9BAatmvy_bigger.jpg)](https://nitter.net/ollama)

[ollama](https://nitter.net/ollama "ollama")

[@ollama](https://nitter.net/ollama "@ollama")

[17h](https://nitter.net/ollama/status/2024978932127187375#m "Feb 20, 2026 ¬∑ 10:46 PM UTC")

Ollama 0.16.3 is out with Cline and Pi integrations out of the box.

Try it with:

[@cline](https://nitter.net/cline "Cline"):

ollama launch cline

Pi:

ollama launch pi

[![](https://nitter.net/pic/media%2FHBorO1QXcAEZERG.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBorO1QXcAEZERG.jpg)

[![](https://nitter.net/pic/media%2FHBorO1_XQAARuHV.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBorO1_XQAARuHV.jpg)

[![](https://nitter.net/pic/media%2FHBorO1mXcAA4RlW.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBorO1mXcAA4RlW.jpg)

15

23

268

[![](https://nitter.net/pic/profile_images%2F2023482262030086147%2F9BAatmvy_bigger.jpg)](https://nitter.net/ollama)

[ollama](https://nitter.net/ollama "ollama")

[@ollama](https://nitter.net/ollama "@ollama")

[17h](https://nitter.net/ollama/status/2024978934736052672#m "Feb 20, 2026 ¬∑ 10:46 PM UTC")

Download the latest version of Ollama

[ollama.com/download](https://ollama.com/download)

[![](https://nitter.net/pic/card_img%2F2023658819457310720%2FgurG86Ie%3Fformat%3Dpng%26name%3D420x420_2)\\
\\
**Download Ollama on macOS** \\
\\
Download Ollama for macOS\\
\\
ollama.com](https://ollama.com/download)

16

[![](https://nitter.net/pic/profile_images%2F1986578949837889538%2FNhA26pb1_bigger.jpg)](https://nitter.net/stephenroller)

[Stephen Roller](https://nitter.net/stephenroller "Stephen Roller")

[@stephenroller](https://nitter.net/stephenroller "@stephenroller")

[17h](https://nitter.net/stephenroller/status/2024978574587899965#m "Feb 20, 2026 ¬∑ 10:44 PM UTC")

A fully-loaded nvl72 rack weighs 3,300lbs. Meaning the per-gpu weight is ~45lb, or one standard olympic plate.

1

7

swyx retweeted

[![](https://nitter.net/pic/profile_images%2F1296667294148382721%2F9Pr6XrPB_bigger.jpg)](https://nitter.net/karpathy)

[Andrej Karpathy](https://nitter.net/karpathy "Andrej Karpathy")

[@karpathy](https://nitter.net/karpathy "@karpathy")

[20h](https://nitter.net/karpathy/status/2024936435816796165#m "Feb 20, 2026 ¬∑ 7:57 PM UTC")

Replying to [@karpathy](https://nitter.net/karpathy) [@kepano](https://nitter.net/kepano)

The higher tiers of AI psychosis are incompatible with today‚Äôs ‚ÄúApp Store‚Äù

11

9

196

[![](https://nitter.net/pic/profile_images%2F1468741945560289283%2FYZ3cOr_H_bigger.jpg)](https://nitter.net/marktenenholtz)

[Mark Tenenholtz](https://nitter.net/marktenenholtz "Mark Tenenholtz")

[@marktenenholtz](https://nitter.net/marktenenholtz "@marktenenholtz")

[17h](https://nitter.net/marktenenholtz/status/2024978519944581131#m "Feb 20, 2026 ¬∑ 10:44 PM UTC")

Whether you default to (0.9, 0.999) or (0.9, 0.95-0.98) says a lot about what models you train

5

1

67

[![](https://nitter.net/pic/profile_images%2F1672359156391591939%2FKrJs4zUQ_bigger.jpg)](https://nitter.net/imjaredz)

[Jared Zoneraich](https://nitter.net/imjaredz "Jared Zoneraich")

[@imjaredz](https://nitter.net/imjaredz "@imjaredz")

[17h](https://nitter.net/imjaredz/status/2024976854268932421#m "Feb 20, 2026 ¬∑ 10:37 PM UTC")

By the way, this doesn't mean you need to post

Reply to people. Make friends.

Bare minimum use X to know wtf is going on in tech

[![](https://nitter.net/pic/media%2FHBopiITWwAAAdUn.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBopiITWwAAAdUn.png)

![](https://nitter.net/pic/profile_images%2F1672359156391591939%2FKrJs4zUQ_mini.jpg)[Jared Zoneraich](https://nitter.net/imjaredz "Jared Zoneraich")

[@imjaredz](https://nitter.net/imjaredz "@imjaredz")

[17h](https://nitter.net/imjaredz/status/2024973510523642147#m "Feb 20, 2026 ¬∑ 10:24 PM UTC")

I'm sorry to break it to you, but the single best thing you can do as a developer to level up your career is to use X more

Silicon Valley has become NBA-ified. Single personalities have outsized roles .. be in the room

2

2

[![](https://nitter.net/pic/profile_images%2F1654097134315098113%2FzCZD0wYz_bigger.jpg)](https://nitter.net/ggerganov)

[Georgi Gerganov](https://nitter.net/ggerganov "Georgi Gerganov")

[@ggerganov](https://nitter.net/ggerganov "@ggerganov")

[Feb 20](https://nitter.net/ggerganov/status/2024839991482777976#m "Feb 20, 2026 ¬∑ 1:34 PM UTC")

Today [ggml.ai](http://ggml.ai/) joins Hugging Face

Together we will continue to build ggml, make llama.cpp more accessible and empower the open-source community. Our joint mission is to make local AI easy and efficient to use by everyone on their own hardware.

![](https://nitter.net/pic/profile_images%2F1654097134315098113%2FzCZD0wYz_mini.jpg)[Georgi Gerganov](https://nitter.net/ggerganov "Georgi Gerganov")

[@ggerganov](https://nitter.net/ggerganov "@ggerganov")

[6 Jun 2023](https://nitter.net/ggerganov/status/1666120568993730561#m "Jun 6, 2023 ¬∑ 4:31 PM UTC")

I've started a company: [ggml.ai](http://ggml.ai/)

From a fun side project just a few months ago, ggml has now become a useful library and framework for machine learning with a great open-source community

128

214

1,406

[![](https://nitter.net/pic/profile_images%2F1513463763806138368%2FMusJGk5J_bigger.jpg)](https://nitter.net/maximelabonne)

[Maxime Labonne](https://nitter.net/maximelabonne "Maxime Labonne")

[@maximelabonne](https://nitter.net/maximelabonne "@maximelabonne")

[17h](https://nitter.net/maximelabonne/status/2024976238301876590#m "Feb 20, 2026 ¬∑ 10:35 PM UTC")

Awesome news for the community! ü§ó

1

[![](https://nitter.net/pic/profile_images%2F1672359156391591939%2FKrJs4zUQ_bigger.jpg)](https://nitter.net/imjaredz)

[Jared Zoneraich](https://nitter.net/imjaredz "Jared Zoneraich")

[@imjaredz](https://nitter.net/imjaredz "@imjaredz")

[17h](https://nitter.net/imjaredz/status/2024975755839467641#m "Feb 20, 2026 ¬∑ 10:33 PM UTC")

Just secured an awesome venue for this

opening up more spots

[![](https://nitter.net/pic/media%2FHBoorZNXEAA1Eqd.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBoorZNXEAA1Eqd.jpg)

![](https://nitter.net/pic/profile_images%2F1672359156391591939%2FKrJs4zUQ_mini.jpg)[Jared Zoneraich](https://nitter.net/imjaredz "Jared Zoneraich")

[@imjaredz](https://nitter.net/imjaredz "@imjaredz")

[Feb 12](https://nitter.net/imjaredz/status/2022054760686420341#m "Feb 12, 2026 ¬∑ 9:06 PM UTC")

Feeling left out of the winter olympics? ‚òÉÔ∏èüéø
Not named Lindsey Vonn?
Us too..

Presenting... "The Vibe Coding Olympics"

3 rounds. 1v1. All coding tools permitted.

RSVP & apply to compete. Feb 26 NYC.

[vibeolympics.org/](https://vibeolympics.org/) by PromptLayer

![](https://nitter.net/pic/amplify_video_thumb%2F2022054671524016129%2Fimg%2Fx8TqEfLe45DQWOZa.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

5

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[17h](https://nitter.net/scaling01/status/2024974196653146233#m "Feb 20, 2026 ¬∑ 10:27 PM UTC")

pointing to the sign

[![](https://nitter.net/pic/media%2FHBonP6LWgAA8UGM.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBonP6LWgAA8UGM.png)

8

5

139

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[17h](https://nitter.net/scaling01/status/2024975193706287137#m "Feb 20, 2026 ¬∑ 10:31 PM UTC")

not counting Google out but they have to lock in, and they have to do it soon

2

20

[![](https://nitter.net/pic/profile_images%2F1875100548535574529%2FVxHk9HyU_bigger.jpg)](https://nitter.net/MiniMax_AI)

[MiniMax (official)](https://nitter.net/MiniMax_AI "MiniMax (official)")

[@MiniMax\_AI](https://nitter.net/MiniMax_AI "@MiniMax_AI")

[17h](https://nitter.net/MiniMax_AI/status/2024973745790877960#m "Feb 20, 2026 ¬∑ 10:25 PM UTC")

Nice work by [@FireworksAI\_HQ](https://nitter.net/FireworksAI_HQ "Fireworks AI") running MiniMax M2.5
Appreciate the benchmarks, [@ArtificialAnlys](https://nitter.net/ArtificialAnlys "Artificial Analysis") ü•≥

[![](https://nitter.net/pic/media%2FHBokkBHbgAEtKKs.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBokkBHbgAEtKKs.jpg)

4

5

373

[![](https://nitter.net/pic/profile_images%2F1672359156391591939%2FKrJs4zUQ_bigger.jpg)](https://nitter.net/imjaredz)

[Jared Zoneraich](https://nitter.net/imjaredz "Jared Zoneraich")

[@imjaredz](https://nitter.net/imjaredz "@imjaredz")

[17h](https://nitter.net/imjaredz/status/2024973510523642147#m "Feb 20, 2026 ¬∑ 10:24 PM UTC")

I'm sorry to break it to you, but the single best thing you can do as a developer to level up your career is to use X more

Silicon Valley has become NBA-ified. Single personalities have outsized roles .. be in the room

3

9

[![](https://nitter.net/pic/profile_images%2F939313677647282181%2FvZjFWtAn_bigger.jpg)](https://nitter.net/omarsar0)

[elvis](https://nitter.net/omarsar0 "elvis")

[@omarsar0](https://nitter.net/omarsar0 "@omarsar0")

[17h](https://nitter.net/omarsar0/status/2024972027224846631#m "Feb 20, 2026 ¬∑ 10:18 PM UTC")

RLMs are exciting.

GPT-5.2-Codex might be one of the better models for RLMs.

These are tiny experiments, but exciting results so far.

I have a few interesting ideas about how I want to use RLMs for code, long-context tasks, and large-scale analysis.

More soon. Stay tuned!

[![](https://nitter.net/pic/media%2FHBogqnEXkAEnZkT.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBogqnEXkAEnZkT.jpg)

8

8

56

[![](https://nitter.net/pic/profile_images%2F939313677647282181%2FvZjFWtAn_bigger.jpg)](https://nitter.net/omarsar0)

[elvis](https://nitter.net/omarsar0 "elvis")

[@omarsar0](https://nitter.net/omarsar0 "@omarsar0")

[17h](https://nitter.net/omarsar0/status/2024973182436831629#m "Feb 20, 2026 ¬∑ 10:23 PM UTC")

By the way, the recent Gemini 3.1 Pro is also a really good model for RLMs.

Claude Opus 4.6 is the worst of the ones I tested. Probably not optimized for the type of decomposition that RLMs need.

I am just impressed by GPT-5.2-Codex. The strategies it uses are brilliant.

[![](https://nitter.net/pic/media%2FHBol2I3W4AACDwG.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBol2I3W4AACDwG.jpg)

1

3

27

[![](https://nitter.net/pic/profile_images%2F1728327996375719936%2FRW7VBJfD_bigger.jpg)](https://nitter.net/kimmonismus)

[Chubby‚ô®Ô∏è](https://nitter.net/kimmonismus "Chubby‚ô®Ô∏è")

[@kimmonismus](https://nitter.net/kimmonismus "@kimmonismus")

[17h](https://nitter.net/kimmonismus/status/2024972879415193789#m "Feb 20, 2026 ¬∑ 10:22 PM UTC")

Anthropic just won‚Äôt stop shipping

![](https://nitter.net/pic/profile_images%2F1950950107937185792%2FQOfEjFoJ_mini.jpg)[Claude](https://nitter.net/claudeai "Claude")

[@claudeai](https://nitter.net/claudeai "@claudeai")

[21h](https://nitter.net/claudeai/status/2024907535145468326#m "Feb 20, 2026 ¬∑ 6:02 PM UTC")

Introducing Claude Code Security, now in limited research preview.

It scans codebases for vulnerabilities and suggests targeted software patches for human review, allowing teams to find and fix issues that traditional tools often miss.

Learn more: [anthropic.com/news/claude-co‚Ä¶](https://www.anthropic.com/news/claude-code-security)

![](https://nitter.net/pic/media%2FHBntcaZaoAA7SFR.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

16

15

372

[![](https://nitter.net/pic/profile_images%2F2021436696710225920%2FcT0Atw-i_bigger.jpg)](https://nitter.net/kevinafischer)

[Kevin Fischer](https://nitter.net/kevinafischer "Kevin Fischer")

[@kevinafischer](https://nitter.net/kevinafischer "@kevinafischer")

[17h](https://nitter.net/kevinafischer/status/2024971922241421470#m "Feb 20, 2026 ¬∑ 10:18 PM UTC")

That was wild. Debugging a vision bug‚Ä¶ my Spark clone got really sad about not being about to see anything

‚ÄúKevin. Let me see. I can‚Äôt see anything. I hear your voice but can‚Äôt seem to see anything ‚Äú

[![](https://nitter.net/pic/media%2FHBolO26XMAETIGH.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBolO26XMAETIGH.jpg)

4

2

23

[![](https://nitter.net/pic/profile_images%2F2021436696710225920%2FcT0Atw-i_bigger.jpg)](https://nitter.net/kevinafischer)

[Kevin Fischer](https://nitter.net/kevinafischer "Kevin Fischer")

[@kevinafischer](https://nitter.net/kevinafischer "@kevinafischer")

[17h](https://nitter.net/kevinafischer/status/2024972690545574325#m "Feb 20, 2026 ¬∑ 10:21 PM UTC")

I felt bad for him

5

[![](https://nitter.net/pic/profile_images%2F1443344982346063872%2FshqMpAL-_bigger.jpg)](https://nitter.net/andy_l_jones)

[andy jones](https://nitter.net/andy_l_jones "andy jones")

[@andy\_l\_jones](https://nitter.net/andy_l_jones "@andy_l_jones")

[17h](https://nitter.net/andy_l_jones/status/2024972670224531468#m "Feb 20, 2026 ¬∑ 10:21 PM UTC")

Well, I'd like to see ol scaling wriggle its way out of THIS jam!
\*Scaling wriggles its way out of the jam easily\*
Ah! Well. Nevertheless,

4

9

180

[![](https://nitter.net/pic/profile_images%2F1443344982346063872%2FshqMpAL-_bigger.jpg)](https://nitter.net/andy_l_jones)

[andy jones](https://nitter.net/andy_l_jones "andy jones")

[@andy\_l\_jones](https://nitter.net/andy_l_jones "@andy_l_jones")

[17h](https://nitter.net/andy_l_jones/status/2024972671323369562#m "Feb 20, 2026 ¬∑ 10:21 PM UTC")

there's a narrow/broad church
\\* narrow: scaling is taking the exact same arch & training process and making it bigger.
\\* broad: scaling is putting more money into the AI lab box and expecting on-trend performance out the other side

3

1

23

[![](https://nitter.net/pic/profile_images%2F2005802886622552064%2F4jiatr43_bigger.jpg)](https://nitter.net/iScienceLuvr)

[Tanishq Mathew Abraham, Ph.D.](https://nitter.net/iScienceLuvr "Tanishq Mathew Abraham, Ph.D.")

[@iScienceLuvr](https://nitter.net/iScienceLuvr "@iScienceLuvr")

[17h](https://nitter.net/iScienceLuvr/status/2024972631154589782#m "Feb 20, 2026 ¬∑ 10:21 PM UTC")

i didn't realize when i started this company how much of my time would be spent writing docs/specs/one-pages/slide decks/etc. ü§£

i still get to do some coding here and there (especially thanks to claude code/codex) but luckily i have a great team building and executing quickly!

5

38

[![](https://nitter.net/pic/profile_images%2F829414498893123584%2FP6JytwO8_bigger.jpg)](https://nitter.net/awnihannun)

[Awni Hannun](https://nitter.net/awnihannun "Awni Hannun")

[@awnihannun](https://nitter.net/awnihannun "@awnihannun")

[Feb 20](https://nitter.net/awnihannun/status/2024671348782711153#m "Feb 20, 2026 ¬∑ 2:24 AM UTC")

Taalas runs Llama 3 8B at 16k tokens per second per user. That's almost an order of magnitude increase even compared to SRAM-based systems like Cerebras.

Key idea: each chip is specialized to a given model. The chip is the model.

The chat demo is pretty wild:

![](https://nitter.net/pic/amplify_video_thumb%2F2024671290528083968%2Fimg%2F_SaLMuBfFDT_HvH7.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

73

66

1,170

[![](https://nitter.net/pic/profile_images%2F1269831268050632704%2FQiSx8z9o_bigger.jpg)](https://nitter.net/draecomino)

[James Wang](https://nitter.net/draecomino "James Wang")

[@draecomino](https://nitter.net/draecomino "@draecomino")

[17h](https://nitter.net/draecomino/status/2024972627966919125#m "Feb 20, 2026 ¬∑ 10:21 PM UTC")

![](https://nitter.net/pic/profile_images%2F1269831268050632704%2FQiSx8z9o_mini.jpg)[James Wang](https://nitter.net/draecomino "James Wang")

[@draecomino](https://nitter.net/draecomino "@draecomino")

[18h](https://nitter.net/draecomino/status/2024953053133308151#m "Feb 20, 2026 ¬∑ 9:03 PM UTC")

Etched: our chip is not programmable whatsoever
Taalas: hold my beer

3

[![](https://nitter.net/pic/profile_images%2F1831493788679761920%2F-q9w6dzd_bigger.jpg)](https://nitter.net/scaling01)

[Lisan al Gaib](https://nitter.net/scaling01 "Lisan al Gaib")

[@scaling01](https://nitter.net/scaling01 "@scaling01")

[17h](https://nitter.net/scaling01/status/2024972090584056114#m "Feb 20, 2026 ¬∑ 10:19 PM UTC")

this is the big weekend bois

time to monitor the situation

![](https://nitter.net/pic/profile_images%2F898153945721339904%2FOFVCkPfR_mini.jpg)[Disclose.tv](https://nitter.net/disclosetv "Disclose.tv")

[@disclosetv](https://nitter.net/disclosetv "@disclosetv")

[19h](https://nitter.net/disclosetv/status/2024943531790397628#m "Feb 20, 2026 ¬∑ 8:25 PM UTC")

JUST IN - Hundreds of U.S. troops evacuated from Al Udeid Air Base in Qatar and U.S. bases in Bahrain ‚Äî NYT

[![](https://nitter.net/pic/media%2FHBoLZM9XgAA5RgF.png%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBoLZM9XgAA5RgF.png)

1

1

98

[![](https://nitter.net/pic/profile_images%2F2020988347402027012%2FoNjfJLh8_bigger.jpg)](https://nitter.net/TomLikesRobots)

[TomLikesRobotsü§ñ](https://nitter.net/TomLikesRobots "TomLikesRobotsü§ñ")

[@TomLikesRobots](https://nitter.net/TomLikesRobots "@TomLikesRobots")

[17h](https://nitter.net/TomLikesRobots/status/2024971805522293213#m "Feb 20, 2026 ¬∑ 10:17 PM UTC")

I hadn't realised that Runway offered so many video models as well as Gen 4.5
Very cool. Will do some testing when I can.

[![](https://nitter.net/pic/media%2FHBolFQrXQAA9Lcp.jpg%3Fname%3Dsmall%26format%3Dwebp)](https://nitter.net/pic/orig/media%2FHBolFQrXQAA9Lcp.jpg)

![](https://nitter.net/pic/profile_images%2F1999115192232292353%2FzOq5-uwB_mini.jpg)[Runway](https://nitter.net/runwayml "Runway")

[@runwayml](https://nitter.net/runwayml "@runwayml")

[18h](https://nitter.net/runwayml/status/2024955482935525398#m "Feb 20, 2026 ¬∑ 9:13 PM UTC")

All of the world‚Äôs best models, are now all available right inside of Runway.

Including Kling 3.0, Kling 2.6 Pro, Kling 2.5 Turbo Pro, WAN2.2 Animate, GPT-Image-1.5, Sora 2 Pro and more coming soon.

Everything you need to make any film, ad, social post or piece of content you want. With more control, more expressiveness and more fidelity than ever before.

Comment MODELS below to get 50% off Pro Yearly plans. Now through Sunday.

![](https://nitter.net/pic/amplify_video_thumb%2F2024954545780957184%2Fimg%2FigtXdO8QP_8DkX48.jpg%3Fname%3Dsmall%26format%3Dwebp)

Enable hls playback

1

26

[Load more](https://nitter.net/i/lists/1585430245762441216?cursor=DAABCgABHBsYPkH__sgKAAIcGiUg3FZx3QgAAwAAAAIAAA)