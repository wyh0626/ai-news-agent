---
title: "AI 日报 — 2026-02-17"
description: "最新研究表明，LLMs在内部对真相的编码比输出更可靠 · Qwen3.5NVFP4上线 · 博主对 Opus 与 Codex 的日常混用进行对比..."
pubDate: "2026-02-17"
category: "daily"
pairSlug: "ai-daily-2026-02-17-en"
---

> 共收录 16 条 AI 领域资讯

## 🔥 今日焦点

### 1. LLM内部真度高用探针降幻觉
最新研究表明，LLMs在内部对真相的编码比输出更可靠。GoodfireAI 的相关论文将这一发现用于实践：在模型激活层训练探针以检测幻觉，并将探针分数作为强化学习奖励来减少幻觉。此方法有望提升对真相的内在把控，降低错误信息传播的风险。[原始链接-x](https://x.com/OrgadHadas/status/2023596564443226313)

### 2. Qwen3.5NVFP4上线
Qwen3.5NVFP4（Blackwell）上线，采用 NVIDIA Model Optimizer 将模型量化到 FP4，检查点约 224GB，总参数 17B，并以 Apache 2.0 许可发布。文章还提到 Speculative Decoding 与内置多 Token 预测头，适合较低并发场景尝试。[原始链接-reddit](https://www.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/)

### 3. 每日混用Opus与Codex
博主对 Opus 与 Codex 的日常混用进行对比与分享，强调两者在实际工作中的互补性与取舍，提供选型参考。对于开发者在不同任务中的应用选择具有一定借鉴意义。[原始链接-x](https://x.com/theo/status/2023729264256782601)

## 📰 重点报道

### LLM 技术进展
- **LLMs内部真度高用探针降幻觉** — 最新研究表明，LLMs在内部对真相的编码比输出更可靠。GoodfireAI 的相关论文将这一发现用于实践：在模型激活层训练探针以检测幻觉，并将探针分数作为强化学习奖励来减少幻觉。[原始链接-x](https://x.com/OrgadHadas/status/2023596564443226313)
- **Qwen3.5NVFP4上线** — Qwen3.5NVFP4（Blackwell）上线，使用 NVIDIA Model Optimizer 将模型量化到 FP4，检查点约 224GB，总参数 17B，并以 Apache 2.0 许可发布；文章提到 Speculative Decoding 与内置多 Token 预测头，适合较低并发场景。[原始链接-reddit](https://www.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/)
- **每日混用Opus与Codex** — 博主对 Opus 与 Codex 的日常混用进行对比与分享，强调两者在实际工作中的互补性与取舍，提供选型参考。[原始链接-x](https://x.com/theo/status/2023729264256782601)

### 开源模型与工具链
- **ACE-Step 1.5 稳定版发布** — ACE-Step 团队发布稳定版 v0.1.0，新增显存检测与自动模型选择/优化，提升低显存 GPU 的兼容性。还优化了一键启动脚本、扩展对 AMD 与 Intel GPU 的支持，并修复若干 bug 与小改进。[原始链接-x](https://x.com/acemusicAI/status/2023707545094025227)
- **DAG优化网页代理提效20%** — 研究提出 WebClipper，将网页代理的搜索过程建模为状态图并裁剪为最小有向无环图，从而在不损失准确性的前提下约减少20%的工具调用轮次。并引入 F-AE Score，用以评估代理轨迹在准确性与效率之间的权衡。对精炼和裁剪后的轨迹进行训练，可使代理从一开始就形成更高效的推理模式，进而降低成本。[原始链接-x](https://x.com/dair_ai/status/2023554252548051409)

### 多语言与多模态
- **Tiny Aya 小模型潜力** — Cohere Labs 推出 Tiny Aya，小型语言模型展现潜力。相比先前的 Aya 版本和同等规模的模型，Tiny Aya 在多语言设计上更具竞争力，证明聚焦多语言研究可在不显著扩大规模的情况下实现更高性能。[原始链接-x](https://x.com/Cohere_Labs/status/2023699487110148523)
- **字节跳动发布 Seed-2.0 模型** — 字节跳动宣布 Seed-2.0，在代理、推理和视觉理解等方面较 Seed-1.8 取得显著进展，且未进行蒸馏。目前全球化部署将很快推进。[原始链接-x](https://x.com/TsingYoga/status/2023764275874197964)

### 行业应用与安全
- （本期重点报道组内未单独设置此分类的项，如需聚焦该方向可调整再补充。）

## ⚡ 快讯速览

- **Podscript 转录工具** — 开发者 timf34 推出 podscript，可将播客或 YouTube 视频转换为带说话者标签和时间戳的 Markdown 转录文本，安装方式为 pip install podscript，并在转录中使用 ElevenLabs 提供的高质量的说话人分离（diarization）。[原始链接-reddit](https://www.reddit.com/r/LocalLLaMA/comments/1r76gi7/i_made_a_cli_that_turns_any_podcast_or_youtube/)

- **RTX5070Ti+5060Ti 39t/s 实现** — 在 RTX 5070 Ti + 5060 Ti（32GB显存、64GB RAM、Windows 11、CUDA 12.4、llama.cpp b8077）环境下，将 Qwen3-Next-80B MoE 的推理速度从约6.5 tokens/s 提升至 39 t/s，指出默认配置存在 CPU 瓶颈、GPU 利用率低等问题，并通过未公开的调整解决。来源于 Reddit。[原始链接-reddit](https://www.reddit.com/r/LocalLLaMA/comments/1r71af3/solution_found_qwen3next_80b_moe_running_at_39_ts/)

- **Leak: BharatGPT is training a 500b non MOE coding + text multi lingual multimodal sovereign LLM from scratch** — 泄漏信息称 BharatGPT 正在从零开始训练一个 500B 规模、非 MOE 的编码+文本多语言多模态主权大模型，算力花费超千万美元级，且将推出可在网页上运行的版本。来源于 Twitter。[原始链接-x](https://x.com/kingofknowwhere/status/2023660464631411172)

- **A friend gave "V4" a test** — 朋友对 V4 在从30K文件提取要点的测试中给出 7/10 的评分，认为相较于 5.2 和 G3P 有明显改进，但仍有改进空间。来源于 Twitter。[原始链接-x](https://x.com/teortaxesTex/status/2023735703834591312)

- **Moonshot AI (Kimi) keeps raising at a stunning pace** — Moonshot AI（Kimi）在短时间内完成新一轮超 7 亿美元融资，领投方包括阿里巴巴、腾讯等现有投资者，融资 pace 极快。来源于 Twitter。[原始链接-x](https://x.com/poezhao0605/status/2023680650386252160)

- **Codex 多代理并行未触及限额** — 发帖者在 Codex 上同时运行三以上代理，持续超过两小时，使用了五小时窗口的 8% 与每周的 2%，仍未达到使用上限，显示当前系统并非严格限额。来源于 Twitter。[原始链接-x](https://x.com/theo/status/2023718038198251904)

---

*本报告由 AI News Agent 自动生成 | 2026-02-17*