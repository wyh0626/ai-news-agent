---
title: "AI Daily â€” 2026-02-17"
description: "LLM æŠ€æœ¯è¿›å±• Â· å¼€æºæ¨¡å‹ä¸å·¥å…·é“¾ Â· å¤šè¯­è¨€ä¸å¤šæ¨¡æ€"
pubDate: "2026-02-17"
category: "daily"
lang: "en"
pairSlug: "ai-daily-2026-02-17"
---

> A total of 16 AI-related news items were collected

## ğŸ”¥ Top Stories

### 1. High Internal Fidelity in LLMs: Using Probes to Reduce Hallucinations
Latest research shows that LLMs' internal encoding of truth is more reliable than their outputs. GoodfireAI's related paper puts this finding into practice: training probes at model activation layers to detect hallucinations, and using probe scores as reinforcement learning rewards to reduce hallucinations. This approach promises to improve intrinsic control of truth and reduce the spread of misinformation. [Source-x](https://x.com/OrgadHadas/status/2023596564443226313)

### 2. Qwen3.5NVFP4 Goes Live (Blackwell)
Qwen3.5NVFP4 (Blackwell) goes live, using NVIDIA Model Optimizer to quantize the model to FP4, checkpoint around 224GB, total parameters 17B, and released under the Apache 2.0 license. The article also mentions Speculative Decoding and built-in multi-token prediction head, suitable for lower-concurrency scenarios. [Source-reddit](https://www.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/)

### 3. Daily Mixed Use of Opus and Codex
A blogger compares and shares the daily mixed usage of Opus and Codex, emphasizing their complementarity and trade-offs in real-world work, providing guidance for selection. For developers, this is instructive for choosing applications for different tasks. [Source-x](https://x.com/theo/status/2023729264256782601)

## ğŸ“° Featured

### LLM æŠ€æœ¯è¿›å±•
- **High Internal Fidelity in LLMs: Using Probes to Reduce Hallucinations** â€” The latest research shows that LLMs' internal encoding of truth is more reliable than their outputs. GoodfireAI's related paper puts this into practice: training probes at model activation layers to detect hallucinations, and using probe scores as reinforcement learning rewards to reduce hallucinations. [Source-x](https://x.com/OrgadHadas/status/2023596564443226313)
- **Qwen3.5NVFP4ä¸Šçº¿** â€” Qwen3.5NVFP4 (Blackwell) goes live, using NVIDIA Model Optimizer to quantize the model to FP4, checkpoint around 224GB, total parameters 17B, and released under the Apache 2.0 license; the article mentions Speculative Decoding and built-in multi-token prediction head, suitable for lower-concurrency scenarios. [Source-reddit](https://www.reddit.com/r/LocalLLaMA/comments/1r77fz7/qwen35_nvfp4_blackwell_is_up/)
- **æ¯æ—¥æ··ç”¨Opusä¸Codex** â€” A blogger compares and shares the daily mixed usage of Opus and Codex, emphasizing their complementarity and trade-offs in real-world work, providing selection guidance. [Source-x](https://x.com/theo/status/2023729264256782601)

### å¼€æºæ¨¡å‹ä¸å·¥å…·é“¾
- **ACE-Step 1.5 ç¨³å®šç‰ˆå‘å¸ƒ** â€” ACE-Step team released stable version v0.1.0, adding VRAM detection and automatic model selection/optimization to improve compatibility with low-VRAM GPUs. Also optimized one-click startup script, expanded support for AMD and Intel GPUs, and fixed several bugs and small improvements. [Source-x](https://x.com/acemusicAI/status/2023707545094025227)
- **DAGä¼˜åŒ–ç½‘é¡µä»£ç†ææ•ˆ20%** â€” The study proposes WebClipper, modeling the search process of web proxies as a state graph and pruning it to a minimal directed acyclic graph, thereby reducing tool-call iterations by about 20% without sacrificing accuracy. It also introduces the F-AE Score to evaluate the trade-off between accuracy and efficiency of the proxy trajectory. Training on refined and pruned trajectories can enable the agent to form more efficient reasoning from the start, thereby lowering costs. [Source-x](https://x.com/dair_ai/status/2023554252548051409)

### å¤šè¯­è¨€ä¸å¤šæ¨¡æ€
- **Tiny Aya å°æ¨¡å‹æ½œåŠ›** â€” Cohere Labsæ¨å‡º Tiny Ayaï¼Œå°å‹è¯­è¨€æ¨¡å‹å±•ç°æ½œåŠ›ã€‚ç›¸æ¯”å…ˆå‰çš„ Aya ç‰ˆæœ¬å’ŒåŒç­‰è§„æ¨¡çš„æ¨¡å‹ï¼ŒTiny Aya åœ¨å¤šè¯­è¨€è®¾è®¡ä¸Šæ›´å…·ç«äº‰åŠ›ï¼Œè¯æ˜èšç„¦å¤šè¯­è¨€ç ”ç©¶å¯åœ¨ä¸æ˜¾è‘—æ‰©å¤§è§„æ¨¡çš„æƒ…å†µä¸‹å®ç°æ›´é«˜æ€§èƒ½ã€‚ [Source-x](https://x.com/Cohere_Labs/status/2023699487110148523)
- **å­—èŠ‚è·³åŠ¨å‘å¸ƒ Seed-2.0 æ¨¡å‹** â€” å­—èŠ‚è·³åŠ¨å®£å¸ƒ Seed-2.0ï¼Œåœ¨ä»£ç†ã€æ¨ç†å’Œè§†è§‰ç†è§£ç­‰æ–¹é¢è¾ƒ Seed-1.8 å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä¸”æœªè¿›è¡Œè’¸é¦ã€‚ç›®å‰å…¨çƒåŒ–éƒ¨ç½²å°†å¾ˆå¿«æ¨è¿›ã€‚ [Source-x](https://x.com/TsingYoga/status/2023764275874197964)

### è¡Œä¸šåº”ç”¨ä¸å®‰å…¨
- - Note: There are no items categorized under Industry Applications & Security in this issue's Featured section; if you'd like to focus on this direction, we can adjust and add.

## âš¡ Quick Bites

- **Podscript Transcription Tool** â€” Developer timf34 released podscript, which can convert podcasts or YouTube videos into Markdown transcripts with speaker labels and timestamps, installable via pip install podscript, and using ElevenLabs' high-quality speaker diarization in transcription. [Source-reddit](https://www.reddit.com/r/LocalLLaMA/comments/1r76gi7/i_made_a_cli_that_turns_any_podcast_or_youtube/)

- **RTX5070Ti + 5060Ti Achieve 39 t/s** â€” In an RTX 5070 Ti + 5060 Ti setup (32GB VRAM, 64GB RAM, Windows 11, CUDA 12.4, llama.cpp b8077), Qwen3-Next-80B MoE inference speed was boosted from ~6.5 tokens/s to 39 t/s. Defaults reportedly had CPU bottlenecks and low GPU utilization, addressed by undisclosed adjustments. Source: Reddit. [Source-reddit](https://www.reddit.com/r/LocalLLaMA/comments/1r71af3/solution_found_qwen3next_80b_moe_running_at_39_ts/)

- **Leak: BharatGPT is training a 500b non MOE coding + text multilingual multimodal sovereign LLM from scratch** â€” Leaked information suggests BharatGPT is training a 500B non-MOE coding+text multilingual multimodal sovereign LLM from scratch, with compute costs exceeding tens of millions of dollars, and a web-accessible version will be released. Source: Twitter. [Source-x](https://x.com/kingofknowwhere/status/2023660464631411172)

- **A Friend Gave "V4" a Test** â€” A friend tested V4 on extracting key points from 30K documents, rating it 7/10, noting clear improvements over 5.2 and G3P, but room for further improvement. Source: Twitter. [Source-x](https://x.com/teortaxesTex/status/2023735703834591312)

- **Moonshot AI (Kimi) Keeps Raising at a Stunning Pace** â€” Moonshot AI (Kimi) quickly completed a new round of funding exceeding $700 million, led by existing investors including Alibaba, Tencent, and others, with an extremely fast funding pace. Source: Twitter. [Source-x](https://x.com/poezhao0605/status/2023680650386252160)

- **Codex Multi-Agent Parallelism Has Not Hit Quota** â€” A poster ran more than three agents in parallel on Codex for over two hours, using an 8% of a five-hour window and 2% per week, and still did not hit the limit, indicating the system is not strictly capped. Source: Twitter. [Source-x](https://x.com/theo/status/2023718038198251904)

---

*This report was automatically generated by AI News Agent | 2026-02-17*