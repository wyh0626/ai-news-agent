---
title: "AI 日报 — 2026-02-13"
description: "生物信息学与开源蛋白质结构预测 · LLM 推理与基础设施优化 · 开源大模型生态与社区活动"
pubDate: "2026-02-13"
category: "daily"
lang: "zh"
---

- ByteDance Protenix-v1：ByteDance 推出 Protenix-v1 开源蛋白质/生物分子结构预测模型，宣称达到 AF3 级别性能，代码托管在 GitHub（bytedance/Protenix），意在提供 AlphaFold 的开源替代方案，引发 Reddit 社区关注。链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/
- llama.cpp llama-server VRAM 修复：llama.cpp 的 llama-server 针对 SSM 混合模型的 VRAM 使用问题已合并修复，在 1M context、并行参数 --parallel=8 下，KV 缓存的显存需求显著下降至 ~6GB，使在 48GB VRAM 条件下可并发服务更多用户，适用于 Qwen3Next、Kimi Linear、Nemotron 3 Nano 等模型。链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/
- Nvidia Dynamic Memory Sparsification (DMS)：NVIDIA 推出 DMS，通过对 KV 缓存进行学习型保留与延迟逐出，最高可实现 KV 缓存降幅 8 倍，同时提升推理速度与并发能力，对自托管大模型推理具有重要意义。链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/

---

## 重点新闻

### 生物信息学与开源蛋白质结构预测
- ByteDance Protenix-v1
  - 简介与背景：ByteDance 推出 Protenix-v1 开源蛋白质/生物分子结构预测模型，定位为 AlphaFold 的开源竞争对手，旨在推动更广泛的社区参与与创新。
  - 影响与分析：若对齐 AF3 级别的性能承诺成立，可能拓展开源生物信息学工具的可及性，加速学术与产业研究的低成本验证与再训练；GitHub 的开源模式也将加速社区审阅与改进。
  - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/

### LLM 推理与基础设施优化
- llama.cpp llama-server VRAM 修复
  - 简介与背景：针对 SSM 混合模型的 VRAM 需求，修复后在高并发场景下显存利用更接近 Transformer 模型，显著降低单卡显存压力。
  - 影响与分析：在 48GB 余量的设备上可并发服务更多用户，提升自托管部署的规模化能力，特别对 Qwen3Next、Kimi Linear、Nemotron 3 Nano 等模型友好。
  - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/
- Nvidia Dynamic Memory Sparsification (DMS)
  - 简介与背景：DMS 在推理阶段对 KV 缓存进行学习型保留与延迟逐出，允许对低重要性 token 进行短时保留以提取信息，提升推理效率与并发处理。
  - 影响与分析：最高可实现 8x 的缓存降幅，显著降低显存压力，提升自托管场景的规模化推理能力，未来在数据中心与边缘部署中值得关注。
  - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/

### 开源大模型生态与社区活动
- MiniMax 相关 AMA 与社区对话
  - AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2.5 SoTA Model
    - 背景：宣布 MiniMax Lab 将出席本地 LLaMA 社区的 AMA，围绕 MiniMax-M2.5 这一开源 SoTA 模型及相关工作进行讨论。
    - 影响：体现社区透明度与开源研究的协作氛围，可能推动后续版本发布与用户反馈的快速闭环。
    - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3csbk/ama_announcement_minimax_the_opensource_lab/
  - AMA with MiniMax — Ask Us Anything!
    - 背景：r/LocalLLaMA 平台再次举行 MiniMax 团队的 AMA，成员覆盖 Founder/CEO、LLM 研究负责人等，强化对 MiniMax 产品线的理解与参与。
    - 影响：有助于建立社区信任并推动多模态产品线的讨论与合作。
    - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/
  - MiniMax onX: Weights dropping REALLY, REALLY, SOON
    - 背景：关于 MiniMax onX 开源权重即将公开的传闻，若成真将直接影响开源大模型社区的权重获取与对比。
    - 影响：可能改变开源模型的可用性与竞争格局，值得持续关注权重上线时间表。
    - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/
- DeepSeek 长上下文与模型进展
  - New DeepSeek update: 1M context window
    - 背景：DeepSeek 正在测试新的长上下文模型架构，Web/APP 端将支持高达 1M 的上下文窗口，显示在扩展模型能力方面的持续投入。
    - 影响：对超长文本处理场景将带来显著的应用潜力，可能推动企业级应用的最近需求。
    - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/
  - Deepseek announced they are testing a new model
    - 背景与分析：公布测试新模型并给出面向阅读理解技能的评测，尽管名称为占位符以区分条目，但显示其在行业标准数据集上的初步能力评估。
    - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/
- Dhi-5B 与多模态训练进展
  - UG student launches Dhi-5B (Trained from Scratch)
    - 背景：本科生披露 50 亿参数的多模态语言模型 Dhi-5B，训练成本据称仅 ₹1.1 lakh，分阶段发布计划包括 Base/Instruct 等版本。
    - 影响：展示个人和小团队在低成本条件下探索高参数模型的可能性，或推动社区在多模态模型训练成本方面的讨论。
    - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/
- Apple Silicon 环境下的开源推理服务器
  - oMLX - open-source MLX inference server with paged SSD caching for Apple Silicon
    - 背景：介绍面向 Apple Silicon 的开源 MLX 推理服务器，提供原生 macOS 应用与易用部署，强调与 Obsidian Copilot、 Ollama 等生态的整合。
    - 影响：为在 Mac 本地进行 LLM 推理的开发者提供便捷方案，推动本地化推理生态的发展。
    - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3qwyi/omlx_opensource_mlx_inference_server_with_paged/
- MiniMax 生态的权重与发布动态
  - MiniMax-M2.5 Checkpoints on huggingface will be in
    - 背景：关于 MiniMax-M2.5 在 HuggingFace 上线检查点的时间线更新，标志着权重公开的时间点逐步逼近。
    - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/

---

## 快讯速览
- GLM 5 has a regression in international language writing according to NCBench
  - 1 条信息摘要：GLM 5 在国际语言写作基准上出现回归，落后于 GLM 4.5–4.7，欧洲语言与印地语表现不佳，但 Language Comprehension 未显回退。来源 Reddit 用户 u/jugalator 的评测贴。
  - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3ob0r/glm_5_has_a_regression_in_international_language/
- MiniMaxAI/MiniMax-M2.5 · Hugging Face
  - 1 条信息摘要：通过 HuggingFace 模型页可查看 MiniMax-M2.5 的最新进展与权重，页面按修改时间排序并带有 search=minimax m2.5 的检索。来源 Reddit 用户 /u/rerri。
  - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/
- MiniMax-M2.5 Checkpoints on huggingface will be in
  - 1 条信息摘要：MiniMax-M2.5 的检查点将在大约 8 小时后在 HuggingFace 上线，社区讨论中包含相关推断与上线准备。来源 Reddit 用户 Own_Forever_5997。
  - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/
- MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters
  - 1 条信息摘要：MiniMax-M2.5 总参数量 2300 亿，活跃参数 100 亿，虽披露规模但尚未在 HuggingFace 上线，等待正式发布。来源 Reddit 用户 /u/Zyj。
  - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/
- RAM shortage problem solved
  - 1 条信息摘要：LocalLLaMA 社区贴文标题“RAM shortage problem solved”，当前仅有标题与链接，未披露具体细节，指示 RAM 短缺问题可能已被解决。
  - 链接：https://www.reddit.com/r/LocalLLaMA/comments/1r3tqox/ram_shortage_problem_solved/